
.. #=============================================================================
   #
   #  Copyright (c) 2020-2023 Qualcomm Technologies, Inc.
   #  All Rights Reserved.
   #  Confidential and Proprietary - Qualcomm Technologies, Inc.
   #
   #=============================================================================

=========================
Qualcomm AI Engine Direct
=========================

Qualcomm AI Engine Direct is also referred to as Qualcomm Neural Network (QNN) in the source code and documentation.
Qualcomm AI Engine Direct is a software development kit (SDK) for building AI based applications.
It provides tools and extensible per-accelerator libraries with uniform API,
enabling flexible integration and efficient execution of machine/deep learning networks on Qualcomm chipsets.

Contents
--------

- Converter tools to translate and optionally quantize source networks into sequence of QNN API calls.
- Per-accelerator backend libraries implementing QNN API
- OpPackage based backend extensibility
- Test tools to exercise backend libraries and converted networks
- Sample applications, OpPackage examples
- QNN SDK Reference Guide

Dependencies
------------

Point your web browser to ${QNN_SDK_ROOT}/docs/QNN/general/setup.html

=============
Release Notes
=============


2.13.0
======

**7/31/2023**

QNN API version: v2.7.2


Changelog
---------

Features
~~~~~~~~
* Tools:
   Converter:
     - Changed the logic for converting 1dOp into 2DOp by expanding along H dimension instead of W dimension.
     - Changed the translation of FloorDiv operator to ElementWiseDivide if the datatype of input is Int32.
     - GRU weights are shared across time unrolling step.
     - Added support for Float32 bias in Float16 execution
    Core:
     - User will be able to skip graph execution when there are multiple graphs present in a context.
* DSP:
    Op:
      - Added support for logSoftmax
* GPU:
   - Performance improvement on Kodiak and Cedros devices.
    Op:
      - Support broadcasting in ElementwiseSelect op.
      - Support 3D inputs in LayerNorm op.
      - Support broadcasting of batch dimensions in MatMul op.
      - Support reduction along batch for 4D inputs in Reduce Op.
      - Support  QNN_DATATYPE_INT_64 input datatype to Cast op.
      - Support inputs with rank < 4 and batch > 1 for rank=4 for LayerNorm op.
* HTP:
     - Introduce O3 Optimization.
     - Added support for 16 bit activations to ElementWiseSquaredDifference
    Op:
      - Added support for uint8 window7x7 stride3x3 maxpool ops.
      - Added support for GroupNorm
* SDK:
   - Added libQnnSystem.so for Hexagon targets.
   - Updated Pandas version in check-python-dependency script to 1.1.5
* OpDef:
   - Added op definition for Conv1D
   - Added op definition for TransposeConv1D
   - Added op definition for ElementWiseXor
   - Added op definition for DepthWiseConv1D
* SNPE SDK:
   - Add HtpPrepare.dll push step for HTP online prepare flow of windows tutorial (tutorial_inceptionv3_win).
* QNN SDK:
   - Add HtpPrepare.so push step in HTP section of android doc as only HTP offline prepare is mentioned here, better to leave a note here (htp_execution_tutorial_2.rst.in).
* API:
   - Added QNN_PROPERTY_GRAPH_SUPPORT_PER_API_PROFILING capability.
   - Added QNN_GRAPH_ERROR_GENERAL error code.
   - Added QnnSystemContext_getMetadata and deprecated QnnSystemContext_getBinaryInfo.
   - Added QNN_SIGNAL_ERROR_INCOMPATIBLE_SIGNAL_TYPE error code and clarified unconfigured QnnSignal behavior.
* Documents:
   - Update latest PyTorch Op support.
* MCP:
   - Combining IO DMA buffers as a perf optimization.
* CPU:
   - Fixed CollectRPNProposal kernel data passed.
* KI:
   - HTA BE support enabled for QRB5165.UBUN.2.0 targets based on GCC9.4 toolchain
Bugs
~~~~
* Tools:
   - Fixed conversion error when the bias_add having different bias shape with channel of preceding Conv.
   - Fixed a bug that matmul+add with matmul's dimension not 2 is mistakenly optimized.
    ONNX converter:
     - Support Split op in opset13, and keep the axes format in layernorm if  input_buffers axis_format is equal to node.op.data_axis_formats.
     - Fix the issue of onnx split translation.
    Quantizer:
     - Fixed the bug that caused the Static input tensors to use the weight_bw instead of activation bw by default
    Converter:
     - Enabled row wise and 4-bit quantization for MatMul Ops.
     - Fixed an error related to python type signature of c++ Set datastructure caused by python3.8 upgrade.
     - Some models with biasadd having bias tensor shape different than the channel shape of the preceding Conv will see failure during conversion in Opvalidation.
     - Fixed a bug that matmul+add with matmul's dimension not 2 is mistakenly optimized
    TF Converter:
     - Support optimized Gelu pattern that contains Mul instead of Realdiv.
     - Added support for conv2d_transpose layer with asymmetric strides
    KI:
     - Quantized models with LSTM Op will fail during inference.
     - Arch_checker will fail with an error related to python type signature of c++ Set datastructure.
* HTP:
   - Fixed vtcm oversize issue for large input node followed by a concat.
   - Add boundary check of gather_element's index generic implementation.
   - Repair bug in ReduceMean optimization during prepare.
   - Fixed issue with some models when preparing for FP16
   - Fixed set context_priority during qnn-throughput-net-run execution
   - Add RESOURCE_HVX flag for custom when using default Op registration. This fixed HVX stuck issue in Custom OP registration
    Op:
     - Fixed a vtcm overflow problem of large input depth matmul.
* DSP:
    Op:
     - Supported Reshape from 4d to 5d.
* SDK:
    SampleApp:
     - Fix issue where multi-target op package failed to load.


2.12.0
======

**6/30/2023**

QNN API version: v2.7.1


Changelog
---------
Features
~~~~~~~~
* Saver:
   - Added configuration option to control output filenames.
* OpDef:
   - Added op definition for ElementWiseBinary
   - Added optional parameter aligned to RoiAlign Op.
   - Added optional input batch splits and optional outputs batch splits, keeps, and keeps size to BoxWithNmsLimit Op.
   - Added optional parameter weights and optional output batch splits to AxisAlignedBboxTransform Op.
   - Added optional parameter allow_invalid_roi to RoiAlign Op.
   - Added optional parameter bbox_xform_clip to GenerateProposals Op.
   - Updated out[0] of DistributeFpnProposals to provide a -1 index value for invalid Rois.
   - Added Op definition for GroupNorm.
* Tool:
   - Support qnn-platform-validator on Windows
  qnn-net-run:
     - Added support for execution timeout
     - Support input tensor caching.
  Converter:
     - Added a new transformation to change MatMul into FullyConnected even without Bias.
     - Added a fix to account for the difference in the offset sign and usage when quantizing tensors
     - Modified the output names generated by Pytorch Converter and TFlite Converters
     - Changed the axis tracking behavior to match the TF & Onnx Converters.
     - Added support for new commandline argument to preserve the input layout and datatype as the source framework model
     - Added a new pattern to squash BatchNorm into FC + Reshape.
  Pytorch Converter:
     - Set model default input and output formats as spatial-first format (NHWC).
* GPU:
   OP:
     - Support 3D inputs in InstanceNorm op.
     - Support GELU operation.
* API:
   - Added QNN_GRAPH_ERROR_TIMED_OUT error code
   - Added QNN_COMMON_ERROR_RESOURCE_UNAVAILABLE error code.
* SDK:
   - Removed unused libPlatformValidatorShared.so artifacts.
* CPU:
   - Added depthwise+relu node fusion logic for INT8 ops.
   - Added 6D Support for Elementwise mul
   - Add allow_invalid_roi parameter in RoiAlign
   OP:
     - Added Support for ElementWiseNeuron
* HTP:
   - Added QNN signal timeout feature
   - Added backend extension support for extreme power saver performance profile mode
   - Added support for PD restart using FASTRPC_SESSION_CLOSE
   - Improved model loading times (FR78518)
   - Cleaned up use of QNN_ERROR_UNKNOWN_ERROR return code.
   - Added support for missing ElementWiseUnary operations: Abs, Asin, Atan, Ceil, Cos, Exp, Floor, Log
* DSP:
   - Supported absolute input value for MultiClassNMS operation.
* HTA:
   - Updated documentation for supported 16bit Ops.
Bugs
~~~~
* GPU:
   OP:
    - Fix bug in Squeeze Op validator which allowed unsupported dimensions
* HTP:
    - Fixed mem grow size cannot set to a smaller value issue.
    - Fix the scale limit of u8 elementwise addsub.
    - Fix the bug of passing down crouton_from_vtcm in dequantize.
    - Fixed undefined symbol for SecurePD QNN.
    - Improved performance of ElementWiseGreater op.
    - Fixed VTCM oversize issue with Gather op.
    - Fixed issue with serializing SpaceToDepth op.
    - Accuracy failure caused by tile misalignment (8b & 16b difference).
    - Improved model VTCM size dependent preparation robustness for FP16 precision.
* API:
   HTP:
     - Added support for QnnSignal timeout.
* Tools:
   qnn-net-run:
     - Fixed incorrect number of files being saved using --keep_num_outputs arg.
     - Correct the number of outputs generated when executing a static batched model with qnn-net-run in Async mode.
   ONNX converter:
     - Added support for constant data tensor as input to Gather Op when the index tensor is 0D (scalar).
     - Fixed Layernorm float dtype overrides, ensuring all tensors have same data type.
     - Fixed issue with Convert op wrongly inserted after a Dequantize op
   Converters:
     - Fixed issue related to Axes of Bias input to Conv Op
     - Fixed a bug where the inputs to Concat Op have different layouts
   Quantizer:
     - Fixed an error related to locking the WeakPtr associated with the Bias tensor to Convolution Op
     - Fixed an issue that prevented weights & bias inputs of Batchnorm from being set as FP16
* SDK:
   - Fixed SecurePD stack overflow issue
* DSP:
   - Fixed issue for loading context from binary getting wrong tensor input/output
* Saver:
   - Increase decimal precision when recording float values.


2.11.0
======

**5/31/2023**

QNN API version: v2.7.0


Changelog
---------

Features
~~~~~~~~
* Op:
    ONNX converter:
      - Added support for Mod
* OpDef:
    - Added op definition for ElementWiseNeuron
* SDK:
    - Added support API table in SDK documentation
    - Removed caffe support from qnn-quantization-checker, qnn-accuracy-evaluator, qnn-netron, and Golden-I
    - Upgraded Linux development host to Ubuntu 20.04 LTS
    - Upgraded Python support to version 3.8
    - Upgraded Android NDK version to 25c
    - Added support for Tensorflow version 2.10.1
    - Added support for ONNX version 1.11.0
    - update docs in SecurePD addon to reflect new directory structure
* API:
    - Added QnnSignal timeout configuration
    - Correct and add some error code returns
    - Added QNN_COMMON_ERROR_INCOMPATIBLE_BINARIES common error code
* HTP:
    - Reject second connection to QNN HTP BE libraries. libQnnHtpPrepare.so, libQnnHtpVXXStub.so, libQnnHtpVXXSkel.so are affected.
    - For x86 offline context binary generation, progress animation is added to indicate the generator still in progress.
    - ElementwiseUnary op support updates
* CPU:
    - INT8 support enabled for LA targets.
    - Removed DetectionOutput clipping
* Tools:
     Converter:
      - TensorFlow: Added support for ExtractPatches.
     TF Converter:
      - Added support for Tensorflow 2.10.1

Bugs
~~~~
* HTA:
    - Fix Concat Accuracy inside HTA Compiler
* HTP:
    - Fixed accuracy bug in Transpose-Reshape-Transpose op chain
    - Fixes DEF_OPTs related to VTCM movement surrounding the "ScatterInverse" op. Previously the related model would run into an op creation failure and not successfully prepare due to a downstream op which requires a TCM tensor type to get a non-TCM tensor type.
    - Fix QNN Graph finalize issues for certain models
    - Fix accuracy issue in FP16 layernorm operation
    - Fix graph finalize issues on certain floating point models
* SDK:
    - Fix doc bug for SecurePD QNN.
    - Fixed SecurePD stack overflow issue
* Tools:
    Converter:
      - Updated algorithm to handle axes transformation for Elementwise Ops and fixed a bug when squashing a Gather Op where output is same as input which would result in a KeyError
      - Fix conversion error when an operator's output is used as graph output and the UDO input at the same time
      - Fix the graphs output missing issue when the UDO output is used as graph output and the next operator's input at the same time.
      - Fixed ScatterElements quantization issue
    ONNX converter:
      - Keep the depthToSpace op's input and output axis format as NSC
* GPU:
    OP:
      - Fixed bug in Concat to change axis param from mandatory to optional.
* DSP:
   - Fixed bug for logger create.
   - Fixed op package generation issue.


2.10.40
=======

**5/10/2023**

QNN API version: v2.6.0


Changelog
---------

Features
~~~~~~~~
* HTP:
   - Set graph priority mappings to legacy pre-qnn-2.8.0 values
   - Added support for the backend platform options configuration
* API:
   - Added platform options backend configuration.
* SDK:
   - Made SDK structure updates related to unified software stack
   - Updated setup scripts and associated documentation
   - Made significant documentation content and style updates
   - Retired support for arm-android and qnn-caffe-converter, removed corresponding artifacts

Bugs
~~~~
* HTP:
   - Fixed an object use-after-free / segfault issue.


2.10.0
======

**4/28/2023**

QNN API version: v2.5.1


Changelog
---------

Features
~~~~~~~~
* OpDef:
    - Added op definition for ElementWiseMod.
    - Added Op definition for ElementWiseAsin.
    - Added op definition for ElementWiseFmod.
* API:
    - Added QNN_COMMON_ERROR_INCOMPATIBLE_BINARIES common error code.
* HTA:
    - Refactored  DepthwiseConv2d Op to support padding and dilation parameters.
* HTP:
    - made stricter constraints for moving indices of scatternd into vtcm to address accuracy loss in some models
    - added ROI Align Op broadcast support
* GPU:
    - Added support for QnnOpPackage_ImplementationV2_0_t.
    Op:
      - Support Pack operation with 1 input.
* DSP:
    - Remove DetectionOutput clipping (#866).
    Op:
      - Support Cast from BOOL_8 to UFIXED_POINT_8.
* CPU:
    - DeformConv2D op support.
    - Added Support for Mod.
    - Added support for ElementWiseUnary.
    - Fix double free in BoxWithNMSLimit due to dynamic output size.
    - Fix double free in GenerateProposal due to dynamic output size.
    - Support optional output in NMS.
    Op:
      - Elementwise Asin support.
* SDK:
    - Updated documentation to separate API and Operations sections.
    - Refine the Example XML OpDef Configs page in documentation
* Tools:
    Quantizer:
      - cleanup / fixes in LSTM op
    Converter:
      - Support elementwiseAsin op in converter.
      - Add Scatter/ScatterElements support in onnx converter.
      - Allow multiple outputs, if all same data type, in split like Ops, for support of mixed precision use cases
      - Added a new optimization sequence to convert BatchNorm into FullyConnected when applicable.
      - Add gather_nd support in tflite/pytorch converter.
      - Solve CenterNet conversion error.
    ONNX Converter:
      - Fix conversion issues for GRU op.

Bugs
~~~~
* HTP:
   - Stricter at constraint of moving indices into vtcm for scatternd at vivo's model.
   - Support Elementwise Sin/Cos with INT8 precision.
   - Improved batch to space performance in certain configurations.
   - Fix fail to finalize Graph on certain networks.

* DSP:
   - Fixed ElementWiseAdd performance issue.
   - Fix of backend features for multi-threads condition.
* Tools:
    - Support for CRNN model
    Converter:
      - Fix quantization override issue for tflite converter.
      - Fix Cast bug and update ArgOp/TransposeOp support.
      - Optimize Gather op's indices_buff in 'remove_identity'.
      - Fixed RoiAlign validator error for certain models.
    Quantizer:
      - Fixed issue with encodings not being consumed properly for PRelu op, due to name mismatches with original model.



2.9.0
======

**3/31/2023**

QNN API version: v2.5.0


Changelog
---------

Features
~~~~~~~~
* OpDef:
    - Added constraint for dilation > 0 in Convolution Ops.
    - Added ElementWiseUnary op definition.
    - Added op definition for NonMaxSuppression.
    - Constrained all ND inputs to have a rank greater than 0.
* CPU:
    - NonMaxSuppression op support
    Op:
      - Transpose Conv 3D support in CPU
* Tool:
    qnn-net-run:
      - Added keep_num_outputs option
    ONNX converter:
      - Added support for NonMaxSuppression op
    qnn-net-run:
      - Added batch_multiplier option
* SDK:
    - Added libQnnJsonProfilingReader.so
* HTP:
    - Optimized pad, transpose operations and VTCM utilization for certain network configurations
    - Fix accuracy issue for INT16 Div operation
    - Improve performance for GridSample operation
    Op:
      - Added support for NonMaxSuppression
* GPU:
    - Support context priority config.
    Op:
      - Support QNN_DATATYPE_FLOAT_16 datatype and non-multiple of 4 input size in Lstm op.
* API:
    - Added QNN_PROPERTY_GRAPH_SUPPORT_EXECUTE capability
* DSP:
    Op:
      - Added support for dilated conv3d
Bugs
~~~~
* CPU:
    - InstanceNorm fix 3d tensor support
* HTP:
    - Fixed accuracy issue in ReduceMax op.
    - Bug fixed for an unexpected error reported for certain graphs during execution with detailed profiling.
    - Fix tensor IDs being casted to a different data type before printing to logs.
    - Fix accuracy bug in 16bit LayerNorm implementation.
    Op:
      - Fix u16 mul crash cases when InA is in 111d format.
* Tool:
    qnn-op-package-generator:
      - Fix CPU OpPackage compilation error seen in 2.8.0
    Quantizer:
      - Fixed per-channel quantization failures caused by incorrect retrieval of static bias input tensors
    Converter:
      - Transpose Op optimization has bug in some cases which has been fixed.
      - User quantization overrides take precedence over external override JSON file values when generating graph
    Onnx:
      - Models with opset version <=11 with a Softmax on channel dimension and input > 2d may see an error running on 2MB VTCM HTP targets and GPU targets because of a required C*H*W reshape which results in a larger dimension
      - Added support for null tensor handling in Slice Op
* HTA:
    - Added validation for FC dimension. Y cannot be bigger than 1024 due to HTA HW support limitation.
* DSP:
    - Fixed Prelu_v2 repression issue
    - Fixed encoding op for ContextCreateFromBinary
    - Fixed op-package support issue on LE devices
    - Fixed softmax accuracy issue for SNPE2 DSP in dynamic encoding mode

2.8.0
======

**2/28/2023**

QNN API version: v2.4.0


Changelog
---------

Features
~~~~~~~~
* Tools:
    qnn-net-run:
      - Add native_input_tensor_names  option to specify native input file data types per input.
    qnn-context-binary-generator:
      - Added support for a context binary with multiple models.
    Quantizer:
      - Added support for quantized LSTMs
      - Added support for infinity
    Converters:
      Onnx:
        - Added support for Sign.
* API:
    - Added new QnnProfile event types to support QnnGraph_executeAsync profiling.
    - Add QnnGraph continuous profiling.
    - Add Qnn_Priority_t QNN_PRIORITY_NORMAL_HIGH.
* HTP:
    - Added a new priority "normal high" which is between normal and high priority levels.
    - Optimized int32 compare operations
    Op:
     - Added support for GridSample.
     - Added support for ElementWiseSign op.
* OpDef:
    - Added UINT32 support for in[1] in Gather op.
    - Added op definition for ElementWiseSign.
    - Clarify DetectionOutput::out[1] and align to backend behaviour.
* CPU:
    - Update BoxWithNMSLimit for static output size
    Op:
     - Add DistributeFPNProposal support
     - Added support for Sign op
     - Added Support for ExtractPatches Op
* DSP:
    - Offline prepare support on Windows QNN DSP
    Op:
     - Transpose5d hookup.
     - EltwiseAdd5D hookup
     - Reshape5D and RoiAlignV2 hookup

Bugs
~~~~
* Tools:
    Converters:
      - Resolved a bug in tracking consumers of a buffer when squashing Identity Op
      - Added the ability to add Bool8 tensor in converted .cpp files as String for QNN Converters
    ONNX Converter:
      - Fixed TransposeOp input axis format NT issue.
    loadqnn:
      - Fixed securepd client reorder option issue
* HTP:
    - Solve vtcm overflow issue happened when change data layout: from uint8 flat to uint8 crouton in tcm.
    - Fixed a race-condition in concurrent backend init/deinit calls.
    - Fixed accuracy issue in per-channel quantized DepthWiseConv2d op
    - Fixed issue with FP16 operations in some networks
    - Fixed issue with VTCM overflow in some networks
    - Fixed model preparation issue in some networks due to insufficient TCM size error
    - Fixed performance issue when model prepared with HVX threads higher than available in HW.
    - Fixed batch multiple support.
    - Improved inference time for networks with batch>1.
* DSP:
    - Fixed pad5d regression issue.
    - Fixed model execution issue due to reshape.
* HTA:
    - Added limitation of total Concat channel to 4096 when one of the channels is not aligned by 32.
    - Added validation for FC dimension. Y cannot be bigger than 1024 due to HTA HW support limitation.
* GPU:
    - Improved accuracy in FP16 mode with Kailua.LA.1.0-01005-STD.INT-1 META onwards.
    Op:
     - Support large dimensions in ReduceMean op.
* SDK:
    - Updated documentation for DSP backend.

2.7.0
======

**2/07/2023**

QNN API version: v2.3.2


Changelog
---------

Features
~~~~~~~~
* OpDef:
   - Added op definition for ExtractPatches.
   - Added INT32 support for in[1] in GatherNd op.
* CPU:
     - Fix output dim issue with fully connected op.
     - Added support for Uint32 in Index Tensor of Gather Op.
   OP:
     - PoolMax3D support.
     - Batch Permutation Op support.
     - Add CollectRPNProposals support in CPU.
     - Add support for MatMul bias optional input.
* Tool:
    qnn-net-run:
      - Support symmetric quantization.
      - Add input data type support for QNN_DATATYPE_SFIXED_POINT_8, QNN_DATATYPE_SFIXED_POINT_16, QNN_DATATYPE_SFIXED_POINT_32, and QNN_DATATYPE_UFIXED_POINT_32
      - Introduce use_native_input_files and use_native_output_files options. Deprecate the input_data_type and output_data_type options.
    qnn-context-binary-generator:
      - Add backend_binary option to output the backend specific cache.
    Converters:
      Onnx:
       - Added support for NonZero.
* API:
   - Deprecate Qnn_SocModel_t.
* DSP:
   - Updated enum names in QnnDspGraph_Encoding_t.
   - Added support for securepd on v66 target, subject to supported soc limitations.
   OP:
    - Added 5D support for Concat.
    - Added support for PoolMax3d.
* HTP:
   - Support u16 and fp16 GridSample in HTP.
   - Enable ElementwiseLess operation with INT32 precision.
   - Enable ElementwiseEqual operation with INT32 precision.
   - TopK now supports up to K <= 256 hardware accelerated.
* SDK:
   - Add V66 Secure PD.

Bugs
~~~~
* CPU:
   - Fixed a memory leak in math library.
   - Fix Memory leak observed in QML allocation.
   - Add int32 support for ElementWise Neg.
* HTP:
   - Fixed soc (miss)detection issue.
   - Fixed fully connected layer performance regression in some cases.
   - Fix potential double unmapping
   - Relax the restriction of slice_shape and conv fusion.
   - Fix missing nullptr check in perfsettings
   - Fixed memory leak occurred when log module is initialized multiple times.
   - Fix Graph Finalize issue on some graphs that use ElementwiseSquaredDifference operation.
   - Fix Graph Finalize issue on some graphs that use ReduceMean operation.
   - Solve memory leak while calling QnnLog_create and QnnLog_free with iterations.
   - Due to store buffer, memory order is not consistent with program order.
   - Fillmore FP16 test enablement is disabled.
   - Fixed with more tiling rules.
   - Fallback dil conv to ref implementation if inputs doesnt fit in vtcm and cant be tiled.
   - Consider padding when doing inplace concat.
* DSP:
   - Fixed context caching by changing add-tensor mechanism.
   - Solve DSP backend accuracy issue introduced by dynamic encoding enablement.
   - Fixed DSP backend does not support QNN_DATATYPE_UINT_8 datatype as input which cause validation failure.
   - Fixed model caching with tensor name for input tensors
   - Fixed undefined symbol in securepd
* HTA:
   - Activated verbose level as HTA level to produce detailed profile information. Execution time will be much slower by bigger graph.
   - Added validation for unsupported dimensions greater then 4D.
* Tools:
      - Fixed an if check which was missing the len() when checking for number of inputs to Elementwise Ops.
      - Fixed an assumption that Gamma/Beta are the 2nd input when squashing a Layernorm pattern.
      - ONNX Converter support GridSample op in SNPE & QNN
    Converters:
       - Fixed a bug in the optimization that merges Matmul + Reshape + Add to FC Op that would incorrectly insert the FC Op before the Constant Bias Op
       - Fixed a couple of bugs in the Converter
      Onnx:
       - Added support to translate GlobalAvgPool1D Op in the Converter.
       - Add a default_attrs param to function extract_attributes to get a default attributes if needed.
       - When x input is constant, allow DequantizeLinear and quantizeLinear caculate it's tensors.
* Op:
   GPU:
    - Fix graph prepare bug for large dimensions in Softmax op.


2.6.0
======

**12/30/2022**

QNN API version: v2.3.1


Changelog
---------

Features
~~~~~~~~
* OpDef:
   - Added Op definition for DistributeFpnProposals.
   - Added QNN_DATATYPE_INT_32 support for CropAndResize in[2].
   - Added QNN_DATATYPE_INT_32 support for ScatterNd in[1].
   - Added Op definition for Nonzero.
   - Added Op definition for CollectRpnProposals.
   - Added support for broadcasting in ElementWiseLess Op.
* CPU:
   - Added support for 3 dim input in instanceNorm op
   - Added 'Axes' parameter support in L2Norm op
   - Added dynamic tensor support for DepthWiseConv
   - Added support for ScatterElements Op
* HTP:
   - Graph option added to set number of HVX threads.
   - Config option enabled to read and set number of HVX threads using QNN apps.
   - Support v69 and v73 targets with HTP oppackage.
* Tools:
   Onnx converter:
    - Support transposeconv1d, map transposeconv1d to transposeconv2d
   Converters:
    - Changed output datatype of Argmax Op to Int32 from Uint32
* OP:
   CPU:
    - Added support for NonZero op
    - INT32 support for scatterND

Bugs
~~~~
* Tools:
   Tensorflow converter:
      - Fix the bugs of lstm with stacked cell.
   Onnx converter:
      - Models with opset version <=11 with a Softmax on channel dimension and input > 2d may see an error running on 2MB VTCM HTP targets and GPU targets because of a required C*H*W reshape which results in a larger dimension
      - Support ChannelShuffleOp's quantize encoding Inherit the encoding of the previous node.
* HTP:
   - Improved pytorch op MultiheadAttention performance when batch=1.
   - FP graphs is not supported on select SoCs.
* CPU:
   - Fixed padding parameter calculations in PoolAvg3d op
   - Fixed op validator issue in tile op
   - Fixed failure when adding CropAndResize op to the graph
   - Added dynamic tensor support for DepthWiseConv
* DSP:
   - Fixed multi-thread priority issue
   - Fix for model context binary with tensor name
   - Fixed backend terminate issue in multi-thread test case
   - Fixed RelSdkSymbolVisibilityChecker failure
* SDK:
   - Fixed issue observed set environment path repeatedly in Windows platform.
* OP:
   CPU:
    - Crop and resize op Support.


2.5.0
======

**11/30/2022**

QNN API version: v2.3.1


Changelog
---------

Features
~~~~~~~~
* CPU:
    - Added support for dynamic weights for TransposeConv2d.
    - Added support for INT32 in index tensor for Argmax Op.
    - Added INT32 data type support for Pack Op.
    - Add INT32 support for ElementWiseSelect op.
    - Add int32 and uint32 input support for Argmin and Argmax.
    - Added INT32 data type support for index tensors in ArgMin Op.
    - Added INT32 data type support for ElementWiseFloorDiv Op
    - Added support for 3 dim input in instanceNorm op.
* OpDef:
    - Added INT32 support for in[1] in GatherElements op.
    - Added INT32 support for out[0] in Argmax op.
    - Added Op definition for BatchPermutation.
    - Added INT32 support for out[0] in Argmin op.
* HTP:
    - Added a HTP specific profiling level in qnn-net-run.
* Tools:
    - Added qnn-accuracy-evaluator. This tool helps to automatically run different model config setups and compare the output results to get the best setup config. (experimental)
    - Added Architecture Checker tool to QNN SDK. Available as command line option to converters. (experimental)
    - Added qnn-quantization-checker tool to QNN SDK (experimental)
    - Added qnn-netron GUI tool to QNN SDK.
   Converter:
     ONNX:
        - Add ElementWise Softplus support.
* Op:
     HTP:
      - Speed up dynamic depthwise convolution with uint8 weights.

Bugs
~~~~
* HTP:
   - Fix vtcm overflow caused by softmax and onehot which have a large depth.
   - Fixed accuracy regression in few models using masked-multiplication FP16 Op.
   - Solve vtcm overflow for transposeconv2d layer whose groups > 1, in depth= out depth, padding =0 and groups != in depth.
   - Mitigated runtime crash due to potential memory corruption (54195)
   - Repair accuracy bug in element wise operations.
* DSP:
   - Fixed QnnProperty_hasCapability to be callable independent of QnnBackend being created.
   - Cache tensor info on tensor create for use in subsequent APIs.
   - Fixed soc (miss)detection issue.
   - Fixed issue in QnnContext_setConfig related to setting priority before graph creation.
   - Fixed the calculation of zero point used for dilated convolution with stride greater than 1.
   - Fix the bug of get output info from the opconfig when add node in DSP.
* Tool:
   Converter:
      - Fixed bugs when select(where) Op have three inputs.
      ONNX:
        - Allowed constant tensor encodings to be equal to the overridden output tensor encodings when bit width=4.
   qnn-netron:
      - Fixed issue causing differences not being presented properly for some models.
      - Fixed dependency script bug with nodejs installation version mis-match.
   Tensorflow converter:
      - Fixed issues with per-channel quantization of weights: set is_symmetric = true by default, added param "axis" and "is_symmetric" into weight encodings info.
      - Fix the bugs of lstm with stacked cell.
   Quantizer:
      - Fixed issue with quantization of weights and biases in Conv3d Op due to squashing with Relu.
* HTA:
    - Fixed Reshape op validator to reflect support for only equal Input and Output dimensions.
    - Fixed issue with detailed profiling information not being produced.
* OP:
    GPU:
      - Fixed Convolution Op configuration to resolve accuracy issues.
      - Fix Concat graph finalize failures on Fillmore and Kodiak devices.
      - Fix concat op having input rank = 4 and axis = 0 validation error on low tier devices.


2.4.0
======

**10/31/2022**

QNN API version: v2.3.0


Changelog
---------

Features
~~~~~~~~
* DSP:
    Op:
      - Support broadcasting for ElementWiseSelect.
* CPU:
   - Added support for broadcasting in ElementWiseSelect Op.
   - GridSample op Support.
* Tools:
    qnn-sample-app:
      - Added support for QnnDevice create and free APIs.
    qnn-net-run:
      - Add duration and num_inferences command line options.
      - Add support for int64/uint64 graph input and outputs.
* API:
    - Introduction of the QnnSignal API.
    - Add support for QNN_SOC_MODEL_SM8325.
    - Added QNN_PROPERTY_GRAPH_SUPPORT_FINALIZE_SIGNAL, QNN_PROPERTY_GRAPH_SUPPORT_EXECUTE_SIGNAL, and QNN_PROPERTY_GRAPH_SUPPORT_EXECUTE_ASYNC_SIGNAL capabilities.
* OpDef:
    - Added Op definition for ScatterElements.
    - Added support for broadcasting in ElementwiseSelect Op.
* GPU: 
   - Fixed Concat Op configuration and validation logic.

Bugs
~~~~
* GPU:
   - Fixed init time regressions when using kernel cache.
   - Fixed soc (miss)detection issue.
* OpDef:
   - Remove incorrect shape constraints for Tile out[0] and multiples param.
* HTP:
   - Updated the core code to export an additional symbol to default visibility for op package integration.
* Tools:
    Quantizer:
       - Fixed bug caused by incorrectly added Convert operation for non-quantized data type conversions.
* CPU:
   - Fixed soc (miss)detection issue.



2.3.0
======

**09/30/2022**

QNN API version: v2.2.0


Changelog
---------

Features
~~~~~~~~
* CPU:
      - Added dynamic tensor support for TransposeConv2D.
    Op:
      - Added support for Shape op.
      - Added support for ConstantOfShape op.
* API:
   - Updated QnnGraph_executeAsync() behavior to block until the execution is enqueued rather than returning early if the queue is full.
   - Clarified behavior with concurrent calls to QnnGraph_execute() and QnnGraph_executeAsync()
   - Introduced a queue depth context config to control the maximum depth of the async execution queue.
   - Remove deprecated QnnGpuBackend_CustomConfig_t from QnnGpuBackend.h
   - Moved default QNN_API definition to QnnCommon.h
* Tools:
    Converters:
      Onnx:
        - Added 5D tensor support for PoolMax3d.
        - Added 5D tensor support for Resize.
        - Added 5D tensor support for PoolAvg3d.
    qnn-net-run:
      - Added support for execution via QnnGraph_executeAsync(), this will be the default mode of execution if supported by a backend.
* HTA:
   - Introduced backend with API 2.x support.
   - Add validation of HW limitation for FC layer.
* DSP:
   - Introduced backend with API 2.x support.
* HTP:
    Op:
      - Added 5D support to ElementWisePower.

Bugs
~~~~
* HTP:
   - Fixed vtcm estimation for axis=3 concat. Now input tensors are also taken into account if concat is not inplaced.
   - Fixed issue with float models containing Reduce Mean op not handling batch > 1 accurately.
   - Bug fix to handle graph finalize issues for certain ML models.
* HTA:
   - Fix wrong return of API error code.
* CPU:
   - Add INT64 support for cast op.
   - Improved CPU BE performance on Windows.
* GPU:
    Op:
     - Fix bug in InstanceNorm validation that fails when passing in normalize_variance param.
     - Fix bug in Tile validator for tiling across batch dimension for input rank >= 4
* Tools:
    Quantizer:
      - Fixed issue observed with int4 weight override support.


2.1.0
======

**08/04/2022**

QNN API version: v2.1.0

- Added QNN_SOC_MODEL_SXR1230P, QNN_SOC_MODEL_SSG2115P, and QNN_SOC_MODEL_SM6450.

Changelog
---------

Features
~~~~~~~~
* OpDef:
    - Added GRU op definition.
* Tools:
    Converters:
      Onnx:
        TensorFlow:
          - Added 5D tensor support for Conv3D.
* DSP:
   Op:
      - support CastUint32toFloat32.
      - support FloorDiv.

Bugs
~~~~
* HTP:
   - Updated rules to properly handle dequantize followed quantize operation.
   - Fixed the dequantize followed by slicepad sequence issue.
   
* Tool:
    qnn-throughput-net-run:
      - fixed potential memory leak issue with profile object allocation.

2.0.0
======

**07/07/2022**

QNN API version: v2.0.0

- QnnInterface:
    - QnnInterface_getProviders function signature update.

- QnnTypes:
    - Qnn_Tensor_t data structure update:
        - Add versioning (i.e. Qnn_TensorV1_t).
        - Add name field. ID field is now backend generated.
        - Consolidate max and current dimensions into one field.
        - INT4 support (see Qnn_BwScaleOffset_t and Qnn_BwAxisScaleOffset_t).
    - Qnn_OpConfig_t data structure update:
        - Add versioning (i.e. Qnn_OpConfigV1_t).
    - Added Qnn_SocModel_t.

- QnnTensor:
    - Qnn_Tensor_t is now an output argument to QnnTensor_createContextTensor and
      QnnTensor_createGraphTensor since the ID is now generated by the backend from the name.
    - Added QNN_TENSOR_ERROR_NAME_HASH_COLLISION error code.

- QnnDevice introduction:
    - Adds multi-core support.

- QnnBackend:
    - Introduce Qnn_BackendHandle_t.
    - These APIs now take a Qnn_BackendHandle_t as an argument:
        - QnnBackend_registerOpPackage
        - QnnBackend_validateOpConfig
        - QnnBackend_registerOpPackag
    - QnnBackend_initialize replaced by QnnBackend_create.
    - QnnBackend_terminate replaced by QnnBackend_free.
    - Added QnnBackend_getSupportedOperations and QnnBackend_OperationName_t.
    - Removed QnnBackend_getPerfInfrastructure (see QnnDevice_getInfrastructure).
    - Added and removed a variety of error codes.

- QnnMem:
    - QnnMem_register now take a Qnn_ContextHandle_t as an argument.
    - Add backend specific memory registration extensions.

- QnnContext:
    - Increased maximum context binary size to 64-bit.
    - Consolidate QnnContext_createFromBinary and QnnContext_createFromBinaryWithConfig.
    - QnnContext_create and QnnContext_createFromBinary function signature updates:
        - Qnn_BackendHandle_t association.
        - Qnn_DeviceHandle_t association.

- QnnLog:
    - Introduce Qnn_LogHandle_t.
    - QnnLog_setLogLevel now takes a Qnn_LogHandle_t as an argument.
    - QnnLog_initialize replaced by QnnLog_create.
    - QnnLog_terminate replaced by QnnLog_free.
    - Qnn_LogHandle_t is associated to a Qnn_BackendHandle_t in QnnBackend_create.
    - Added and removed a variety of error codes.

- QnnProperty:
    - Removed QnnProperty_get and QnnProperty_free.
    - Removed the following capability keys:
        - QNN_PROPERTY_BACKEND_SUPPORT_BUILD_ID
        - QNN_PROPERTY_BACKEND_SUPPORT_PERF_INFRASTRUCTURE
        - QNN_PROPERTY_BACKEND_SUPPORT_OP_VALIDATION
        - QNN_PROPERTY_CONTEXT_SUPPORT_GET_BINARY
        - QNN_PROPERTY_CONTEXT_SUPPORT_GET_BINARY_SIZE
        - QNN_PROPERTY_CONTEXT_SUPPORT_CREATE_BINARY
    - Added the following capability keys:
        - QNN_PROPERTY_CONTEXT_SUPPORT_CACHING
        - QNN_PROPERTY_GRAPH_SUPPORT_PRIORITY_CONTROL
        - QNN_PROPERTY_GROUP_DEVICE
        - QNN_PROPERTY_DEVICE_SUPPORT_INFRASTRUCTURE
        - QNN_PROPERTY_GRAPH_SUPPORT_PRIORITY_CONTROL
    - Added and removed a variety of error codes.

- QnnGraph:
    - Add priority configuration.
    - Add QnnGraph_setConfig API.

- QnnProfile:
    - QnnProfile_create associated with a Qnn_BackendHandle_t.

- QnnOpPackage:
    - Introduce Qnn_OpPackageHandle_t.
    - Introduce 2.0 interface to the backend.
    - Removed the QNN_OP_PACKAGE_API_VERSION_* macros and replaced them with 
      QNN_OP_PACKAGE_API_VERSION_1_4_0 and QNN_OP_PACKAGE_API_VERSION_2_0_0.

- QnnSystem:
    - QnnSystemInterface_getProviders function signature update.
    - QnnSystemContext_getBinaryInfo function signature update for const output.
    - Added QnnSystemContext_BinaryInfoV2_t to support QnnDevice.

- QnnOpDef:
    - Added op set version.

- Other:
    - Prune header inclusions.

