

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Tools &mdash; Qualcomm® AI Engine Direct</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Converters" href="converters.html" />
    <link rel="prev" title="Example XML OpDef Configs" href="example_op_defs.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® AI Engine Direct
          

          
          </a>

          
            
            
              <div class="version">
                v2.13.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="backend.html">Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_packages.html">Op Packages</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tools</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-conversion">Model Conversion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#qnn-tensorflow-converter">qnn-tensorflow-converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-tflite-converter">qnn-tflite-converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-pytorch-converter">qnn-pytorch-converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-onnx-converter">qnn-onnx-converter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-preparation">Model Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quantization-support">Quantization Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-model-lib-generator">qnn-model-lib-generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-op-package-generator">qnn-op-package-generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-context-binary-generator">qnn-context-binary-generator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#execution">Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#qnn-net-run">qnn-net-run</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-throughput-net-run">qnn-throughput-net-run</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#analysis">Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#qnn-quantization-checker-experimental">qnn-quantization-checker (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#viewing-the-results-html-csv-or-log-files">Viewing the results (html, csv or log files)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-accuracy-evaluator-experimental">qnn-accuracy-evaluator (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#minimal-mode">Minimal Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#config-mode">Config Mode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#architecture-checker-experimental">Architecture Checker (Experimental)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-accuracy-debugger-experimental">qnn-accuracy-debugger (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#step-1-framework-diagnosis">Step 1: Framework Diagnosis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-2-inference-engine">Step 2: Inference Engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-3-verification">Step 3: Verification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-qnn-accuracy-debugger-e2e">Run QNN Accuracy Debugger E2E</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-platform-validator">qnn-platform-validator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-profile-viewer">qnn-profile-viewer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-netron-beta">qnn-netron (Beta)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#launching-tool">Launching Tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qnn-netron-visualize-deep-dive">QNN Netron Visualize Deep Dive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#netron-diff-customization-deep-dive">Netron Diff Customization Deep Dive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference-vs-inference">Inference vs Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#results-and-outputs">Results and Outputs:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#performance-and-accuracy-diff-visualizations">Performance and Accuracy Diff Visualizations:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qnn-netron-diff-navigation">QNN Netron Diff Navigation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® AI Engine Direct</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Tools</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tools">
<h1>Tools<a class="headerlink" href="#tools" title="Permalink to this headline">¶</a></h1>
<p>This page describes the various SDK tools and feature for Linux/Android and Windows developers.
For the integration flow of different developers, please refer to <a class="reference internal" href="overview.html"><span class="doc">Overview</span></a> page for further information.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 17%" />
<col style="width: 32%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 14%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" rowspan="3"><p>Category</p></th>
<th class="head" rowspan="3"><p>Tool</p></th>
<th class="head" colspan="6"><p>Developer</p></th>
</tr>
<tr class="row-even"><th class="head" colspan="3"><p>Linux/Android</p></th>
<th class="head" colspan="3"><p>Windows</p></th>
</tr>
<tr class="row-odd"><th class="head"><p>Ubuntu</p></th>
<th class="head"><p>WSL x86</p></th>
<th class="head"><p>Device</p></th>
<th class="head"><p>WSL x86</p></th>
<th class="head"><p>Native Windows x86</p></th>
<th class="head"><p>Device</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="4"><p><a class="reference internal" href="#model-conversion">Model Conversion</a></p></td>
<td><p><a class="reference internal" href="#qnn-tensorflow-converter">qnn-tensorflow-converter</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-tflite-converter">qnn-tflite-converter</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#qnn-pytorch-converter">qnn-pytorch-converter</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-onnx-converter">qnn-onnx-converter</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td rowspan="4"><p><a class="reference internal" href="#model-preparation">Model Preparation</a></p></td>
<td><p><a class="reference internal" href="#quantization-support">Quantization Support</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-model-lib-generator">qnn-model-lib-generator</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td><p>YES</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#qnn-op-package-generator">qnn-op-package-generator</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-context-binary-generator">qnn-context-binary-generator</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
</tr>
<tr class="row-even"><td rowspan="2"><p><a class="reference internal" href="#execution">Execution</a></p></td>
<td><p><a class="reference internal" href="#qnn-net-run">qnn-net-run</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-throughput-net-run">qnn-throughput-net-run</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td><p>YES</p></td>
</tr>
<tr class="row-even"><td rowspan="8"><p><a class="reference internal" href="#analysis">Analysis</a></p></td>
<td><p><a class="reference internal" href="#qnn-quantization-checker-experimental">qnn-quantization-checker (Experimental)</a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-accuracy-evaluator-experimental">qnn-accuracy-evaluator (Experimental)</a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#architecture-checker-experimental">Architecture Checker (Experimental)</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td colspan="2"><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-accuracy-debugger-experimental">qnn-accuracy-debugger (Experimental)</a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td colspan="2"><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#qnn-platform-validator">qnn-platform-validator</a></p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-profile-viewer">qnn-profile-viewer</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES*</p></td>
<td><p>YES*</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="benchmarking.html"><span class="doc">Benchmarking</span></a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-netron-beta">qnn-netron (Beta)</a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Notes</strong></p>
<ul class="simple">
<li><p>For more detailed information on converters please refer to <a class="reference internal" href="converters.html"><span class="doc">Converters</span></a>.</p></li>
<li><p>For Windows developers, please replace all ‘.so’ files with the analogous ‘.dll’ file in the following sections. Please refer to <a class="reference internal" href="introduction.html#platform-differences"><span class="std std-ref">Platform Differences</span></a> for more details.</p></li>
<li><p>[*] libQnnGpuProfilingReader.dll is not supported on Windows platform for qnn-profile-viewer.</p></li>
</ul>
<div class="section" id="model-conversion">
<h2>Model Conversion<a class="headerlink" href="#model-conversion" title="Permalink to this headline">¶</a></h2>
<div class="section" id="qnn-tensorflow-converter">
<h3>qnn-tensorflow-converter<a class="headerlink" href="#qnn-tensorflow-converter" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-tensorflow-converter</strong> tool converts a model from the TensorFlow framework to
a CPP file representing the model as a series of QNN API calls. Additionally,
a binary file containing static weights of the model is produced.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">tensorflow</span><span class="o">-</span><span class="n">converter</span><span class="w"> </span><span class="o">-</span><span class="n">d</span><span class="w"> </span><span class="n">INPUT_NAME</span><span class="w"> </span><span class="n">INPUT_DIM</span><span class="w"> </span><span class="o">--</span><span class="n">out_node</span><span class="w"> </span><span class="n">OUT_NAMES</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">input_type</span><span class="w"> </span><span class="n">INPUT_NAME</span><span class="w"> </span><span class="n">INPUT_TYPE</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">input_dtype</span><span class="w"> </span><span class="n">INPUT_NAME</span><span class="w"> </span><span class="n">INPUT_DTYPE</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">input_encoding</span><span class="w"> </span><span class="n">INPUT_ENCODING</span><span class="w"> </span><span class="p">[</span><span class="n">INPUT_ENCODING</span><span class="w"> </span><span class="p">...]]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">input_layout</span><span class="w"> </span><span class="n">INPUT_NAME</span><span class="w"> </span><span class="n">INPUT_LAYOUT</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">custom_io</span><span class="w"> </span><span class="n">CUSTOM_IO</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">show_unconsumed_nodes</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">saved_model_tag</span><span class="w"> </span><span class="n">SAVED_MODEL_TAG</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">saved_model_signature_key</span><span class="w"> </span><span class="n">SAVED_MODEL_SIGNATURE_KEY</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">quantization_overrides</span><span class="w"> </span><span class="n">QUANTIZATION_OVERRIDES</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">keep_quant_nodes</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">disable_batchnorm_folding</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">keep_disconnected_nodes</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">INPUT_LIST</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">param_quantizer</span><span class="w"> </span><span class="n">PARAM_QUANTIZER</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">act_quantizer</span><span class="w"> </span><span class="n">ACT_QUANTIZER</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">algorithms</span><span class="w"> </span><span class="n">ALGORITHMS</span><span class="w"> </span><span class="p">[</span><span class="n">ALGORITHMS</span><span class="w"> </span><span class="p">...]]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">bias_bw</span><span class="w"> </span><span class="n">BIAS_BW</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">act_bw</span><span class="w"> </span><span class="n">ACT_BW</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">weight_bw</span><span class="w"> </span><span class="n">WEIGHT_BW</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">ignore_encodings</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">use_per_row_quantization</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">use_per_channel_quantization</span><span class="w"> </span><span class="p">[</span><span class="n">USE_PER_CHANNEL_QUANTIZATION</span><span class="w"> </span><span class="p">[</span><span class="n">USE_PER_CHANNEL_QUANTIZATION</span><span class="w"> </span><span class="p">...]]]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">use_native_input_files</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">use_native_dtype</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">use_native_output_files</span><span class="p">]</span><span class="w"> </span><span class="o">--</span><span class="n">input_network</span><span class="w"> </span><span class="n">INPUT_NETWORK</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">debug</span><span class="w"> </span><span class="p">[</span><span class="n">DEBUG</span><span class="p">]]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">OUTPUT_PATH</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">copyright_file</span><span class="w"> </span><span class="n">COPYRIGHT_FILE</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">float_bw</span><span class="w"> </span><span class="n">FLOAT_BW</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">overwrite_model_prefix</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">exclude_named_tensors</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">op_package_lib</span><span class="w"> </span><span class="n">OP_PACKAGE_LIB</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">-</span><span class="n">p</span><span class="w"> </span><span class="n">PACKAGE_NAME</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="o">--</span><span class="n">op_package_config</span><span class="w"> </span><span class="n">CUSTOM_OP_CONFIG_PATHS</span><span class="w"> </span><span class="p">[</span><span class="n">CUSTOM_OP_CONFIG_PATHS</span><span class="w"> </span><span class="p">...]]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">arch_checker</span><span class="p">]</span>
</pre></div>
</div>
<p>Script to convert TF model into QNN</p>
<blockquote>
<div><dl>
<dt>required arguments:</dt><dd><dl>
<dt>-d INPUT_NAME INPUT_DIM, –input_dim INPUT_NAME INPUT_DIM</dt><dd><p>The names and dimensions of the network input layers specified in the format
[input_name comma-separated-dimensions], for example:</p>
<blockquote>
<div><p>‘data’ 1,224,224,3</p>
</div></blockquote>
<p>Note that the quotes should always be included in order to
handlespecial characters, spaces, etc.
For multiple inputs specify multiple –input_dim on the command line like:</p>
<blockquote>
<div><p>–input_dim ‘data1’ 1,224,224,3 –input_dim ‘data2’ 1,50,100,3</p>
</div></blockquote>
</dd>
<dt>–out_node OUT_NODE,  –out_name OUT_NAMES</dt><dd><p>Name of the graph’s output nodes. Multiple output nodes should be
provided separately like:</p>
<blockquote>
<div><p>–out_node out_1 –out_node out_2</p>
</div></blockquote>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--input_network <var>INPUT_NETWORK</var></span>, <span class="option">-i <var>INPUT_NETWORK</var></span></kbd></dt>
<dd><p>Path to the source framework model.</p>
</dd>
</dl>
</dd>
<dt>optional arguments:</dt><dd><dl>
<dt>–input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE</dt><dd><p>Type of data expected by each input op/layer. Type for each input is
<a href="#id5"><span class="problematic" id="id6">|default|</span></a> if not specified. For example: “data” image.Note that the quotes
should always be included in order to handle special characters, spaces,etc.
For multiple inputs specify multiple –input_type on the command line.
Eg:</p>
<blockquote>
<div><p>–input_type “data1” image –input_type “data2” opaque</p>
</div></blockquote>
<p>These options get used by DSP runtime and following descriptions state how
input will be handled for each option.
Image:
Input is float between 0-255 and the input’s mean is 0.0f and the input’s
max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
DSP.
Default:
Pass the input as floats to the dsp directly and the DSP will quantize it.
Opaque:
Assumes input is float because the consumer layer(i.e next layer) requires
it as float, therefore it won’t be quantized.
Choices supported:</p>
<blockquote>
<div><p>image
default
opaque</p>
</div></blockquote>
</dd>
<dt>–input_dtype INPUT_NAME INPUT_DTYPE</dt><dd><p>The names and datatype of the network input layers specified in the format
[input_name datatype], for example:</p>
<blockquote>
<div><p>‘data’ ‘float32’.</p>
</div></blockquote>
<p>Default is float32 if not specified.
Note that the quotes should always be included in order to handle special
characters, spaces, etc.
For multiple inputs specify multiple –input_dtype on the command line like:</p>
<blockquote>
<div><p>–input_dtype ‘data1’ ‘float32’ –input_dtype ‘data2’ ‘float32’</p>
</div></blockquote>
</dd>
<dt>–input_encoding INPUT_ENCODING [INPUT_ENCODING …], -e INPUT_ENCODING [INPUT_ENCODING …]</dt><dd><p>Usage:     –input_encoding “INPUT_NAME” INPUT_ENCODING_IN
[INPUT_ENCODING_OUT]
Input encoding of the network inputs. Default is bgr.
e.g.</p>
<blockquote>
<div><p>–input_encoding “data” rgba</p>
</div></blockquote>
<p>Quotes must wrap the input node name to handle special characters,
spaces, etc. To specify encodings for multiple inputs, invoke
–input_encoding for each one.
e.g.</p>
<blockquote>
<div><p>–input_encoding “data1” rgba –input_encoding “data2” other</p>
</div></blockquote>
<p>Optionally, an output encoding may be specified for an input node by
providing a second encoding. The default output encoding is bgr.
e.g.</p>
<blockquote>
<div><p>–input_encoding “data3” rgba rgb</p>
</div></blockquote>
<dl class="simple">
<dt>Input encoding types:</dt><dd><p>image color encodings: bgr,rgb, nv21, nv12, …
time_series: for inputs of rnn models;
other: not available above or is unknown.</p>
</dd>
<dt>Supported encodings:</dt><dd><p>bgr
rgb
rgba
argb32
nv21
nv12
time_series
other</p>
</dd>
</dl>
</dd>
<dt>–input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT</dt><dd><p>Layout of each input tensor. If not specified, it will use the default
based on the Source Framework, shape of input and input encoding.
Accepted values are-</p>
<blockquote>
<div><p>NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL</p>
</div></blockquote>
<p>N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
NDHWC/NCDHW used for 5d inputs
NHWC/NCHW used for 4d image-like inputs
NFC/NCF used for inputs to Conv1D or other 1D ops
NTF/TNF used for inputs with time steps like the ones used for LSTM op
NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
F used for 1D inputs, e.g. Bias tensor
NONTRIVIAL for everything elseFor multiple inputs specify multiple
–input_layout on the command line.
Eg:</p>
<blockquote>
<div><p>–input_layout “data1” NCHW –input_layout “data2” NCHW</p>
</div></blockquote>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--custom_io <var>CUSTOM_IO</var></span></kbd></dt>
<dd><p>Use this option to specify a yaml file for custom IO</p>
</dd>
<dt><kbd><span class="option">--show_unconsumed_nodes</span></kbd></dt>
<dd><p>Displays a list of unconsumed nodes, if there any are found. Nodes which are
unconsumed do not violate the structural fidelity of thegenerated graph.</p>
</dd>
<dt><kbd><span class="option">--saved_model_tag <var>SAVED_MODEL_TAG</var></span></kbd></dt>
<dd><p>Specify the tag to seletet a MetaGraph from savedmodel. ex:
–saved_model_tag serve. Default value will be ‘serve’ when it is not
assigned.</p>
</dd>
<dt><kbd><span class="option">--saved_model_signature_key <var>SAVED_MODEL_SIGNATURE_KEY</var></span></kbd></dt>
<dd><p>Specify signature key to select input and output of the model. ex:
–saved_model_signature_key serving_default. Default value will be
‘serving_default’ when it is not assigned</p>
</dd>
</dl>
<p>–disable_batchnorm_folding
–keep_disconnected_nodes</p>
<blockquote>
<div><p>Disable Optimization that removes Ops not connected to the main graph.
This optimization uses output names provided over commandline OR
inputs/outputs extracted from the Source model to determine the main graph</p>
</div></blockquote>
<p>–debug [DEBUG]       Run the converter in debug mode.
-o OUTPUT_PATH, –output_path OUTPUT_PATH</p>
<blockquote>
<div><p>Path where the converted Output model should be saved.If not specified, the
converter model will be written to a file with same name as the input model</p>
</div></blockquote>
<dl class="option-list">
<dt><kbd><span class="option">--copyright_file <var>COPYRIGHT_FILE</var></span></kbd></dt>
<dd><p>Path to copyright file. If provided, the content of the file will be added
to the output model.</p>
</dd>
<dt><kbd><span class="option">--float_bw <var>FLOAT_BW</var></span></kbd></dt>
<dd><p>Use the –float_bw option to select the bitwidth to use when using float for
parameters(weights/bias) and activations for all ops  or specific Op (via
encodings) selected through encoding, either 32 (default) or 16.</p>
</dd>
<dt><kbd><span class="option">--overwrite_model_prefix</span></kbd></dt>
<dd><p>If option passed, model generator will use the output path name to use as
model prefix to name functions in &lt;qnn_model_name&gt;.cpp. (Useful for running
multiple models at once) eg: ModelName_composeGraphs. Default is to use
generic “<a href="#id7"><span class="problematic" id="id8">QnnModel_</span></a>”.</p>
</dd>
<dt><kbd><span class="option">--exclude_named_tensors</span></kbd></dt>
<dd><p>Remove using source framework tensorNames; instead use a counter for naming
tensors. Note: This can potentially help to reduce the final model library
that will be generated(Recommended for deploying model). Default is False.</p>
</dd>
<dt><kbd><span class="option">-h</span>, <span class="option">--help</span></kbd></dt>
<dd><p>show this help message and exit</p>
</dd>
</dl>
</dd>
<dt>Quantizer Options:</dt><dd><dl class="option-list">
<dt><kbd><span class="option">--quantization_overrides <var>QUANTIZATION_OVERRIDES</var></span></kbd></dt>
<dd><p>Use this option to specify a json file with parameters to use for
quantization. These will override any quantization data carried from
conversion (eg TF fake quantization) or calculated during the normal
quantization process. Format defined as per AIMET specification.</p>
</dd>
<dt><kbd><span class="option">--keep_quant_nodes</span></kbd></dt>
<dd><p>Use this option to keep activation quantization nodes in the graph rather
than stripping them.</p>
</dd>
<dt><kbd><span class="option">--input_list <var>INPUT_LIST</var></span></kbd></dt>
<dd><p>Path to a file specifying the input data. This file should be a plain text
file, containing one or more absolute file paths per line. Each path is
expected to point to a binary file containing one input in the “raw” format,
ready to be consumed by the quantizer without any further preprocessing.
Multiple files per line separated by spaces indicate multiple inputs to the
network. See documentation for more details. Must be specified for
quantization. All subsequent quantization options are ignored when this is
not provided.</p>
</dd>
<dt><kbd><span class="option">--param_quantizer <var>PARAM_QUANTIZER</var></span></kbd></dt>
<dd><p>Optional parameter to indicate the weight/bias quantizer to use. Must be
followed by one of the following options: “tf”: Uses the real min/max of the
data and specified bitwidth (default) “enhanced”: Uses an algorithm useful
for quantizing models with long tails present in the weight distribution
“adjusted”: Uses an adjusted min/max for computing the range, particularly
good for denoise models “symmetric”: Ensures min and max have the same
absolute values about zero. Data will be stored as int#_t data such that the
offset is always 0.</p>
</dd>
<dt><kbd><span class="option">--act_quantizer <var>ACT_QUANTIZER</var></span></kbd></dt>
<dd><p>Optional parameter to indicate the activation quantizer to use. Must be
followed by one of the following options: “tf”: Uses the real min/max of the
data and specified bitwidth (default) “enhanced”: Uses an algorithm useful
for quantizing models with long tails present in the weight distribution
“adjusted”: Uses an adjusted min/max for computing the range, particularly
good for denoise models “symmetric”: Ensures min and max have the same
absolute values about zero. Data will be stored as int#_t data such that the
offset is always 0.</p>
</dd>
</dl>
<dl class="simple">
<dt>–algorithms ALGORITHMS [ALGORITHMS …]</dt><dd><p>Use this option to enable new optimization algorithms. Usage is:
–algorithms &lt;algo_name1&gt; … The available optimization algorithms are:
“cle” - Cross layer equalization includes a number of methods for equalizing
weights and biases across layers in order to rectify imbalances that cause
quantization errors. “bc” - Bias correction adjusts biases to offset
activation quantization errors. Typically used in conjunction with “cle” to
improve quantization accuracy.</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--bias_bw <var>BIAS_BW</var></span></kbd></dt>
<dd><p>Use the –bias_bw option to select the bitwidth to use when quantizing the
biases, either 8 (default) or 32.</p>
</dd>
<dt><kbd><span class="option">--act_bw <var>ACT_BW</var></span></kbd></dt>
<dd><p>Use the –act_bw option to select the bitwidth to use when quantizing the
activations, either 8 (default) or 16.</p>
</dd>
<dt><kbd><span class="option">--weight_bw <var>WEIGHT_BW</var></span></kbd></dt>
<dd><p>Use the –weight_bw option to select the bitwidth to use when quantizing the
weights, currently only 8 bit (default) supported.</p>
</dd>
<dt><kbd><span class="option">--ignore_encodings</span></kbd></dt>
<dd><p>Use only quantizer generated encodings, ignoring any user or model provided
encodings.
Note: Cannot use –ignore_encodings with –quantization_overrides</p>
</dd>
<dt><kbd><span class="option">--use_per_row_quantization</span></kbd></dt>
<dd><p>Use this option to enable rowwise quantization of Matmul and FullyConnected
ops.</p>
</dd>
</dl>
<dl class="simple">
<dt>–use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION …]]</dt><dd><p>Use per-channel quantization for convolution-based op weights.
Note: This will replace built-in model QAT encodings when used for a given
weight.Usage “–use_per_channel_quantization” to enable or “–
use_per_channel_quantization false” (default) to disable</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--use_native_input_files</span></kbd></dt>
<dd><p>Boolean flag to indicate how to read input files:
1. float (default): reads inputs as floats and quantizes if necessary based
on quantization parameters in the model.
2. native: reads inputs assuming the data type to be native to the
model. For ex., uint8_t.</p>
</dd>
<dt><kbd><span class="option">--use_native_dtype</span></kbd></dt>
<dd><p>Note: This option is deprecated, use –use_native_input_files option in
future.
Boolean flag to indicate how to read input files:
1. float (default): reads inputs as floats and quantizes if necessary based
on quantization parameters in the model.
2. native: reads inputs assuming the data type to be native to the
model. For ex., uint8_t.</p>
</dd>
<dt><kbd><span class="option">--use_native_output_files</span></kbd></dt>
<dd><p>Use this option to indicate the data type of the output files
1. float (default): output the file as floats.
2. native: outputs the file that is native to the model. For ex.,
uint8_t.</p>
</dd>
</dl>
</dd>
<dt>Custom Op Package Options:</dt><dd><dl class="simple">
<dt>–op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB</dt><dd><p>Use this argument to pass an op package library for quantization. Must be in
the form
&lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
comma for multiple package libs</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">-p <var>PACKAGE_NAME</var></span>, <span class="option">--package_name <var>PACKAGE_NAME</var></span></kbd></dt>
<dd><p>A global package name to be used for each node in the Model.cpp file.
Defaults to Qnn header defined package name</p>
</dd>
</dl>
<dl class="simple">
<dt>–op_package_config OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG …], -opc OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG …]</dt><dd><p>Path to a Qnn Op Package XML configuration file that contains user defined
custom operations.</p>
</dd>
</dl>
</dd>
<dt>Architecture Checker Options(Experimental):</dt><dd><dl class="option-list">
<dt><kbd><span class="option">--arch_checker</span></kbd></dt>
<dd><p>Pass this option to enable architecture checker tool.
This is an experimental option for models that are intended to run on HTP
backend.</p>
</dd>
</dl>
</dd>
</dl>
<p>Note: Only one of: {‘package_name’, ‘op_package_config’} can be specified</p>
</div></blockquote>
<p>Basic command line usage looks like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-tensorflow-converter -i &lt;path&gt;/frozen_graph.pb
                    -d &lt;network_input_name&gt; &lt;dims&gt;
                    --out_node &lt;network_output_name&gt;
                    -o &lt;optional_output_path&gt;
                    --allow_unconsumed_nodes  # optional, but most likely will be need for larger models
                    -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
</pre></div>
</div>
</div>
<div class="section" id="qnn-tflite-converter">
<h3>qnn-tflite-converter<a class="headerlink" href="#qnn-tflite-converter" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-tflite-converter</strong> tool converts a TFLite model to
a CPP file representing the model as a series of QNN API calls. Additionally,
a binary file containing static weights of the model is produced.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-tflite-converter -d INPUT_NAME INPUT_DIM [--out_node OUT_NAMES]
                            [--input_type INPUT_NAME INPUT_TYPE]
                            [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
                            [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                            [--dump_relay DUMP_RELAY]
                            [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
                            [--disable_batchnorm_folding] [--keep_disconnected_nodes]
                            [--input_list INPUT_LIST] [--param_quantizer PARAM_QUANTIZER]
                            [--act_quantizer ACT_QUANTIZER]
                            [--algorithms ALGORITHMS [ALGORITHMS ...]] [--bias_bw BIAS_BW]
                            [--act_bw ACT_BW] [--weight_bw WEIGHT_BW] [--ignore_encodings]
                            [--use_per_row_quantization]
                            [--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]]
                            [--use_native_input_files] [--use_native_dtype]
                            [--use_native_output_files] --input_network INPUT_NETWORK
                            [--debug [DEBUG]] [-o OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
                            [--float_bw FLOAT_BW] [--overwrite_model_prefix]
                            [--exclude_named_tensors] [--op_package_lib OP_PACKAGE_LIB]
                            [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                            [-h] [--arch_checker]

Script to convert TFLite model into QNN

required arguments:
  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers specified in the format
                        [input_name comma-separated-dimensions], for example:
                            &#39;data&#39; 1,224,224,3 Note that the quotes should always be included in order to handle special
                        characters, spaces, etc. For multiple inputs specify multiple --input_dim on the command
                        line like:
                            --input_dim &#39;data1&#39; 1,224,224,3 --input_dim &#39;data2&#39; 1,50,100,3
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces,etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                            --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                            image
                            default
                            opaque
  --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                            --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
  --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                           --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
  --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
  --dump_relay DUMP_RELAY
                        Dump Relay ASM and Params at the path provided with the argument
                        Usage: --dump_relay &lt;path_to_dump&gt;
  --show_unconsumed_nodes
                        Displays a list of unconsumed nodes, if there any are
                        found. Nodeswhich are unconsumed do not violate the
                        structural fidelity of thegenerated graph.
  --disable_batchnorm_folding
  --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
  -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --float_bw FLOAT_BW   Use the --float_bw option to select the bitwidth to use when using float for
                        parameters(weights/bias) and activations for all ops  or specific Op (via
                        encodings) selected through encoding, either 32 (default) or 16.
  --overwrite_model_prefix
                        If option passed, model generator will use the output path name to use as
                        model prefix to name functions in &lt;qnn_model_name&gt;.cpp. (Useful for running
                        multiple models at once) eg: ModelName_composeGraphs. Default is to use
                        generic &quot;QnnModel_&quot;.
  --exclude_named_tensors
                        Remove using source framework tensorNames; instead use a counter for naming
                        tensors. Note: This can potentially help to reduce the final model library
                        that will be generated(Recommended for deploying model). Default is False.
  -h, --help            show this help message and exit

Quantizer Options:
  --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
  --keep_quant_nodes    Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.
  --input_list INPUT_LIST
                        Path to a file specifying the input data. This file should be a plain text
                        file, containing one or more absolute file paths per line. Each path is
                        expected to point to a binary file containing one input in the &quot;raw&quot; format,
                        ready to be consumed by the quantizer without any further preprocessing.
                        Multiple files per line separated by spaces indicate multiple inputs to the
                        network. See documentation for more details. Must be specified for
                        quantization. All subsequent quantization options are ignored when this is
                        not provided.
  --param_quantizer PARAM_QUANTIZER
                        Optional parameter to indicate the weight/bias quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --act_quantizer ACT_QUANTIZER
                        Optional parameter to indicate the activation quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --algorithms ALGORITHMS [ALGORITHMS ...]
                        Use this option to enable new optimization algorithms. Usage is:
                        --algorithms &lt;algo_name1&gt; ... The available optimization algorithms are:
                        &quot;cle&quot; - Cross layer equalization includes a number of methods for equalizing
                        weights and biases across layers in order to rectify imbalances that cause
                        quantization errors. &quot;bc&quot; - Bias correction adjusts biases to offset
                        activation quantization errors. Typically used in conjunction with &quot;cle&quot; to
                        improve quantization accuracy.
  --bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use when quantizing the
                        biases, either 8 (default) or 32.
  --act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use when quantizing the
                        activations, either 8 (default) or 16.
  --weight_bw WEIGHT_BW
                        Use the --weight_bw option to select the bitwidth to use when quantizing the
                        weights, currently only 8 bit (default) supported.
  --ignore_encodings    Use only quantizer generated encodings, ignoring any user or model provided
                        encodings.
                        Note: Cannot use --ignore_encodings with --quantization_overrides
  --use_per_row_quantization
                        Use this option to enable rowwise quantization of Matmul and FullyConnected
                        ops.
  --use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]
                        Use per-channel quantization for convolution-based op weights.
                        Note: This will replace built-in model QAT encodings when used for a given
                        weight.Usage &quot;--use_per_channel_quantization&quot; to enable or &quot;--
                        use_per_channel_quantization false&quot; (default) to disable
  --use_native_input_files
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_dtype    Note: This option is deprecated, use --use_native_input_files option in
                        future.
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_output_files
                        Use this option to indicate the data type of the output files
                        1. float (default): output the file as floats.
                        2. native: outputs the file that is native to the model. For ex.,
                        uint8_t.

Custom Op Package Options:
  --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
  -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
  --op_package_config OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...], -opc OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Architecture Checker Options(Experimental):
  --arch_checker        Pass this option to enable architecture checker tool.
                        This is an experimental option for models that are intended to run on HTP
                        backend.

Note: Only one of: {&#39;package_name&#39;, &#39;op_package_config&#39;} can be specified
</pre></div>
</div>
<p>Basic command line usage looks like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-tflite-converter -i &lt;path&gt;/model.tflite
                       -d &lt;network_input_name&gt; &lt;dims&gt;
                       -o &lt;optional_output_path&gt;
                       -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
</pre></div>
</div>
</div>
<div class="section" id="qnn-pytorch-converter">
<h3>qnn-pytorch-converter<a class="headerlink" href="#qnn-pytorch-converter" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-pytorch-converter</strong> tool converts a PyTorch model to
a CPP file representing the model as a series of QNN API calls. Additionally,
a binary file containing static weights of the model is produced.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-pytorch-converter -d INPUT_NAME INPUT_DIM [--out_node OUT_NAMES]
                          [--input_type INPUT_NAME INPUT_TYPE]
                          [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
                          [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                          [--dump_relay DUMP_RELAY]
                          [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
                          [--disable_batchnorm_folding] [--keep_disconnected_nodes]
                          [--input_list INPUT_LIST] [--param_quantizer PARAM_QUANTIZER]
                          [--act_quantizer ACT_QUANTIZER]
                          [--algorithms ALGORITHMS [ALGORITHMS ...]] [--bias_bw BIAS_BW]
                          [--act_bw ACT_BW] [--weight_bw WEIGHT_BW] [--ignore_encodings]
                          [--use_per_row_quantization]
                          [--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]]
                          [--use_native_input_files] [--use_native_dtype]
                          [--use_native_output_files] --input_network INPUT_NETWORK
                          [--debug [DEBUG]] [-o OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
                          [--float_bw FLOAT_BW] [--overwrite_model_prefix]
                          [--exclude_named_tensors] [--op_package_lib OP_PACKAGE_LIB]
                          [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                          [-h] [--arch_checker]

Script to convert PyTorch model into QNN

required arguments:
  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers specified in the format
                        [input_name comma-separated-
                        dimensions], for example:
                            &#39;data&#39; 1,3,224,224
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dim on the command line like:
                            --input_dim &#39;data1&#39; 1,3,224,224 --input_dim &#39;data2&#39; 1,50,100,3
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces, etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                            --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes
                        input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                            image
                            default
                            opaque
  --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                            --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
  --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                           --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
  --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
  --dump_relay DUMP_RELAY
                        Dump Relay ASM and Params at the path provided with the argument
                        Usage: --dump_relay &lt;path_to_dump&gt;
  --disable_batchnorm_folding
  --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
  --debug [DEBUG]       Run the converter in debug mode.
  -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --overwrite_model_prefix
                        If option passed, model generator will use the output path name to use as
                        model prefix to name functions in &lt;qnn_model_name&gt;.cpp. (Useful for running
                        multiple models at once) eg: ModelName_composeGraphs. Default is to use
                        generic &quot;QnnModel_&quot;.
  --exclude_named_tensors
                        Remove using source framework tensorNames; instead use a counter for naming
                        tensors. Note: This can potentially help to reduce the final model library
                        that will be generated(Recommended for deploying model). Default is False.
  -h, --help            show this help message and exit

Quantizer Options:
  --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
  --keep_quant_nodes    Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.
  --input_list INPUT_LIST
                        Path to a file specifying the input data. This file should be a plain text
                        file, containing one or more absolute file paths per line. Each path is
                        expected to point to a binary file containing one input in the &quot;raw&quot; format,
                        ready to be consumed by the quantizer without any further preprocessing.
                        Multiple files per line separated by spaces indicate multiple inputs to the
                        network. See documentation for more details. Must be specified for
                        quantization. All subsequent quantization options are ignored when this is
                        not provided.
  --param_quantizer PARAM_QUANTIZER
                        Optional parameter to indicate the weight/bias quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --act_quantizer ACT_QUANTIZER
                        Optional parameter to indicate the activation quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --algorithms ALGORITHMS [ALGORITHMS ...]
                        Use this option to enable new optimization algorithms. Usage is:
                        --algorithms &lt;algo_name1&gt; ... The available optimization algorithms are:
                        &quot;cle&quot; - Cross layer equalization includes a number of methods for equalizing
                        weights and biases across layers in order to rectify imbalances that cause
                        quantization errors. &quot;bc&quot; - Bias correction adjusts biases to offset
                        activation quantization errors. Typically used in conjunction with &quot;cle&quot; to
                        improve quantization accuracy.
  --bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use when quantizing the
                        biases, either 8 (default) or 32.
  --act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use when quantizing the
                        activations, either 8 (default) or 16.
  --weight_bw WEIGHT_BW
                        Use the --weight_bw option to select the bitwidth to use when quantizing the
                        weights, currently only 8 bit (default) supported.
  --ignore_encodings    Use only quantizer generated encodings, ignoring any user or model provided
                        encodings.
                        Note: Cannot use --ignore_encodings with --quantization_overrides
  --use_per_row_quantization
                        Use this option to enable rowwise quantization of Matmul and FullyConnected
                        ops.
  --use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]
                        Use per-channel quantization for convolution-based op weights.
                        Note: This will replace built-in model QAT encodings when used for a given
                        weight.Usage &quot;--use_per_channel_quantization&quot; to enable or &quot;--
                        use_per_channel_quantization false&quot; (default) to disable
  --use_native_input_files
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_dtype    Note: This option is deprecated, use --use_native_input_files option in
                        future.
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_output_files
                        Use this option to indicate the data type of the output files
                        1. float (default): output the file as floats.
                        2. native: outputs the file that is native to the model. For ex.,
                        uint8_t.

Custom Op Package Options:
  --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
  -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
  --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -opc CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Architecture Checker Options(Experimental):
  --arch_checker        Pass this option to enable architecture checker tool.
                        This is an experimental option for models that are intended to run on HTP
                        backend.
</pre></div>
</div>
<p>Note: Only one of: {‘package_name’, ‘op_package_config’} can be specified</p>
<p>Basic command line usage looks like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-pytorch-converter -i &lt;path&gt;/model.pt
                       -d &lt;network_input_name&gt; &lt;dims&gt;
                       -o &lt;optional_output_path&gt;
                       -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
</pre></div>
</div>
</div>
<div class="section" id="qnn-onnx-converter">
<h3>qnn-onnx-converter<a class="headerlink" href="#qnn-onnx-converter" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-onnx-converter</strong> tool converts a model from the ONNX framework to
a CPP file representing the model as a series of QNN API calls. Additionally,
a binary file containing static weights of the model is produced.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-onnx-converter [--out_node OUT_NAMES] [--input_type INPUT_NAME INPUT_TYPE]
            [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
            [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
            [--dry_run [DRY_RUN]] [-d INPUT_NAME INPUT_DIM] [-n] [-b BATCH]
            [-s SYMBOL_NAME VALUE]
            [--dump_custom_io_config_template DUMP_CUSTOM_IO_CONFIG_TEMPLATE]
            [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
            [--disable_batchnorm_folding] [--keep_disconnected_nodes]
            [--input_list INPUT_LIST] [--param_quantizer PARAM_QUANTIZER]
            [--act_quantizer ACT_QUANTIZER] [--algorithms ALGORITHMS [ALGORITHMS ...]]
            [--bias_bw BIAS_BW] [--act_bw ACT_BW] [--weight_bw WEIGHT_BW]
            [--ignore_encodings] [--use_per_row_quantization]
            [--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]]
            [--use_native_input_files] [--use_native_dtype]
            [--use_native_output_files] --input_network INPUT_NETWORK
            [--debug [DEBUG]] [-o OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
            [--float_bw FLOAT_BW] [--overwrite_model_prefix] [--exclude_named_tensors]
            [--op_package_lib OP_PACKAGE_LIB]
            [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
            [-h] [--arch_checker]


Script to convert ONNX model into QNN

required arguments:
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output tensor names. Multiple output
                        nodes should be provided separately like:
                            --out_name out_1 --out_name out_2
  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for
                        each input is |default| if not specified. For example:
                        &quot;data&quot; image.Note that the quotes should always be
                        included in order to handle special characters,
                        spaces,etc. For multiple inputs specify multiple
                        --input_type on the command line. Eg:
                            --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following
                        descriptions state how input will be handled for each
                        option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to
                        the DSP.
                        Default:
                        Pass the input as floats to the dsp
                        directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next
                        layer) requires it as float, therefore it won&#39;t be
                        quantized.
                        Choices supported:
                            image
                            default
                            opaque
  --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers
                        specified in the format [input_name datatype], for
                        example:
                            &#39;data&#39; &#39;float32&#39;.
                        Default is float32 if not specified.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        e.g.
                            --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
  --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                            --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
                        Note: This flag does not set the layout of the input tensor in the converted DLC.
                            Please use --custom_io for that.
  --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
  --dry_run [DRY_RUN]   Evaluates the model without actually converting any ops, and returns
                        unsupported ops/attributes as well as unused inputs and/or outputs if any.
                        Leave empty or specify &quot;info&quot; to see dry run as a table, or specify &quot;debug&quot;
                        to show more detailed messages only&quot;
  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The name and dimension of all the input buffers to the network specified in
                        the format [input_name comma-separated-dimensions],
                        for example: &#39;data&#39; 1,224,224,3.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        NOTE: This feature works only with Onnx 1.6.0 and above
  -n, --no_simplification
                        Do not attempt to simplify the model automatically. This may prevent some
                        models from properly converting
  -b BATCH, --batch BATCH
                        The batch dimension override. This will take the first dimension of all
                        inputs and treat it as a batch dim, overriding it with the value provided
                        here. For example:
                        --batch 6
                        will result in a shape change from [1,3,224,224] to [6,3,224,224].
                        If there are inputs without batch dim this should not be used and each input
                        should be overridden independently using -d option for input dimension
                        overrides.
  -s SYMBOL_NAME VALUE, --define_symbol SYMBOL_NAME VALUE
                        This option allows overriding specific input dimension symbols. For instance
                        you might see input shapes specified with variables such as :
                        data: [1,3,height,width]
                        To override these simply pass the option as:
                        --define_symbol height 224 --define_symbol width 448
                        which results in dimensions that look like:
                        data: [1,3,224,448]
  --dump_custom_io_config_template
                        Dumps the yaml template for Custom I/O configuration. This file can be edited
                        as per the custom requirements and passed using the option --custom_ioUse
                        this option to specify a yaml file to which the custom IO config template is
                        dumped.
  --disable_batchnorm_folding
  --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
  --debug [DEBUG]       Run the converter in debug mode.
  -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --float_bw FLOAT_BW   Use the --float_bw option to select the bitwidth to use when using float for
                        parameters(weights/bias) and activations for all ops  or specific Op (via
                        encodings) selected through encoding, either 32 (default) or 16.
  --overwrite_model_prefix
                        If option passed, model generator will use the output path name to use as
                        model prefix to name functions in &lt;qnn_model_name&gt;.cpp. (Useful for running
                        multiple models at once) eg: ModelName_composeGraphs. Default is to use
                        generic &quot;QnnModel_&quot;.
  --exclude_named_tensors
                        Remove using source framework tensorNames; instead use a counter for naming
                        tensors. Note: This can potentially help to reduce  the final model library
                        that will be generated(Recommended for deploying model). Default is False.
  -h, --help            show this help message and exit
</pre></div>
</div>
<dl>
<dt>Quantizer Options:</dt><dd><dl class="option-list">
<dt><kbd><span class="option">--quantization_overrides <var>QUANTIZATION_OVERRIDES</var></span></kbd></dt>
<dd><p>Use this option to specify a json file with parameters to use for
quantization. These will override any quantization data carried from
conversion (eg TF fake quantization) or calculated during the normal
quantization process. Format defined as per AIMET specification.</p>
</dd>
<dt><kbd><span class="option">--keep_quant_nodes</span></kbd></dt>
<dd><p>Use this option to keep activation quantization nodes in the graph rather
than stripping them.</p>
</dd>
<dt><kbd><span class="option">--input_list <var>INPUT_LIST</var></span></kbd></dt>
<dd><p>Path to a file specifying the input data. This file should be a plain text
file, containing one or more absolute file paths per line. Each path is
expected to point to a binary file containing one input in the “raw” format,
ready to be consumed by the quantizer without any further preprocessing.
Multiple files per line separated by spaces indicate multiple inputs to the
network. See documentation for more details. Must be specified for
quantization. All subsequent quantization options are ignored when this is
not provided.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--param_quantizer <var>PARAM_QUANTIZER</var></span></kbd></dt>
<dd><p>Optional parameter to indicate the weight/bias quantizer to use. Must be
followed by one of the following options: “tf”: Uses the real min/max of the
data and specified bitwidth (default) “enhanced”: Uses an algorithm useful
for quantizing models with long tails present in the weight distribution
“adjusted”: Uses an adjusted min/max for computing the range, particularly
good for denoise models “symmetric”: Ensures min and max have the same
absolute values about zero. Data will be stored as int#_t data such that the
offset is always 0.</p>
</dd>
<dt><kbd><span class="option">--act_quantizer <var>ACT_QUANTIZER</var></span></kbd></dt>
<dd><p>Optional parameter to indicate the activation quantizer to use. Must be
followed by one of the following options: “tf”: Uses the real min/max of the
data and specified bitwidth (default) “enhanced”: Uses an algorithm useful
for quantizing models with long tails present in the weight distribution
“adjusted”: Uses an adjusted min/max for computing the range, particularly
good for denoise models “symmetric”: Ensures min and max have the same
absolute values about zero. Data will be stored as int#_t data such that the
offset is always 0.</p>
</dd>
</dl>
<dl>
<dt>–algorithms ALGORITHMS [ALGORITHMS …]</dt><dd><blockquote>
<div><blockquote>
<div><p>Use this option to enable new optimization algorithms. Usage is:
–algorithms &lt;algo_name1&gt; … The available optimization algorithms are:
“cle” - Cross layer equalization includes a number of methods for equalizing
weights and biases across layers in order to rectify imbalances that cause
quantization errors. “bc” - Bias correction adjusts biases to offset
activation quantization errors. Typically used in conjunction with “cle” to
improve quantization accuracy.</p>
</div></blockquote>
<dl class="option-list">
<dt><kbd><span class="option">--bias_bw <var>BIAS_BW</var></span></kbd></dt>
<dd><p>Use the –bias_bw option to select the bitwidth to use when quantizing the
biases, either 8 (default) or 32.</p>
</dd>
<dt><kbd><span class="option">--act_bw <var>ACT_BW</var></span></kbd></dt>
<dd><p>Use the –act_bw option to select the bitwidth to use when quantizing the
activations, either 8 (default) or 16.</p>
</dd>
<dt><kbd><span class="option">--weight_bw <var>WEIGHT_BW</var></span></kbd></dt>
<dd><p>Use the –weight_bw option to select the bitwidth to use when quantizing the
weights, currently only 8 bit (default) supported.</p>
</dd>
<dt><kbd><span class="option">--ignore_encodings</span></kbd></dt>
<dd><p>Use only quantizer generated encodings, ignoring any user or model provided
encodings.
Note: Cannot use –ignore_encodings with –quantization_overrides</p>
</dd>
<dt><kbd><span class="option">--use_per_row_quantization</span></kbd></dt>
<dd><p>Use this option to enable rowwise quantization of Matmul and FullyConnected
ops.</p>
</dd>
</dl>
<dl class="simple">
<dt>–use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION …]]</dt><dd><p>Use per-channel quantization for convolution-based op weights.
Note: This will replace built-in model QAT encodings when used for a given
weight.Usage “–use_per_channel_quantization” to enable or “–
use_per_channel_quantization false” (default) to disable</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--use_native_input_files</span></kbd></dt>
<dd><p>Boolean flag to indicate how to read input files:
1. float (default): reads inputs as floats and quantizes if necessary based
on quantization parameters in the model.
2. native: reads inputs assuming the data type to be native to the
model. For ex., uint8_t.</p>
</dd>
<dt><kbd><span class="option">--use_native_dtype</span></kbd></dt>
<dd><p>Note: This option is deprecated, use –use_native_input_files option in
future.
Boolean flag to indicate how to read input files:
1. float (default): reads inputs as floats and quantizes if necessary based
on quantization parameters in the model.
2. native: reads inputs assuming the data type to be native to the
model. For ex., uint8_t.</p>
</dd>
<dt><kbd><span class="option">--use_native_output_files</span></kbd></dt>
<dd><p>Use this option to indicate the data type of the output files
1. float (default): output the file as floats.
2. native:          outputs the file that is native to the model. For ex.,
uint8_t.</p>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>Custom Op Package Options:</dt><dd><dl class="simple">
<dt>–op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB</dt><dd><p>Use this argument to pass an op package library for quantization. Must be in
the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
comma for multiple package libs</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">-p <var>PACKAGE_NAME</var></span>, <span class="option">--package_name <var>PACKAGE_NAME</var></span></kbd></dt>
<dd><p>A global package name to be used for each node in the Model.cpp file.
Defaults to Qnn header defined package name</p>
</dd>
</dl>
<dl class="simple">
<dt>–op_package_config OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG …], -opc OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG …]</dt><dd><p>Path to a Qnn Op Package XML configuration file that contains user defined
custom operations.</p>
</dd>
</dl>
</dd>
<dt>Architecture Checker Options(Experimental):</dt><dd><dl class="option-list">
<dt><kbd><span class="option">--arch_checker</span></kbd></dt>
<dd><p>Pass this option to enable architecture checker tool.
This is an experimental option for models that are intended to run on HTP
backend.</p>
</dd>
</dl>
</dd>
</dl>
<p>Note: Only one of: {‘package_name’, ‘op_package_config’} can be specified</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="model-preparation">
<h2>Model Preparation<a class="headerlink" href="#model-preparation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="quantization-support">
<h3>Quantization Support<a class="headerlink" href="#quantization-support" title="Permalink to this headline">¶</a></h3>
<p>Quantization is supported through the converter interface and is performed at
conversion time. The only required option to enable quantization along with
conversion is the –input_list option, which provides the quantizer with the
required input data for the given model. The following options are available
in each converter listed above to enable and configure quantization:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Quantizer Options:
--quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters
                        to use for quantization. These will override any
                        quantization data carried from conversion (eg TF fake
                        quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET
                        specification.
--input_list INPUT_LIST
                      Path to a file specifying the input data. This file
                      should be a plain text file, containing one or more
                      absolute file paths per line. Each path is expected to
                      point to a binary file containing one input in the
                      &quot;raw&quot; format, ready to be consumed by the quantizer
                      without any further preprocessing. Multiple files per
                      line separated by spaces indicate multiple inputs to
                      the network. See documentation for more details. Must
                      be specified for quantization. All subsequent
                      quantization options are ignored when this is not
                      provided.
--param_quantizer PARAM_QUANTIZER
                      Optional parameter to indicate the weight/bias
                      quantizer to use. Must be followed by one of the
                      following options: &quot;tf&quot;: Uses the real min/max of the
                      data and specified bitwidth (default) &quot;enhanced&quot;: Uses
                      an algorithm useful for quantizing models with long
                      tails present in the weight distribution &quot;adjusted&quot;:
                      Uses an adjusted min/max for computing the range,
                      particularly good for denoise models &quot;symmetric&quot;:
                      Ensures min and max have the same absolute values
                      about zero. Data will be stored as int#_t data such
                      that the offset is always 0.
--act_quantizer ACT_QUANTIZER
                      Optional parameter to indicate the activation
                      quantizer to use. Must be followed by one of the
                      following options: &quot;tf&quot;: Uses the real min/max of the
                      data and specified bitwidth (default) &quot;enhanced&quot;: Uses
                      an algorithm useful for quantizing models with long
                      tails present in the weight distribution &quot;adjusted&quot;:
                      Uses an adjusted min/max for computing the range,
                      particularly good for denoise models &quot;symmetric&quot;:
                      Ensures min and max have the same absolute values
                      about zero. Data will be stored as int#_t data such
                      that the offset is always 0.
--algorithms ALGORITHMS [ALGORITHMS ...]
                      Use this option to enable new optimization algorithms.
                      Usage is: --algorithms &lt;algo_name1&gt; ... The
                      available optimization algorithms are: &quot;cle&quot; - Cross
                      layer equalization includes a number of methods for
                      equalizing weights and biases across layers in order
                      to rectify imbalances that cause quantization errors.
--bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use
                      when quantizing the biases, either 8 (default) or 32.
--act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use
                      when quantizing the activations, either 8 (default) or
                      16.
--weight_bw WEIGHT_BW
                      Use the --weight_bw option to select the bitwidth to
                      use when quantizing the weights, currently only 8 bit
                      (default) supported.
--ignore_encodings    Use only quantizer generated encodings, ignoring any
                      user or model provided encodings. Note: Cannot use
                      --ignore_encodings with --quantization_overrides
--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]
                      Use per-channel quantization for
                      convolution-based op weights. Note: This will replace
                      built-in model QAT encodings when used for a given
                      weight.Usage &quot;--use_per_channel_quantization&quot; to
                      enable or &quot;--use_per_channel_quantization false&quot;
                      (default) to disable
--use_per_row_quantization [USE_PER_ROW_QUANTIZATION [USE_PER_ROW_QUANTIZATION ...]]
                      Use this option to enable rowwise quantization of Matmul and
                      FullyConnected op. Usage &quot;--use_per_row_quantization&quot; to enable
                      or &quot;--use_per_row_quantization false&quot; (default) to
                      disable. This option may not be supported by all backends.
</pre></div>
</div>
<p>Basic command line usage to convert and quantize a model using the TF converter would look like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-tensorflow-converter -i &lt;path&gt;/frozen_graph.pb
                    -d &lt;network_input_name&gt; &lt;dims&gt;
                    --out_node &lt;network_output_name&gt;
                    -o &lt;optional_output_path&gt;
                    --allow_unconsumed_nodes  # optional, but most likely will be need for larger models
                    -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
                    --input_list input_list.txt
</pre></div>
</div>
<p>This will quantize the network using the default quantizer and bitwidths (8 bits for activations, weights, and biases).</p>
<p>For more detailed information on quantization, options, and algorithms please refer to <a class="reference internal" href="quantization.html"><span class="doc">Quantization</span></a>.</p>
</div>
<div class="section" id="qnn-model-lib-generator">
<h3>qnn-model-lib-generator<a class="headerlink" href="#qnn-model-lib-generator" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-model-lib-generator</strong> tool compiles QNN model source code
into artifacts for a specific target.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">lib</span><span class="o">-</span><span class="n">generator</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="n">QNN_MODEL</span><span class="o">&gt;</span><span class="p">.</span><span class="n">cpp</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="n">QNN_MODEL</span><span class="o">&gt;</span><span class="p">.</span><span class="n">bin</span><span class="p">]</span>
<span class="w">       </span><span class="p">[</span><span class="o">-</span><span class="n">t</span><span class="w"> </span><span class="n">LIB_TARGETS</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">LIB_NAME</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">OUTPUT_DIR</span><span class="p">]</span>
<span class="n">Script</span><span class="w"> </span><span class="n">compiles</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">Qnn</span><span class="w"> </span><span class="n">Model</span><span class="w"> </span><span class="n">artifacts</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">targets</span><span class="p">.</span>

<span class="n">Required</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="n">QNN_MODEL</span><span class="o">&gt;</span><span class="p">.</span><span class="n">cpp</span><span class="w">                    </span><span class="n">Filepath</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">qnn</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">file</span>

<span class="n">optional</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">-</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="n">QNN_MODEL</span><span class="o">&gt;</span><span class="p">.</span><span class="n">bin</span><span class="w">                    </span><span class="n">Filepath</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">qnn</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="p">.</span><span class="n">bin</span><span class="w"> </span><span class="n">file</span>
<span class="w">                                       </span><span class="p">(</span><span class="n">Note</span><span class="o">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">passed</span><span class="p">,</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">fail</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">needs</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">items</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="p">.</span><span class="n">bin</span><span class="w"> </span><span class="n">file</span><span class="p">.)</span>

<span class="w"> </span><span class="o">-</span><span class="n">t</span><span class="w"> </span><span class="n">LIB_TARGETS</span><span class="w">                        </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">targets</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">models</span><span class="w"> </span><span class="k">for</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="o">:</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span>
<span class="w"> </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">LIB_NAME</span><span class="w">                           </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">libraries</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="o">:</span><span class="w"> </span><span class="n">uses</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="o">&lt;</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span><span class="o">&gt;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">provided</span><span class="p">,</span>
<span class="w">                                       </span><span class="k">else</span><span class="w"> </span><span class="n">generic</span><span class="w"> </span><span class="n">qnn_model</span><span class="p">.</span><span class="n">so</span>
<span class="w">  </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">OUTPUT_DIR</span><span class="w">                         </span><span class="n">Location</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">saving</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">libraries</span><span class="p">.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For Windows users, please execute this tool with python3.</p>
</div>
</div>
<div class="section" id="qnn-op-package-generator">
<h3>qnn-op-package-generator<a class="headerlink" href="#qnn-op-package-generator" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-op-package-generator</strong> tool is used to generate skeleton code for a QNN op package using
an XML config file that describes the attributes of the package. The tool creates the package as a directory containing
skeleton source code and makefiles that can be compiled to create a shared library object.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">op</span><span class="o">-</span><span class="n">package</span><span class="o">-</span><span class="n">generator</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="o">--</span><span class="n">config_path</span><span class="w"> </span><span class="n">CONFIG_PATH</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">debug</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="n">OUTPUT_PATH</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">f</span><span class="p">]</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">-</span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">help</span><span class="w">            </span><span class="n">show</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">exit</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">config_path</span><span class="w"> </span><span class="n">CONFIG_PATH</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">p</span><span class="w"> </span><span class="n">CONFIG_PATH</span>
<span class="w">                        </span><span class="n">The</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">defines</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">Op</span>
<span class="w">                        </span><span class="n">package</span><span class="p">(</span><span class="n">s</span><span class="p">).</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">debug</span><span class="w">               </span><span class="n">Returns</span><span class="w"> </span><span class="n">debugging</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">generating</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">package</span>
<span class="w">  </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="n">OUTPUT_PATH</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">OUTPUT_PATH</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">package</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">saved</span>
<span class="w">  </span><span class="o">-</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">force</span><span class="o">-</span><span class="n">generation</span>
<span class="w">                        </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">delete</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">entire</span><span class="w"> </span><span class="n">existing</span><span class="w"> </span><span class="n">package</span>
<span class="w">                        </span><span class="n">Note</span><span class="w"> </span><span class="n">appropriate</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">permissions</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span>
<span class="w">                        </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="p">.</span>
</pre></div>
</div>
</div>
<div class="section" id="qnn-context-binary-generator">
<h3>qnn-context-binary-generator<a class="headerlink" href="#qnn-context-binary-generator" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-context-binary-generator</strong> tool is used to create a context binary by using a particular backend and consuming
a model library created by the <a class="reference internal" href="#qnn-model-lib-generator">qnn-model-lib-generator</a>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">context</span><span class="o">-</span><span class="n">binary</span><span class="o">-</span><span class="n">generator</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">QNN_MODEL</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">QNN_BACKEND</span><span class="p">.</span><span class="n">so</span>
<span class="w">                                    </span><span class="o">--</span><span class="n">binary_file</span><span class="w"> </span><span class="n">BINARY_FILE_NAME</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">model_prefix</span><span class="w"> </span><span class="n">MODEL_PREFIX</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">OUTPUT_DIRECTORY</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">op_packages</span><span class="w"> </span><span class="n">ONE_OR_MORE_OP_PACKAGES</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">config_file</span><span class="w"> </span><span class="n">CONFIG_FILE</span><span class="p">.</span><span class="n">json</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">verbose</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">version</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">help</span><span class="p">]</span>

<span class="n">REQUIRED</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="w">  </span><span class="o">--</span><span class="n">model</span><span class="w">             </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">&lt;</span><span class="n">qnn_model_name</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">network</span><span class="p">.</span>
<span class="w">                                  </span><span class="n">To</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="p">,</span><span class="w"> </span><span class="n">use</span>
<span class="w">                                  </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">files</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span>
<span class="w">                                  </span><span class="o">&lt;</span><span class="n">qnn_model_name_1</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">qnn_model_name_2</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">backend</span><span class="w">           </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">binary_file</span><span class="w">       </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">to</span><span class="p">.</span>
<span class="w">                                  </span><span class="n">Saved</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="p">.</span><span class="n">bin</span>
<span class="w">                                  </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">extension</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">provided</span><span class="p">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">binary</span>
<span class="w">                                  </span><span class="n">is</span><span class="w"> </span><span class="n">created</span><span class="p">.</span>


<span class="n">OPTIONAL</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="w">  </span><span class="o">--</span><span class="n">model_prefix</span><span class="w">                  </span><span class="n">Function</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">loading</span><span class="w"> </span><span class="o">&lt;</span><span class="n">qnn_model_name</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="w"> </span><span class="n">file</span>
<span class="w">                                  </span><span class="n">containing</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">network</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="o">:</span><span class="w"> </span><span class="n">QnnModel</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">output_dir</span><span class="w">        </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;</span><span class="w">       </span><span class="n">The</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">to</span><span class="p">.</span><span class="w"> </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">output</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">op_packages</span><span class="w">       </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="n">packages</span>
<span class="w">                                  </span><span class="n">and</span><span class="w"> </span><span class="n">interface</span><span class="w"> </span><span class="n">providers</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="k">register</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="o">:</span>
<span class="w">                                  </span><span class="nl">op_package_path</span><span class="p">:</span><span class="n">interface_provider</span><span class="p">[,</span><span class="n">op_package_path</span><span class="o">:</span><span class="n">interface_provider</span><span class="p">...]</span>

<span class="w">  </span><span class="o">--</span><span class="n">config_file</span><span class="w">       </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">currently</span>
<span class="w">                                  </span><span class="n">supports</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">extensions</span><span class="w"> </span><span class="n">and</span>
<span class="w">                                  </span><span class="n">context</span><span class="w"> </span><span class="n">priority</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">refer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">documentation</span>
<span class="w">                                  </span><span class="k">for</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">details</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">enable_intermediate_outputs</span><span class="w">   </span><span class="n">Enable</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">nodes</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">along</span><span class="w"> </span><span class="n">with</span>
<span class="w">                                  </span><span class="k">default</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">saved</span><span class="w"> </span><span class="n">context</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">backend_binary</span><span class="w">    </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="o">-</span><span class="n">specific</span><span class="w"> </span><span class="n">context</span>
<span class="w">                                  </span><span class="n">binary</span><span class="w"> </span><span class="n">to</span><span class="p">.</span>
<span class="w">                                  </span><span class="n">Saved</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="p">.</span><span class="n">bin</span>
<span class="w">                                  </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">extension</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">log_level</span><span class="w">                     </span><span class="n">Specifies</span><span class="w"> </span><span class="n">max</span><span class="w"> </span><span class="n">logging</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">settings</span><span class="o">:</span>
<span class="w">                                  </span><span class="s">&quot;error&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;warn&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;info&quot;</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="s">&quot;verbose&quot;</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">version</span><span class="w">                       </span><span class="n">Print</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">version</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">help</span><span class="w">                          </span><span class="n">Show</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="p">.</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#qnn-net-run">qnn-net-run</a> section for more details about <code class="docutils literal notranslate"><span class="pre">--op_packages</span></code> and <code class="docutils literal notranslate"><span class="pre">--config_file</span></code> options.</p>
</div>
</div>
<div class="section" id="execution">
<h2>Execution<a class="headerlink" href="#execution" title="Permalink to this headline">¶</a></h2>
<div class="section" id="qnn-net-run">
<h3>qnn-net-run<a class="headerlink" href="#qnn-net-run" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-net-run</strong> tool is used to consume a model library compiled from
the output of the QNN converter, and run it on a particular backend.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">DESCRIPTION</span><span class="p">:</span>
<span class="o">------------</span>
<span class="n">Example</span><span class="w"> </span><span class="n">application</span><span class="w"> </span><span class="n">demonstrating</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">load</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span>
<span class="n">using</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">APIs</span><span class="p">.</span>


<span class="n">REQUIRED</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="w">  </span><span class="o">--</span><span class="n">model</span><span class="w">             </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">network</span><span class="p">.</span>
<span class="w">                                   </span><span class="n">To</span><span class="w"> </span><span class="n">compose</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="p">,</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span>
<span class="w">                                   </span><span class="n">model</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">files</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span>
<span class="w">                                   </span><span class="o">&lt;</span><span class="n">qnn_model_name_1</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">qnn_model_name_2</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">backend</span><span class="w">           </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">input_list</span><span class="w">        </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">listing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">network</span><span class="p">.</span>
<span class="w">                                   </span><span class="n">If</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">so</span><span class="p">,</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">has</span>
<span class="w">                                   </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">files</span><span class="p">.</span>
<span class="w">                                   </span><span class="n">When</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">present</span><span class="p">,</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">skip</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">use</span>
<span class="w">                                   </span><span class="s">&quot;__&quot;</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">underscore</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">quotes</span><span class="p">)</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span>
<span class="w">                                   </span><span class="n">comma</span><span class="o">-</span><span class="n">seperated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">files</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w">  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">cached</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">load</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">saved</span>
<span class="w">                                  </span><span class="n">context</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">graphs</span><span class="p">.</span><span class="w"> </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w"> </span><span class="n">and</span>
<span class="w">                                  </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span><span class="w"> </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span>
<span class="w">                                  </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>


<span class="n">OPTIONAL</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="w">  </span><span class="o">--</span><span class="n">model_prefix</span><span class="w">                             </span><span class="n">Function</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">loading</span><span class="w"> </span><span class="o">&lt;</span><span class="n">qnn_model_name</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">.</span>
<span class="w">                                             </span><span class="nl">Default</span><span class="p">:</span><span class="w"> </span><span class="n">QnnModel</span>

<span class="w">  </span><span class="o">--</span><span class="n">debug</span><span class="w">                                    </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">layers</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">network</span>
<span class="w">                                             </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">saved</span><span class="p">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">loading</span>
<span class="w">                                             </span><span class="n">a</span><span class="w"> </span><span class="n">saved</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w"> </span><span class="n">option</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">output_dir</span><span class="w">                   </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;</span><span class="w">       </span><span class="n">The</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">to</span><span class="p">.</span><span class="w"> </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">output</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">use_native_output_files</span><span class="w">                  </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span>
<span class="w">                                             </span><span class="n">type</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">graph</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">specified</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span>
<span class="w">                                             </span><span class="n">be</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">floating</span><span class="w"> </span><span class="n">point</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">use_native_input_files</span><span class="w">                   </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">parsed</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span>
<span class="w">                                             </span><span class="n">type</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">graph</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">specified</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span>
<span class="w">                                             </span><span class="n">be</span><span class="w"> </span><span class="n">parsed</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">floating</span><span class="w"> </span><span class="n">point</span><span class="p">.</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">use_native_input_files</span>
<span class="w">                                             </span><span class="n">and</span><span class="w"> </span><span class="o">--</span><span class="n">native_input_tensor_names</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">native_input_tensor_names</span><span class="w">    </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">names</span><span class="p">,</span>
<span class="w">                                             </span><span class="k">for</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">read</span><span class="o">/</span><span class="n">parsed</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">format</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">use_native_input_files</span><span class="w"> </span><span class="n">and</span>
<span class="w">                                             </span><span class="o">--</span><span class="n">native_input_tensor_names</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="n">graphName0</span><span class="o">:</span><span class="n">tensorName0</span><span class="p">,</span><span class="n">tensorName1</span><span class="p">;</span><span class="n">graphName1</span><span class="o">:</span><span class="n">tensorName0</span><span class="p">,</span><span class="n">tensorName1</span>

<span class="w">  </span><span class="o">--</span><span class="n">op_packages</span><span class="w">                  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="n">packages</span><span class="p">,</span><span class="w"> </span><span class="n">interface</span>
<span class="w">                                             </span><span class="n">providers</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="p">,</span><span class="w"> </span><span class="n">optionally</span><span class="p">,</span><span class="w"> </span><span class="n">targets</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="k">register</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">values</span>
<span class="w">                                             </span><span class="k">for</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">CPU</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">HTP</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="o">:</span>
<span class="w">                                             </span><span class="nl">op_package_path</span><span class="p">:</span><span class="n">interface_provider</span><span class="o">:</span><span class="n">target</span><span class="p">[,</span><span class="n">op_package_path</span><span class="o">:</span><span class="n">interface_provider</span><span class="o">:</span><span class="n">target</span><span class="p">...]</span>

<span class="w">  </span><span class="o">--</span><span class="n">profiling_level</span><span class="w">              </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Enable</span><span class="w"> </span><span class="n">profiling</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">Values</span><span class="o">:</span>
<span class="w">                                               </span><span class="mf">1.</span><span class="w"> </span><span class="n">basic</span><span class="o">:</span><span class="w">    </span><span class="n">captures</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">init</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>
<span class="w">                                               </span><span class="mf">2.</span><span class="w"> </span><span class="n">detailed</span><span class="o">:</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">addition</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">basic</span><span class="p">,</span><span class="w"> </span><span class="n">captures</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">Op</span><span class="w"> </span><span class="n">timing</span>
<span class="w">                                                            </span><span class="k">for</span><span class="w"> </span><span class="n">execution</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">supports</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">perf_profile</span><span class="w">                 </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">perf</span><span class="w"> </span><span class="n">profile</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">set</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">settings</span><span class="w"> </span><span class="n">are</span>
<span class="w">                                             </span><span class="n">low_balanced</span><span class="p">,</span><span class="w"> </span><span class="n">balanced</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="p">,</span><span class="w"> </span><span class="n">high_performance</span><span class="p">,</span>
<span class="w">                                             </span><span class="n">sustained_high_performance</span><span class="p">,</span><span class="w"> </span><span class="n">burst</span><span class="p">,</span><span class="w"> </span><span class="n">low_power_saver</span><span class="p">,</span>
<span class="w">                                             </span><span class="n">power_saver</span><span class="p">,</span><span class="w"> </span><span class="n">high_power_saver</span><span class="p">,</span><span class="w"> </span><span class="n">extreme_power_saver</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">system_settings</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">config_file</span><span class="w">                  </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">currently</span>
<span class="w">                                             </span><span class="n">supports</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">extensions</span><span class="p">,</span>
<span class="w">                                             </span><span class="n">context</span><span class="w"> </span><span class="n">priority</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">configs</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">refer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">SDK</span>
<span class="w">                                             </span><span class="n">documentation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">details</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">log_level</span><span class="w">                    </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">max</span><span class="w"> </span><span class="n">logging</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">settings</span><span class="o">:</span>
<span class="w">                                             </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">warn</span><span class="p">,</span><span class="w"> </span><span class="n">info</span><span class="p">,</span><span class="w"> </span><span class="n">debug</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">verbose</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">shared_buffer</span><span class="w">                            </span><span class="n">Specifies</span><span class="w"> </span><span class="n">creation</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">shared</span><span class="w"> </span><span class="n">buffers</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">I</span><span class="o">/</span><span class="n">O</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">application</span>
<span class="w">                                             </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">device</span><span class="o">/</span><span class="n">coprocessor</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">directly</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">currently</span><span class="w"> </span><span class="n">supported</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Android</span><span class="w"> </span><span class="n">only</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">synchronous</span><span class="w">                              </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">executed</span><span class="w"> </span><span class="n">synchronously</span><span class="w"> </span><span class="n">rather</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">asynchronously</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">If</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">asynchronous</span><span class="w"> </span><span class="n">execution</span><span class="p">,</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">unnecessary</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">num_inferences</span><span class="w">               </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">inferences</span><span class="p">.</span><span class="w"> </span><span class="n">Loops</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input_list</span><span class="w"> </span><span class="n">until</span>
<span class="w">                                             </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">inferences</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">transpired</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">duration</span><span class="w">                     </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">duration</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">seconds</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Loops</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input_list</span><span class="w"> </span><span class="n">until</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">amount</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">transpired</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">keep_num_outputs</span><span class="w">             </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">saved</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Once</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">reach</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">limit</span><span class="p">,</span><span class="w"> </span><span class="n">subsequent</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">just</span><span class="w"> </span><span class="n">discarded</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">batch_multiplier</span><span class="w">             </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">dimensions</span>
<span class="w">                                             </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">multiplied</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">modified</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">during</span>
<span class="w">                                             </span><span class="n">the</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">graphs</span><span class="p">.</span><span class="w"> </span><span class="n">Composed</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">still</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">model</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">timeout</span><span class="w">                      </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">timeout</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">micro</span><span class="w"> </span><span class="n">seconds</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">note</span>
<span class="w">                                             </span><span class="n">using</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">timeout</span><span class="w"> </span><span class="n">signals</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">max_input_cache_tensor_sets</span><span class="w">  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">sets</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">cached</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Use</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="s">&quot;-1&quot;</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">cache</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">created</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">max_input_cache_tensor_sets</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">--</span><span class="n">max_input_cache_size_mb</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">max_input_cache_size_mb</span><span class="w">      </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">cache</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">mega</span><span class="w"> </span><span class="n">bytes</span><span class="p">(</span><span class="n">MB</span><span class="p">).</span>
<span class="w">                                             </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">max_input_cache_tensor_sets</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">--</span><span class="n">max_input_cache_size_mb</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">version</span><span class="w">                                  </span><span class="n">Print</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">version</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">help</span><span class="w">                                     </span><span class="n">Show</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="p">.</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/examples/QNN/NetRun</span></code> folder for reference example on how to use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool.</p>
<p><strong>Typical arguments:</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">--backend</span></code> - The appropriate argument depends on what target and backend you want to run on</p>
<blockquote>
<div><p>Android (aarch64): <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/lib/aarch64-android/</span></code></p>
</div></blockquote>
<p>&#64;CPU_NET_RUN_LIBS&#64;
&#64;GPU_NET_RUN_LIBS&#64;
&#64;HTA_NET_RUN_LIBS&#64;
&#64;DSP_NET_RUN_LIBS&#64;
&#64;HTP_NET_RUN_LIBS&#64;
&#64;HTP_ALT_PREP_NET_RUN_LIBS&#64;
&#64;SAVER_NET_RUN_LIBS&#64;</p>
<blockquote>
<div><p>Linux x86: <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/lib/x86_64-linux-clang/</span></code></p>
</div></blockquote>
<p>&#64;CPU_NET_RUN_LIBS&#64;
&#64;HTP_NET_RUN_LIBS&#64;
&#64;SAVER_NET_RUN_LIBS&#64;</p>
<blockquote>
<div><p>Windows x86: <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/lib/x86_64-windows-msvc/</span></code></p>
</div></blockquote>
<p>&#64;CPU_NET_RUN_LIBS_WINDOWS&#64;
&#64;SAVER_NET_RUN_LIBS_WINDOWS&#64;</p>
<blockquote>
<div><p>WoS: <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/lib/aarch64-windows-msvc/</span></code></p>
</div></blockquote>
<p>&#64;CPU_NET_RUN_LIBS_WINDOWS&#64;
&#64;DSP_NET_RUN_LIBS_WINDOWS&#64;
&#64;HTP_NET_RUN_LIBS_WINDOWS&#64;
&#64;SAVER_NET_RUN_LIBS_WINDOWS&#64;</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Hexagon based backend libraries are emulations on x86_64 platforms</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--input_list</span></code> - This argument provides a file containing paths to input files to be used for graph
execution. Input files can be specified with the below format:</p>
<blockquote>
<div><div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">input_layer_name</span><span class="o">&gt;:=&lt;</span><span class="n">input_layer_path</span><span class="o">&gt;</span><span class="p">[</span><span class="o">&lt;</span><span class="n">space</span><span class="o">&gt;&lt;</span><span class="n">input_layer_name</span><span class="o">&gt;:=&lt;</span><span class="n">input_layer_path</span><span class="o">&gt;</span><span class="p">]</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">input_layer_name</span><span class="o">&gt;:=&lt;</span><span class="n">input_layer_path</span><span class="o">&gt;</span><span class="p">[</span><span class="o">&lt;</span><span class="n">space</span><span class="o">&gt;&lt;</span><span class="n">input_layer_name</span><span class="o">&gt;:=&lt;</span><span class="n">input_layer_path</span><span class="o">&gt;</span><span class="p">]]</span>
<span class="p">...</span>
</pre></div>
</div>
</div></blockquote>
<p>Below is an example containing 3 sets of inputs with layer names “Input_1” and “Input_2”, and files
located in the relative path “Placeholder_1/real_input_inputs_1/”:</p>
<blockquote>
<div><div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Input_1:=Placeholder_1/real_input_inputs_1/0-0#e6fb51.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/0-1#8a171b.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/1-0#67c965.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/1-1#54f1ff.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/2-0#b42dc6.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/2-1#346a0e.rawtensor
</pre></div>
</div>
</div></blockquote>
<p>Note:  If the batch dimension of the model is greater than 1, the number of batch elements in the
input file has to either match the batch dimension specified in the model or it has to be one. In the
latter case, qnn-net-run will combine multiple lines into a single input tensor.</p>
<p><code class="docutils literal notranslate"><span class="pre">--op_packages</span></code> - This argument is only needed if you are using custom op packages. The native QNN
ops are already included as part of the backend libraries.</p>
<blockquote>
<div><p>When using custom op packages, each provided op package requires a colon separated command line
argument containing the path to the op package shared library (.so) file, as well as the name of the
interface provider, formatted as <code class="docutils literal notranslate"><span class="pre">&lt;op_package_path&gt;:&lt;interface_provider&gt;</span></code>.</p>
<p>The interface_provider argument must be the name of the function in the op package library that
satisfies the <a class="reference internal" href="../api-rst/typedef_QnnOpPackage_8h_1a71759daf7267945c50a6e5417026e869.html#exhale-typedef-qnnoppackage-8h-1a71759daf7267945c50a6e5417026e869"><span class="std std-ref">QnnOpPackage_InterfaceProvider_t</span></a>
interface. In the skeleton code created by <code class="docutils literal notranslate"><span class="pre">qnn-op-package-generator</span></code>, this function will be named
<code class="docutils literal notranslate"><span class="pre">&lt;package_name&gt;&lt;backend&gt;InterfaceProvider</span></code>.</p>
<p>See <a class="reference internal" href="generating_op_packages.html"><span class="doc">Generating Op Packages</span></a> for more information.</p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">--config_file</span></code> - This argument is only needed if you need to specify context priority or provide
backend extensions related parameters. These parameters are specified through a JSON file.</p>
<blockquote>
<div><p>Below is the template for the JSON file:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="s">&quot;backend_extensions&quot;</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;shared_library_path&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;path_to_shared_library&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;config_file_path&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;path_to_config_file&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">  </span><span class="s">&quot;context_configs&quot;</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;context_priority&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;low | normal | normal_high | high&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="s">&quot;graph_configs&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;graph_name&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;graph_name_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;graph_priority&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;low | normal | normal_high | high&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>All the options in the JSON file are optional. <em>context_priority</em> is used to specify priority of the context
as a context config. <em>graph_configs</em> can be used to specify asynchronous execution order and depth, if a backend
supports asynchronous execution. Every set of graph configs has to be specified along with a graph name.</p>
<p><em>backend_extensions</em> is used to exercise custom options in a particular backend. This can be done by providing an
extensions shared library (.so) and a config file, if necessary. This is also required to enable various performance modes,
which can be exercised using <code class="docutils literal notranslate"><span class="pre">--perf_profile</span></code> option.</p>
<p>Currently, HTP supports it through <code class="docutils literal notranslate"><span class="pre">libQnnHtpNetRunExtensions.so</span></code> shared library and DSP supports it through <code class="docutils literal notranslate"><span class="pre">libQnnDspNetRunExtensions.so</span></code>.
For different custom options which can be enabled with HTP see <a class="reference internal" href="htp/htp_backend.html#qnn-htp-backend-extensions"><span class="std std-ref">HTP Backend Extensions</span></a></p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">--shared_buffer</span></code> - This argument is only needed to indicate qnn-net-run to use shared buffers for zero-copy use case with
a device/coprocessor associated with a particular backend (for ex., DSP with HTP backend) for graph input and output tensor data.
This option is supported on Android only. qnn-net-run implements this feature using rpcmem APIs, which further create shared
buffers using ION/DMA-BUF memory allocator on Android, available through the shared library libcdsprpc.so. In addition to
specifying this option, for qnn-net-run to be able to discover libcdsprpc.so, the path in which the shared library is present
needs to be appended to LD_LIBRARY_PATH variable.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">export</span><span class="w"> </span><span class="n">LD_LIBRARY_PATH</span><span class="o">=</span><span class="n">$LD_LIBRARY_PATH</span><span class="o">:/</span><span class="n">vendor</span><span class="o">/</span><span class="n">lib64</span>
</pre></div>
</div>
<p>&#64;HTP_NET_RUN_EXEC&#64;</p>
</div>
<div class="section" id="qnn-throughput-net-run">
<h3>qnn-throughput-net-run<a class="headerlink" href="#qnn-throughput-net-run" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-throughput-net-run</strong> tool is used to exericse the execution of multiple models on a QNN backend
or on different backends in a multi-threaded fashion. It allows repeated execution of models on a specified
backend for a specified duration or number of iterations.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">Usage</span><span class="p">:</span>
<span class="o">------</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">throughput</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">config</span><span class="w"> </span><span class="o">&lt;</span><span class="n">config_file</span><span class="o">&gt;</span><span class="p">.</span><span class="n">json</span><span class="p">]</span>
<span class="w">                       </span><span class="p">[</span><span class="o">--</span><span class="n">output</span><span class="w"> </span><span class="o">&lt;</span><span class="n">results</span><span class="o">&gt;</span><span class="p">.</span><span class="n">json</span><span class="p">]</span>

<span class="n">REQUIRED</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">--</span><span class="n">config</span><span class="w">        </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="p">.</span><span class="n">json</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">json</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="p">.</span>

<span class="n">OPTIONAL</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">--</span><span class="n">output</span><span class="w">        </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="p">.</span><span class="n">json</span><span class="w">       </span><span class="n">Specify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">json</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">results</span><span class="p">.</span>
</pre></div>
</div>
<p><strong>Configuration JSON File:</strong></p>
<p><strong>qnn-throughput-net-run</strong> uses configuration file as input to run the models on the backends.
The configuration json file comprises of four objects (required) - <strong>backends</strong>, <strong>models</strong>, <strong>contexts</strong> and
<strong>testCase</strong>.</p>
<p>Below is an example of a json configuration file. Please refer the following <a class="reference internal" href="#qtnr-config-link"><span class="std std-ref">section</span></a>
for detailed information on the four configuration objects <a class="reference internal" href="#backends-link"><span class="std std-ref">backends</span></a>, <a class="reference internal" href="#models-link"><span class="std std-ref">models</span></a>,
<a class="reference internal" href="#contexts-link"><span class="std std-ref">contexts</span></a> and <a class="reference internal" href="#testcase-link"><span class="std std-ref">testCase</span></a>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="s">&quot;backends&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;backendName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_backend&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;backendPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libQnnCpu.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;profilingLevel&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;BASIC&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;backendExtensions&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libQnnHtpNetRunExtensions.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;perfProfile&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;high_performance&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;backendName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_backend&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;backendPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libQnnGpu.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;profilingLevel&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;OFF&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="s">&quot;models&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;modelName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;modelPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libqnn_model_1.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;loadFromCachedBinary&quot;</span><span class="o">:</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;inputPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1-input_list.txt&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;inputDataType&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FLOAT&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;postProcessor&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;MSE&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;outputPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1-output&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;outputDataType&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FLOAT_ONLY&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;saveOutput&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;NATIVE_ALL&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;groundTruthPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1-golden_list.txt&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;modelName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_2&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;modelPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libqnn_model_2.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;loadFromCachedBinary&quot;</span><span class="o">:</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;inputPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_2-input_list.txt&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;inputDataType&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FLOAT&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;postProcessor&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;MSE&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;outputPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_2-output&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;outputDataType&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FLOAT_ONLY&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;saveOutput&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;NATIVE_LAST&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="s">&quot;contexts&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;contextName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_context_1&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;contextName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_context_1&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="s">&quot;testCase&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="s">&quot;iteration&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;logLevel&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;error&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;threads&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;threadName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_thread_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;backend&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_backend&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;context&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_context_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;model&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;interval&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;loopUnit&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;count&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;loop&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;threadName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_thread_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;backend&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_backend&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;context&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_context_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;model&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_2&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;interval&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;loopUnit&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;count&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;loop&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">10</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p id="backends-link"><span id="qtnr-config-link"></span><strong>backends</strong> : Property value is an array of json objects, where each object contains the needed backend
information on which the models are executed.
Each object of the array has the following properties as key/value pairs.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">backendName</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Is a unique identifer for the testcase to designate on which backend the model should be run.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">backendPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the on device backend .so library file path.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">profilingLevel</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">OFF</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Sets the QNN profiling level for the backend.
Possible values: OFF, BASIC, DETAILED.</p>
<blockquote>
<div><ul class="simple">
<li><p>BASIC - Captures execution and init times.</p></li>
<li><p>DETAILED - In addition to BASIC captures per Op timing for execution, if backend supports.</p></li>
</ul>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">backendExtensions</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Enables backend specific options through optional backend extensions
shared library and config file.
<code class="docutils literal notranslate"><span class="pre">Syntax:</span> <span class="pre">path_to_shared_library</span></code>.</p>
<p>This is required to enable various performance modes which are
exercised using <code class="docutils literal notranslate"><span class="pre">perfProfile</span></code> option.
Currently, HTP supports it through <code class="docutils literal notranslate"><span class="pre">libQnnHtpNetRunExtensions.so</span></code> shared library.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">perfProfile</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">default</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies performance profile to set.</p>
<p>Possible values: <code class="docutils literal notranslate"><span class="pre">low_balanced,</span> <span class="pre">balanced,</span> <span class="pre">default,</span> <span class="pre">high_performance,</span></code>
<code class="docutils literal notranslate"><span class="pre">sustained_high_performance,</span> <span class="pre">burst,</span> <span class="pre">low_power_saver,</span> <span class="pre">power_saver,</span></code>
<code class="docutils literal notranslate"><span class="pre">high_power_saver,</span> <span class="pre">extreme_power_saver</span> <span class="pre">and</span> <span class="pre">system_settings</span></code>.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">opPackagePath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Native</span> <span class="pre">QNN</span> <span class="pre">Ops.</span></code>
<code class="docutils literal notranslate"><span class="pre">part</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">backend</span></code>
<code class="docutils literal notranslate"><span class="pre">libraries</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Comma seperated list of custom op packages and interface providers for registration.</p>
<p><code class="docutils literal notranslate"><span class="pre">Syntax:</span> <span class="pre">op_package_1_path:interface_provider_1[,op_package_2_path:interface_provider_2…]</span></code></p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">platformOption</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Enables backend specific platform options through QnnBackend_Config_t.</p>
<p><code class="docutils literal notranslate"><span class="pre">Syntax:</span> <span class="pre">&quot;key:value&quot;</span></code></p>
</td>
</tr>
</tbody>
</table>
<p id="models-link"><strong>models</strong> : Property value is an array of json objects, where each object contatins details about a model and
corresponding input data and post-processing information.
Each object of the array has the following properties as key/value pairs.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">modelName</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Is a unique identifer for the testcase to designate which model to run.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">modelPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the &lt;model&gt;.so / &lt;serialized_context&gt;.bin file path.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">loadFromCachedBinary</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set to <code class="docutils literal notranslate"><span class="pre">true</span></code> if &lt;serialized_context&gt;.bin is used in <code class="docutils literal notranslate"><span class="pre">modelPath</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">inputPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Path to a file listing the inputs for the model. If not set, Random Input Data is used.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">inputDataType</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NATIVE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: NATIVE, FLOAT.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">postProcessor</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NONE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: NONE, MSE, MSE_FLOAT32, MSE_INT8, MSE_INT16.</p>
<p>MSE will output a mean squared error result for each execution with the golden file specified by the parameter
<code class="docutils literal notranslate"><span class="pre">groundTruthPath</span></code>. If the <code class="docutils literal notranslate"><span class="pre">groundTruthPath</span></code> is not specified, the first execution output result is used to
compute the MSE. If the datatype of the file specified in <code class="docutils literal notranslate"><span class="pre">groundTruthPath</span></code> is different from the network’s
output type, users need to specify the relevant datatype in the <code class="docutils literal notranslate"><span class="pre">postProcessor</span></code> parameter.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">outputPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">postProcessor</span></code> is not <code class="docutils literal notranslate"><span class="pre">NONE</span></code>, output files and profiling logs will be saved to this directory.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">outputDataType</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NATIVE_ONLY</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: NATIVE_ONLY, FLOAT_ONLY, FLOAT_AND_NATIVE.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">saveOutput</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NONE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: NONE, NATIVE_LAST,NATIVE_ALL.</p>
<blockquote>
<div><ul class="simple">
<li><p>NATIVE_LAST - Saves only the result of the last network execution to the <code class="docutils literal notranslate"><span class="pre">outputPath</span></code>.</p></li>
<li><p>NATIVE_ALL - Saves the results of all network executions to the <code class="docutils literal notranslate"><span class="pre">outputPath</span></code>.</p></li>
</ul>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">groundTruthPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NONE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies the golden file path for computing the MSE.</p></td>
</tr>
</tbody>
</table>
<p id="contexts-link"><strong>contexts</strong> : Property value is an array of json objects, where each object contains all the context information.
Each object of the array has the following properties as key/value pairs.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">contextName</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Is a unique identifer for the testcase to designate the context in which a model should be created.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DEFAULT</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specfies the priority of the context.
Possible values: DEFAULT, LOW, NORMAL, HIGH.</p></td>
</tr>
</tbody>
</table>
<p id="testcase-link"><strong>testCase</strong> : Property value is a json object that specifies the testing configuration that controls
multi-threaded execution.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">iteration</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Number of times the entire usecase is repeated. If the value is <code class="docutils literal notranslate"><span class="pre">negative</span></code>, test runs forever
until keyboard interrupt.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">logLevel</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies max logging level to be set. Valid settings: <code class="docutils literal notranslate"><span class="pre">error</span></code>, <code class="docutils literal notranslate"><span class="pre">warn</span></code>, <code class="docutils literal notranslate"><span class="pre">info</span></code>, <code class="docutils literal notranslate"><span class="pre">debug</span></code>, and <code class="docutils literal notranslate"><span class="pre">verbose</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">threads</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Property value is an array of json objects, where each object contatins all the thread details,
that are to be executed by the qnn-throughput-net-run. Each object of the array has the below
properties listed under <a class="reference internal" href="#threads-link"><span class="std std-ref">threads</span></a> as  key/value pairs.</p></td>
</tr>
</tbody>
</table>
<p id="threads-link"><code class="docutils literal notranslate"><span class="pre">threads</span></code> : Property value is an array contatining all the threads and corresponding backend, context
and models information.
Each element of the array can have the following required/optional property.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">threadName</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Is a unique identifer for the testcase to identify the thread and save the output results.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">backend</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the backend to be used when this thread executes the graph.
The value specified should match with one of the <code class="docutils literal notranslate"><span class="pre">backendName</span></code> entry in the <code class="docutils literal notranslate"><span class="pre">backends</span></code> property of the configuration json.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">context</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the context to be used when this thread executes the graph.
The value specified should match with one of the <code class="docutils literal notranslate"><span class="pre">contextName</span></code> entry in the <code class="docutils literal notranslate"><span class="pre">contexts</span></code> property of the configuration json.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">model</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the model to be used by the thread for execution.
The value specified should match with one of the <code class="docutils literal notranslate"><span class="pre">modelName</span></code> entry in the <code class="docutils literal notranslate"><span class="pre">models</span></code> property of the configuration json.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">initModelInLoop</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set it to <code class="docutils literal notranslate"><span class="pre">true</span></code> if the model needs to be initialized repeatedly for every iteration. The value cannot be set to <code class="docutils literal notranslate"><span class="pre">true</span></code>
if <code class="docutils literal notranslate"><span class="pre">loadFromCachedBinary</span></code> from <code class="docutils literal notranslate"><span class="pre">models</span></code> property is  <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">loadInputDataInLoop</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set it to <code class="docutils literal notranslate"><span class="pre">true</span></code> if the input needs to be reloaded for every loop of execution.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">useRandomData</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set it to <code class="docutils literal notranslate"><span class="pre">true</span></code> if random data is needed to be used as input.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">interval</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Repesents the interval (in microseconds) between each graph execution in the thread.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">loopUnit</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">count</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: count, second.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">loop</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Value is taken either as seconds or count based on the value for the <code class="docutils literal notranslate"><span class="pre">loopUnit</span></code>.
If <code class="docutils literal notranslate"><span class="pre">loopUnit</span></code> is <code class="docutils literal notranslate"><span class="pre">second</span></code>, the value specifies the number of seconds the threads repeats execution.
If <code class="docutils literal notranslate"><span class="pre">loopUnit</span></code> is <code class="docutils literal notranslate"><span class="pre">count</span></code>, the value specifies number of times thread repeats execution.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">backendConfig</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies the backend config file to enable backend specific options through <code class="docutils literal notranslate"><span class="pre">backendExtensions</span></code>
shared library.
<code class="docutils literal notranslate"><span class="pre">Syntax:</span> <span class="pre">path_to_backend_config_file</span></code>.</p></td>
</tr>
</tbody>
</table>
<p>An example json file <code class="docutils literal notranslate"><span class="pre">sample_config.json</span></code> file can be found at <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/examples/QNN/ThroughputNetRun</span></code>.</p>
</div>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<div class="section" id="qnn-quantization-checker-experimental">
<h3>qnn-quantization-checker (Experimental)<a class="headerlink" href="#qnn-quantization-checker-experimental" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-quantization-checker</strong> tool is used to analyze the
activations, weights and biases of all the possible
quantization options available in the qnn-converter for each subsequent
layer of a single model file or a directory of models. The analysis
involves comparing the weight, bias and activation tensors of the quantized
options to their unquantized counterparts. The tool runs the model to
generate floating-point activations and analyzes floating-point weights, biases
and activations to determine the quality of the encodings. It finally outputs the
problematic weight, bias and activation tensors for all quantization options.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">MODEL_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">INPUT_LIST_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">activation_width</span><span class="w"> </span><span class="n">ACT_BW</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">bias_width</span><span class="w"> </span><span class="n">BIAS_BW</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">OUTPUT_DIR_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">skip_building_model</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">skip_generator</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">skip_runner</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">generate_histogram</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">per_channel_histogram</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">output_csv</span><span class="p">]</span>
<span class="w">                                </span><span class="o">--</span><span class="n">config_file</span><span class="w"> </span><span class="n">CONFIG_FILE_PATH</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">config_file</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">specifying</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">execution</span><span class="p">.</span>
<span class="w">                  </span><span class="n">E</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="p">[</span><span class="n">MODEL_PATH</span><span class="p">,</span><span class="w"> </span><span class="n">INPUT_LIST_PATH</span><span class="p">,</span><span class="w"> </span><span class="n">SDK_ROOT</span><span class="p">,</span><span class="w"> </span><span class="n">ACTIVATION_WIDTH</span><span class="p">,</span><span class="w"> </span><span class="n">BIAS_WIDTH</span><span class="p">,</span><span class="w"> </span><span class="n">OUTPUT_DIR_PATH</span><span class="p">,</span><span class="w"> </span><span class="n">ANDROID_NDK_PATH</span><span class="p">,</span><span class="w"> </span><span class="n">CLANG_PATH</span><span class="p">,</span><span class="w"> </span><span class="n">BASH_PATH</span><span class="p">,</span><span class="w"> </span><span class="n">BIN_PATH</span><span class="p">,</span><span class="w"> </span><span class="n">PY3_PATH</span><span class="p">,</span><span class="w"> </span><span class="n">TENSORFLOW_HOME</span><span class="p">,</span><span class="w"> </span><span class="n">ONNX_HOME</span><span class="p">,</span><span class="w"> </span><span class="n">QUANTIZATION_OVERRIDES</span><span class="p">,</span><span class="w"> </span><span class="n">WEIGHT_COMPARISON_ALGORITHMS</span><span class="p">,</span><span class="w"> </span><span class="n">BIAS_COMPARISON_ALGORITHMS</span><span class="p">,</span><span class="w"> </span><span class="n">ACT_COMPARISON_ALGORITHMS</span><span class="p">,</span><span class="w"> </span><span class="n">INPUT_DATA_ANALYSIS_ALGORITHMS</span><span class="p">,</span><span class="w"> </span><span class="n">OUTPUT_CSV</span><span class="p">]</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">MODEL_PATH</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config_file</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">text</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">files</span><span class="p">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">MODEL_PATH</span><span class="o">/</span><span class="n">model</span><span class="w"> </span><span class="n">refers</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">INPUT_LIST_PATH</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config_file</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">activation_width</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Bit</span><span class="o">-</span><span class="n">width</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">activations</span><span class="p">.</span><span class="w"> </span><span class="n">E</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mf">16.</span><span class="w"> </span><span class="n">Default</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="mf">8.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">ACTIVATION_WIDTH</span><span class="w"> </span><span class="n">field</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config_file</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">bias_width</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Bit</span><span class="o">-</span><span class="n">width</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">biases</span><span class="p">.</span><span class="w"> </span><span class="n">E</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mf">32.</span><span class="w"> </span><span class="n">Default</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="mf">8.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">BIAS_WIDTH</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config_file</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">unquantized</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">files</span><span class="p">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">OUTPUT_DIR_PATH</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config_file</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">skip_building_model</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Stop</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tool</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">building</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">assumed</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">pre</span><span class="o">-</span><span class="n">built</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">skip_generator</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Stop</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tool</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">converter</span><span class="p">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">assumed</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">necessary</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">available</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">skip_runner</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Stop</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tool</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">assumed</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">necessary</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">available</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">generate_histogram</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="n">histogram</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">weights</span><span class="o">/</span><span class="n">biases</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">skip</span><span class="w"> </span><span class="n">histgoram</span><span class="w"> </span><span class="n">generation</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">per_channel_histogram</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">channel</span><span class="w"> </span><span class="n">histogram</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">weights</span><span class="o">/</span><span class="n">biases</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">skip</span><span class="w"> </span><span class="n">histgoram</span><span class="w"> </span><span class="n">generation</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">output_csv</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">Optional</span><span class="p">]</span><span class="w"> </span><span class="n">Store</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">csv</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">directory</span><span class="p">.</span>

<span class="n">Please</span><span class="w"> </span><span class="n">note</span><span class="o">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">commands</span><span class="w"> </span><span class="n">accepted</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">command</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">override</span><span class="w"> </span><span class="n">those</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">overlap</span><span class="p">.</span>
</pre></div>
</div>
<p>Here is an example of different algorithms and thresholds mentioned in the config file:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="p">{</span>
<span class="linenos">2</span><span class="w">   </span><span class="c1">// All other required user options can be added here</span>
<span class="linenos">3</span>
<span class="hll"><span class="linenos">4</span><span class="w">   </span><span class="s">&quot;WEIGHT_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;maxdiff&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;sqnr&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;26&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_distribution_analyzer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;0.6&quot;</span><span class="p">}],</span>
</span><span class="linenos">5</span><span class="w">   </span><span class="s">&quot;BIAS_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;maxdiff&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;sqnr&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;26&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_distribution_analyzer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;0.6&quot;</span><span class="p">}],</span>
<span class="linenos">6</span><span class="w">   </span><span class="s">&quot;ACT_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">}]</span>
<span class="linenos">7</span><span class="p">}</span>
</pre></div>
</div>
<p>The tool generates a <strong>html</strong> directory to include all .html files containing clear indicators of failures and successes for different combinations of quantization options and input files.
The tool also produces a <strong>csv</strong> directory to include all .csv files storing detailed computation results for each metric.
Additionally, the tool generates an output log file under the <strong>qnn-quantization-checker-log</strong> directory which contains all log outputs.</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">--</span><span class="n">config_file</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="n">qti</span><span class="o">/</span><span class="n">aisw</span><span class="o">/</span><span class="n">quantization_checker</span><span class="o">/</span><span class="n">configs</span><span class="o">/</span><span class="n">internal_developer_config</span><span class="p">.</span><span class="n">json</span>
</pre></div>
</div>
<p><strong>Sample Config File</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="s">&quot;ANDROID_NDK_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;{Path_To_Android_NDK}/Android/android-ndk-r25c&quot;</span><span class="p">,</span>
<span class="s">&quot;CLANG_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;{Path_To_Clang_Lib}/clang-9.0.0/bin&quot;</span><span class="p">,</span>
<span class="s">&quot;PY3_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{PY3_PATH}/py3env/bin&quot;</span><span class="p">,</span>
<span class="s">&quot;BASH_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/bin&quot;</span><span class="p">,</span>
<span class="s">&quot;BIN_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/usr/bin&quot;</span><span class="p">,</span>
<span class="s">&quot;TENSORFLOW_HOME&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_Tensorflow}/bionic/py3&quot;</span><span class="p">,</span>
<span class="s">&quot;TFLITE_HOME&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_TFLITE}/bionic/py3&quot;</span><span class="p">,</span>
<span class="s">&quot;ONNX_HOME&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_Onnx}/bionic/py3&quot;</span><span class="p">,</span>
<span class="s">&quot;MODEL_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_Model}/model_frozen.pb&quot;</span><span class="p">,</span>
<span class="s">&quot;INPUT_LIST_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_Input}/input_list.txt&quot;</span><span class="p">,</span>
<span class="s">&quot;WEIGHT_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;maxdiff&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;sqnr&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;26&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_distribution_analyzer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;0.6&quot;</span><span class="p">}],</span>
<span class="s">&quot;BIAS_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;maxdiff&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;sqnr&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;26&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_distribution_analyzer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;0.6&quot;</span><span class="p">}],</span>
<span class="s">&quot;ACT_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">}],</span>
<span class="s">&quot;INPUT_DATA_ANALYSIS_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">}],</span>
<span class="s">&quot;OUTPUT_CSV&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;True&quot;</span><span class="p">,</span>
<span class="s">&quot;INPUT_DIMENSION&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;&#39;input&#39; 1,299,299,3&quot;</span><span class="p">],</span>
<span class="s">&quot;OUTPUT_DIR_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;{Path_To_Output_Dir}&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="section" id="viewing-the-results-html-csv-or-log-files">
<h4>Viewing the results (html, csv or log files)<a class="headerlink" href="#viewing-the-results-html-csv-or-log-files" title="Permalink to this headline">¶</a></h4>
<p>All results for a sample model directory “sample_model_dir” are stored in its original location or user sepcified output directory.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># In sample_model_dir, all .html files are stored in &quot;sample_model_dir/html&quot;, .csv files are stored in &quot;sample_model_dir/csv&quot; and log files are stored in &quot;sample_model_dir/qnn-quantization-checker-log&quot;.</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">sample_model_dir</span><span class="o">/</span><span class="n">html</span>
<span class="cp"># This directory contains all the html files produced by the qnn-quantization-checker which can be opened with any standard web browser.</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">sample_model_dir</span><span class="o">/</span><span class="n">csv</span>
<span class="cp"># This directory contains all the csv files produced by the qnn-quantization-checker which can be viewed with any standard text editor or spreadsheet application such as Excel, LibreOffice, etc.</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">sample_model_dir</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">checker</span><span class="o">-</span><span class="n">log</span>
<span class="cp"># This directory contains all the log files produced by the qnn-quantization-checker. The files are time stamped and can be viewed in any text editor.</span>

<span class="n">If</span><span class="w"> </span><span class="n">histogram</span><span class="w"> </span><span class="n">generation</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">histogram</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">biases</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">produced</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">respective</span><span class="w"> </span><span class="n">directories</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">followed</span><span class="o">:</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">sample_model_dir</span><span class="o">/</span><span class="n">hist_analysis_weights</span>
<span class="cp"># This directory contains all the png files representing pixelwise data distribution for unquantized and quantized weights. The files can be viewed in any image viewer.</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">sample_model_dir</span><span class="o">/</span><span class="n">hist_analysis_biases</span>
<span class="cp"># This directory contains all the png files representing pixelwise data distribution for unquantized and quantized biases. The files can be viewed in any image viewer.</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><strong>HTML Results Files</strong></div>
</div>
<p>Each HTML file contains a summary of the results for each quantization option and for each input file provided.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Snapshot of HTML content has been added for clarity.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/qnn_quantatization_checker_html_sample.png" src="../_static/resources/qnn_quantatization_checker_html_sample.png" />
</div>
<div class="line-block">
<div class="line"><strong>CSV Results Files</strong></div>
</div>
<p>Each CSV file contains detailed computation results for a specific node type (activation/weight/bias) and quantization option.
Each row in the csv file displays the op name, node name, passes accuracy (True/False), computation result (accuracy differences), threshold being used for each algorithm and the algorithm name.
Computation results(accuracy differences) format can be somewhat different according to different algorithms/metrics.</p>
<p>The following are a few short notes about the different algorithms and the information each csv row contains:</p>
<ul class="simple">
<li><dl class="simple">
<dt>minmax: Indicates the difference between the unquantized minimum and the dequantized minimum value. Correspondingly, indicates the same difference for the maximum unquantized and dequantized value.</dt><dd><p>Example computation result: “min: #VALUE max: #VALUE”</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>maxdiff: Calculates the absolute difference between the unquantized and dequantized data for all data points and displays the maximum value of the result.</dt><dd><p>Example computation result: “#VALUE”</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>sqnr: Calculates the signal to quantization noise ratio between the two tensors of unquantized and dequantized data.</dt><dd><p>Example computation result: “#VALUE”</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>data_range_analyzer: Calculates the difference between the maximum and minimum values in a tensor and compares that to the maximum value supported by the bit-width used to determine if the range of values can be reasonably represented by the selected quantization bit width.</dt><dd><p>Example computation result: “unique dec places: #INT_VALUE data range : #VALUE”. Information in the computation results field includes how many unique decimal places we need to express the unquantized data in quantized format and what is the actual data range.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>data_distribution_analyzer: Calculates the clustering of the data to find whether a large number of unique unquantized values are quantized to the same value or not.</dt><dd><p>Example computation result: “Distribution of pixels above threshold: #VALUE”</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>stats: Calculates some basic statistics on the received data such as the min, max, median, variance, standard deviation, the mode and the skew. The skew is used to indicate how symmetric the data is.</dt><dd><p>Example computation result: skew: #VALUE min: #VALUE max: #VALUE median: #VALUE variance: #VALUE stdDev: #VALUE mode: #VALUE</p>
</dd>
</dl>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Snapshot of a CSV file content for weight data for one of the quantization options has been added below.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/qnn_quantatization_checker_csv_weights.png" src="../_static/resources/qnn_quantatization_checker_csv_weights.png" />
</div>
<p>Separate .csv files are available for activations, weights and biases for each quantization option.
The activation related results also include analysis for each input file provided.</p>
<div class="line-block">
<div class="line"><strong>Log Result File</strong></div>
</div>
<p>The log files contain the following information:</p>
<ol class="arabic simple">
<li><p>All the commands executed as part of the script’s run. This will
include different runs of the qnn-converter tool with different
quantization options</p></li>
<li><p>Activations Analysis Failures</p></li>
<li><p>Weights Analysis Failures</p></li>
<li><p>Biases Analysis Failures</p></li>
</ol>
<p>A sample log output looks like below:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;====</span><span class="n">ACTIVATIONS</span><span class="w"> </span><span class="n">ANALYSIS</span><span class="w"> </span><span class="n">FAILURES</span><span class="o">====&gt;</span>

<span class="n">Results</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">enhanced</span><span class="w"> </span><span class="n">quantization</span><span class="o">:</span>
<span class="o">|</span><span class="w">         </span><span class="n">Op</span><span class="w"> </span><span class="n">Name</span><span class="w">         </span><span class="o">|</span><span class="w"> </span><span class="n">Activation</span><span class="w"> </span><span class="n">Node</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Passes</span><span class="w"> </span><span class="n">Accuracy</span><span class="w"> </span><span class="o">|</span><span class="w">         </span><span class="n">Accuracy</span><span class="w"> </span><span class="n">Difference</span><span class="w">          </span><span class="o">|</span><span class="w"> </span><span class="n">Threshold</span><span class="w"> </span><span class="n">Used</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="n">Used</span><span class="w"> </span><span class="o">|</span>
<span class="o">|</span><span class="w">  </span><span class="n">conv_tanh_comp1_conv0</span><span class="w">  </span><span class="o">|</span><span class="w">    </span><span class="n">ReLU_6919</span><span class="w">    </span><span class="o">|</span><span class="w">      </span><span class="n">False</span><span class="w">      </span><span class="o">|</span><span class="w"> </span><span class="n">minabs_diff</span><span class="o">:</span><span class="w"> </span><span class="mf">0.59</span><span class="w"> </span><span class="n">maxabs_diff</span><span class="o">:</span><span class="w"> </span><span class="mf">17.16</span><span class="w"> </span><span class="o">|</span><span class="w">      </span><span class="mf">0.05</span><span class="w">      </span><span class="o">|</span><span class="w">     </span><span class="n">minmax</span><span class="w">     </span><span class="o">|</span>
</pre></div>
</div>
<p>where,</p>
<ol class="arabic simple">
<li><p>Op Name : Op Name as expressed in qnn_model.cpp</p></li>
<li><p>Activation Node : Activation Node Name in the Op</p></li>
<li><p>Passes Accuracy : Pass if the quantized activation(or weight or
bias) meets threshold when compared with values from float32 graph</p></li>
<li><p>Accuracy Difference : Details about the accuracy per the algorithm
used</p></li>
<li><p>Threshold Used : The threshold used to influence the result of
“Passes Accuracy” column</p></li>
<li><p>Algorithm Used : Metric used to compare actual quantized activations/weights/biases
against unquantized float data or analyze the quality of unquantized float data.
Metrics can be minmax, maxdiff, sqnr, stats, data_range_analyzer, data_distribution_analyzer.</p></li>
</ol>
</div>
</div>
<div class="section" id="qnn-accuracy-evaluator-experimental">
<h3>qnn-accuracy-evaluator (Experimental)<a class="headerlink" href="#qnn-accuracy-evaluator-experimental" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-accuracy-evaluator</strong> tool is used to identify the best quantization options for a model on a given set of inputs.
In addition, the tool also provides a framework to evaluate end to end accuracy metrics for a model on a given dataset.</p>
<p><strong>Dependencies</strong></p>
<p>The QNN Accuracy Evaluator depends on the environment setup and platform dependencies as outlined in the
<a class="reference internal" href="setup.html"><span class="doc">Setup</span></a> page. The user needs to run the envsetup.sh script and setup the ONNX and TensorFlow frameworks. The
user also needs to install the following pip packages before using the tool.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">joblib</span><span class="o">==</span><span class="mf">1.0.1</span>
<span class="n">opencv</span><span class="o">-</span><span class="n">python</span><span class="o">==</span><span class="mf">4.5.2.52</span>
<span class="n">torch</span><span class="o">==</span><span class="mf">1.8.1</span>
<span class="n">torchvision</span><span class="o">==</span><span class="mf">0.9.1</span>
<span class="n">tabulate</span><span class="o">==</span><span class="mf">0.8.5</span>
<span class="n">onnxruntime</span><span class="o">==</span><span class="mf">1.9.0</span>
<span class="n">matplotlib</span><span class="o">==</span><span class="mf">3.3.4</span>
</pre></div>
</div>
<p>All the python dependencies mentioned above are installed using the check-python-dependency mentioned in <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a>
except for the torch, torchvision, onnxruntime since they are framework specific.</p>
<p>Run the following script to check and install all the python dependencies:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">check</span><span class="o">-</span><span class="n">python</span><span class="o">-</span><span class="n">dependency</span>
<span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="mf">.6</span><span class="w"> </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="n">pip</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">torch</span><span class="o">==</span><span class="mf">1.8.1</span>
<span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="mf">.6</span><span class="w"> </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="n">pip</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">torchvision</span><span class="o">==</span><span class="mf">0.9.1</span>
<span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="mf">.6</span><span class="w"> </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="n">pip</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">onnxruntime</span><span class="o">==</span><span class="mf">1.9.0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>onnxruntime package is needed only for Onnx models</p>
</div>
<p>To install the linux dependencies, use the check-linux-dependency.sh script as mentioned in <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">bash</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">check</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">dependency</span><span class="p">.</span><span class="n">sh</span>
</pre></div>
</div>
<div class="section" id="usage">
<h4>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h4>
<p>There are two modes of usage for the tool - minimal mode and config mode.</p>
<p><strong>Minimal mode</strong> option is to perform accuracy analysis on all possible
quantization options and rank them in order based on the given comparator when compared against the cpu fp32 outputs.</p>
<p><strong>Config mode</strong> supports accuracy analysis of a model on a given dataset by customizing the backends, quantization options
and reference platforms.</p>
</div>
<div class="section" id="minimal-mode">
<h4>Minimal Mode<a class="headerlink" href="#minimal-mode" title="Permalink to this headline">¶</a></h4>
<p><strong>Minimal mode</strong> option is to perform accuracy analysis on all possible
quantization options and rank them in order based on the given comparator when compared against the cpu fp32 outputs.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>qnn-accuracy-evaluator Options

minimal mode options:
    -model MODEL          path to model or model directory, available only for
                          htp platform.
    -backend {htp}        Backend to run the inference
    -target-arch {aarch64-android,x86_64-linux-clang}
                          Target architecture to compile.
    -preproc-file PREPROC_FILE
                          Path to a file specifying paths to raw inputs.
    -comparator COMPARATOR
                          comparator to be used.
    -tol-thresh TOL_THRESH
                          Tolerance threshold to be used for the comparator
    -act-bw ACT_BW        [Optional] bitwidth to use for activations. E.g., 8,
                          1.  Default is 8.
    -bias-bw BIAS_BW      [Optional] bitwidth to use for biases. either 8
                          (default) or 32.
    -box-input BOX_INPUT  Path to the json file. Used only with the box
                          comparator

other options:
    -input-info INPUT_INFO INPUT_INFO
                            The name and dimension of all the input buffers to the
                            network specified in the format [input_name comma-
                            separated-dimensions], for example: &#39;data&#39;
                            1,224,224,3. This option is mandatory for pytorch
                            models in minimal mode.
    -onnx-symbol ONNX_SYMBOL [ONNX_SYMBOL ...]
                            Replace onnx symbols in input/output shapes.Can be
                            passed multiple timesDefault replaced by 1. e.g
                            __unk_200:1
    -device-id DEVICE_ID  Target device-id/device-serial to be provided
    -work-dir WORK_DIR    working directory path. default is ./qacc_temp
    -silent               Run in silent mode
</pre></div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">evaluator</span><span class="w"> </span><span class="o">-</span><span class="n">model</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_MODEL_ZOO</span><span class="p">}</span><span class="o">/</span><span class="n">onnx</span><span class="o">-</span><span class="n">cnns_mobilenet</span><span class="o">/</span><span class="n">Source_model</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">onnx</span>
<span class="w">                       </span><span class="o">-</span><span class="n">preproc</span><span class="o">-</span><span class="n">file</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_MODEL_ZOO</span><span class="p">}</span><span class="o">/</span><span class="n">onnx</span><span class="o">-</span><span class="n">cnns_mobilenet</span><span class="o">/</span><span class="n">inputs</span><span class="o">/</span><span class="n">input_list</span><span class="p">.</span><span class="n">txt</span>
<span class="w">                       </span><span class="o">-</span><span class="n">backend</span><span class="w"> </span><span class="n">htp</span>
<span class="w">                       </span><span class="o">-</span><span class="n">target</span><span class="o">-</span><span class="n">arch</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span>
<span class="w">                       </span><span class="o">-</span><span class="n">comparator</span><span class="w"> </span><span class="n">abs</span>
<span class="w">                       </span><span class="o">-</span><span class="n">tol</span><span class="o">-</span><span class="n">thresh</span><span class="w"> </span><span class="mf">0.1</span>
</pre></div>
</div>
<p><strong>Results</strong></p>
<p>The tool displays a table with quantization options ordered by output match based on the selected comparator and also
generates a csv file with the same data. Comparator column shows output match percentage/value based on the selected
comparator. Quant params column displays the quantization params used for that run. Other columns also show backend,
runtime/compile params used. The information is also stored in a csv file at <cite>{work_dir}/metrics-info.csv</cite>.</p>
<p>The quantization option combinations that are run in minimal mode:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">param_quantizer</span><span class="p">:</span><span class="w">  </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">enhanced</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">adjusted</span>
<span class="nl">algorithms</span><span class="p">:</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">cle</span>
<span class="nl">use_per_channel_quantization</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">False</span>
</pre></div>
</div>
<p>Each quantization option work directory is stored at <cite>{work_dir}/infer/{platform}</cite>. QNN IR files are stored at
<cite>{work_dir}/infer/{platform}/qnn_ir</cite> and outputs are stored at <cite>{work_dir}/infer/{platform}/Result_{i}</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Snapshot of console log has been added for clarity.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/qnn_acc_eval_output.png" src="../_static/resources/qnn_acc_eval_output.png" />
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Snapshot of csv file has been added for clarity.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/qnn_acc_eval_csv.png" src="../_static/resources/qnn_acc_eval_csv.png" />
</div>
</div>
<div class="section" id="config-mode">
<h4>Config Mode<a class="headerlink" href="#config-mode" title="Permalink to this headline">¶</a></h4>
<p><strong>Config mode</strong> supports accuracy analysis of a model on a given dataset by customizing the backends,
quantization options and reference platforms. Sample config files can be found at
<cite>${QNN_SDK_ROOT}/lib/python/qti/aisw/accuracy_evaluator/configs/samples/model_configs</cite>.</p>
<p>User can configure custom datasets in the file <cite>dataset.yaml</cite>. The configuration is shown in the example below.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>COCO2014:
    path: &#39;/home/ml-datasets/COCO/2014/&#39;
    inputlist_file: inputlist.txt
    calibration:
        type: index
        file: calibration-index.txt
</pre></div>
</div>
<p>Details of the dataset fields is as follows:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>path              Base directory of the dataset files
inputlist_file    Text file containing all the pre-processed input files relative to the path field, one input
                  per line. For models having multiple inputs, the inputs in each line have to be comma separated
calibration       Specifies the calibration file type to be used when using Profile Guided Quantization (PGQ).
                  Optional, if PGQ not used
                  type: Type can be &#39;index&#39;, &#39;raw&#39; or &#39;dataset&#39;
                        index  : File provided contains the indexes to be picked from inputlist for calibration
                        raw    : File provided contains entries of pre-processed raw files for calibration
                        dataset: File provided contains images processed separately and passed to inference
                  file: pre-processed calibration file name
</pre></div>
</div>
<p><cite>defaults.yaml</cite> and <cite>dataset.yaml</cite> files are located at <cite>${QNN_SDK_ROOT}/lib/python/qti/aisw/accuracy_evaluator/configs</cite>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">acc</span><span class="o">-</span><span class="n">evaluator</span><span class="w"> </span><span class="n">options</span>

<span class="n">required</span><span class="w"> </span><span class="n">options</span><span class="o">:</span>
<span class="w">    </span><span class="o">-</span><span class="n">config</span><span class="w"> </span><span class="n">CONFIG</span><span class="w">        </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">yaml</span>

<span class="w">    </span><span class="n">pipeline</span><span class="w"> </span><span class="n">options</span><span class="o">:</span>
<span class="w">    </span><span class="o">-</span><span class="n">preproc</span><span class="o">-</span><span class="n">file</span><span class="w"> </span><span class="n">PREPROC_FILE</span>
<span class="w">                            </span><span class="n">preprocessed</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">overrides</span><span class="w"> </span><span class="n">inputfile</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">config</span>
<span class="w">    </span><span class="o">-</span><span class="n">calib</span><span class="o">-</span><span class="n">file</span><span class="w"> </span><span class="n">CALIB_FILE</span>
<span class="w">                            </span><span class="n">calibration</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">overrides</span><span class="w"> </span><span class="n">calibration</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">config</span>

<span class="n">other</span><span class="w"> </span><span class="n">options</span><span class="o">:</span>
<span class="w">    </span><span class="o">-</span><span class="n">device</span><span class="o">-</span><span class="n">id</span><span class="w"> </span><span class="n">DEVICE_ID</span><span class="w">  </span><span class="n">Target</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="n">id</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">provided</span>
<span class="w">    </span><span class="o">-</span><span class="n">work</span><span class="o">-</span><span class="n">dir</span><span class="w"> </span><span class="n">WORK_DIR</span><span class="w">    </span><span class="n">working</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">path</span><span class="p">.</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">qacc_temp</span>
<span class="w">    </span><span class="o">-</span><span class="n">silent</span><span class="w">               </span><span class="n">Run</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">silent</span><span class="w"> </span><span class="n">mode</span>
<span class="w">    </span><span class="o">-</span><span class="n">platform</span><span class="w"> </span><span class="n">PLATFORM</span><span class="w">    </span><span class="n">run</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">platform</span>
<span class="w">    </span><span class="o">-</span><span class="n">platform</span><span class="o">-</span><span class="n">tag</span><span class="w"> </span><span class="n">PLATFORM_TAG</span>
<span class="w">                            </span><span class="n">run</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">platform</span><span class="w"> </span><span class="n">tag</span>
<span class="w">    </span><span class="o">-</span><span class="n">batchsize</span><span class="w"> </span><span class="n">BATCHSIZE</span><span class="w">  </span><span class="n">overrides</span><span class="w"> </span><span class="n">batchsize</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">config</span>
<span class="w">    </span><span class="o">-</span><span class="n">platform</span><span class="o">-</span><span class="n">tag</span><span class="o">-</span><span class="n">params</span><span class="w"> </span><span class="n">PLATFORM_TAG_PARAMS</span><span class="w"> </span><span class="p">[</span><span class="n">PLATFORM_TAG_PARAMS</span><span class="w"> </span><span class="p">...]</span>
<span class="w">                            </span><span class="n">Update</span><span class="w"> </span><span class="n">platform</span><span class="w"> </span><span class="n">params</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">platform</span><span class="o">-</span><span class="n">tag</span><span class="w"> </span><span class="n">supplied</span>
<span class="w">                            </span><span class="n">in</span><span class="w"> </span><span class="n">config</span>
<span class="w">    </span><span class="o">-</span><span class="n">onnx</span><span class="o">-</span><span class="n">symbol</span><span class="w"> </span><span class="n">ONNX_SYMBOL</span><span class="w"> </span><span class="p">[</span><span class="n">ONNX_SYMBOL</span><span class="w"> </span><span class="p">...]</span>
<span class="w">                            </span><span class="n">Replace</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span><span class="n">symbols</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">input</span><span class="o">/</span><span class="n">output</span><span class="w"> </span><span class="n">shapes</span><span class="p">.</span><span class="n">Can</span><span class="w"> </span><span class="n">be</span>
<span class="w">                            </span><span class="n">passed</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">timesDefault</span><span class="w"> </span><span class="n">replaced</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span>
<span class="w">                            </span><span class="nl">__unk_200</span><span class="p">:</span><span class="mi">1</span>
<span class="w">    </span><span class="o">-</span><span class="n">set</span><span class="o">-</span><span class="n">global</span><span class="w"> </span><span class="n">SET_GLOBAL</span><span class="w"> </span><span class="p">[</span><span class="n">SET_GLOBAL</span><span class="w"> </span><span class="p">...]</span>
<span class="w">                            </span><span class="n">Replace</span><span class="w"> </span><span class="n">global</span><span class="w"> </span><span class="n">symbols</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">value</span><span class="p">.</span><span class="n">Can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">passed</span>
<span class="w">                            </span><span class="n">multiple</span><span class="w"> </span><span class="n">times</span><span class="p">.</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="w"> </span><span class="o">&lt;</span><span class="n">symbol</span><span class="o">&gt;:</span><span class="mi">2</span>
</pre></div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">evaluator</span><span class="w"> </span><span class="o">-</span><span class="n">config</span><span class="w"> </span><span class="p">{</span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">configs</span><span class="p">}</span><span class="o">/</span><span class="n">qnn_resnet50_config</span><span class="p">.</span><span class="n">yaml</span>
</pre></div>
</div>
<p><strong>Results</strong></p>
<p>Refer to the minimal mode section</p>
<p><strong>Config file options</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="w"> </span><span class="n">platform</span><span class="o">:</span>
<span class="w">    </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span>
<span class="w">    </span><span class="nl">target_arch</span><span class="p">:</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span>
<span class="w">    </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="n">cpu</span><span class="o">/</span><span class="n">htp</span>
<span class="w">    </span><span class="nl">device_ids</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="w">    </span><span class="nl">precision</span><span class="p">:</span><span class="w"> </span><span class="n">fp32</span><span class="o">/</span><span class="n">quant</span>
<span class="w">    </span><span class="nl">tag</span><span class="p">:</span><span class="w"> </span><span class="n">htp_int8</span>
<span class="w">    </span><span class="nl">runtime_params</span><span class="p">:</span>
<span class="w">        </span><span class="nl">vtcm_mb</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span>
<span class="w">        </span><span class="nl">rpc_control_latency</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span>
<span class="w">    </span><span class="nl">converter_params</span><span class="p">:</span>
<span class="w">        </span><span class="nl">param_quantizer</span><span class="p">:</span><span class="w"> </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">enhanced</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">adjusted</span>
<span class="w">        </span><span class="nl">act_quantizer</span><span class="p">:</span><span class="w"> </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span>
<span class="w">        </span><span class="nl">algorithms</span><span class="p">:</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">cle</span>
<span class="w">        </span><span class="nl">use_per_channel_quantization</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">False</span>
<span class="w">        </span><span class="nl">quantization_overrides</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;path to the ext quant json&quot;</span>
<span class="w">        </span><span class="nl">act_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">16</span>
<span class="w">        </span><span class="nl">bias_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">32</span>
<span class="w">        </span><span class="nl">weight_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span>
</pre></div>
</div>
<p><strong>Comparators</strong></p>
<p>Following are the comparators that can be used to compare the outputs. Some of the comparators output percentage match between
the two tensors and some output the absolute value, corresponding to the selected comparator.</p>
<ol class="arabic">
<li><p><strong>abs</strong> - Percentage match between the two tensors based on the relative tolerance threshold value</p></li>
<li><p><strong>cos</strong> - Percentage match between the two tensors based on the Cosine Similarity score</p></li>
<li><p><strong>topk</strong> - Percentage match between the two tensors based on the topk match between the two tensors</p></li>
<li><p><strong>avg</strong> - Percentage match between the two tensors based on the average difference between the two tensors</p></li>
<li><p><strong>l1norm</strong> - Percentage match between the two tensors based on the L1 Norm of the diff</p></li>
<li><p><strong>l2norm</strong> - Percentage match between the two tensors based on the L2 Norm of the diff</p></li>
<li><p><strong>std</strong> - Percentage match between the two tensors based on the standard deviation difference</p></li>
<li><p><strong>rme</strong> - Percentage match between the two tensors based on the RMSE between the tensors</p></li>
<li><p><strong>snr</strong> - Signal to Noise Ratio between the two tensors</p></li>
<li><p><strong>maxerror</strong> - max error value between the two tensors</p></li>
<li><p><strong>kld</strong> - KL Divergence value between the two tensors</p></li>
<li><p><strong>pixelbypixel</strong> - pixel by pixel plot difference between the two tensors. For each input i, plot is saved at <cite>{work_dir}/{platform}/Result_{i}</cite></p></li>
<li><p><strong>box</strong> - The box verifier requires the <cite>–box_input</cite> parameter which accepts the filename of a json file with the following format:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s">&quot;box&quot;</span><span class="o">:</span><span class="s">&quot;Result_0/detection_boxes_0.raw&quot;</span><span class="p">,</span>
<span class="s">&quot;class&quot;</span><span class="o">:</span><span class="s">&quot;Result_0/detection_classes_0.raw&quot;</span><span class="p">,</span>
<span class="s">&quot;score&quot;</span><span class="o">:</span><span class="s">&quot;Result_0/detection_scores_0.raw&quot;</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<p><strong>Directory of models</strong></p>
<p>In Minimal mode, User can also pass a directory of models. The directory structure is expected to be in the following format:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">model_dir</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="n">__model1</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="n">__Source_model</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__model</span><span class="p">.</span><span class="n">onnx</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="n">__inputs</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__data</span><span class="o">/</span><span class="n">images</span><span class="p">.</span><span class="n">raw</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__input_list</span><span class="p">.</span><span class="n">txt</span>
<span class="w">  </span><span class="o">|</span><span class="n">__model2</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="n">__Source_model</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__model</span><span class="p">.</span><span class="n">onnx</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="n">__inputs</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__data</span><span class="o">/</span><span class="n">images</span><span class="p">.</span><span class="n">raw</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__input_list</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>The output directory for each model will be available at <cite>{work_dir}/{model_dir}</cite></p>
</div>
</div>
<div class="section" id="architecture-checker-experimental">
<h3>Architecture Checker (Experimental)<a class="headerlink" href="#architecture-checker-experimental" title="Permalink to this headline">¶</a></h3>
<p>Architecture Checker is supported through the converter interface and is performed at conversion time. This tool is made for models running with HTP backend, including quantized 8-bit, quantized 16-bit and FP16 models.  It outputs a list of issues in the model that keep the model from getting better performance while running on the HTP backend.</p>
<p>The following option is available in each converter listed above to enable the checker:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Architecture</span><span class="w"> </span><span class="n">Checker</span><span class="w"> </span><span class="n">Options</span><span class="p">(</span><span class="n">Experimental</span><span class="p">)</span><span class="o">:</span>
<span class="o">--</span><span class="n">arch_checker</span><span class="w">        </span><span class="n">Pass</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">enable</span><span class="w"> </span><span class="n">architecture</span><span class="w"> </span><span class="n">checker</span><span class="w"> </span><span class="n">tool</span><span class="p">.</span>
<span class="w">                      </span><span class="n">This</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">experimental</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">models</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">intended</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">HTP</span>
<span class="w">                      </span><span class="n">backend</span><span class="p">.</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Note: If running on a quantized model, an input list with one image is good enough to satisfy the quantization requirement and have the tool run properly.</div>
<div class="line">Basic command line usage to trigger the checker using the TFLite converter would look like:</div>
</div>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-tflite-converter -i &lt;path&gt;/model.tflite
                       -d &lt;network_input_name&gt; &lt;dims&gt;
                       -o &lt;optional_output_path&gt;
                       -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
                       --arch_checker
</pre></div>
</div>
<p>The output is a csv file and will be saved as &lt;optional_output_path&gt;/&lt;model_name&gt;_architecture_checker.csv.
An example output is shown below:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Graph/Node_name</p></th>
<th class="head"><p>Input_tensor_name:[dims]</p></th>
<th class="head"><p>Output_tensor_name:[dims]</p></th>
<th class="head"><p>Issue</p></th>
<th class="head"><p>Recommendation</p></th>
<th class="head"><p>Additional parameters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Graph</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>This model uses 16-bit activation data. 16-bit activation data takes twice the amount of memory than 8-bit activation data does.</p></td>
<td><p>Try to use a smaller datatype to get better performance. E.g., 8-bit</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Node_name_1</p></td>
<td><p>input_1:[1, 250, 250, 3], __param_1:[5, 5, 3, 32], convolution_0_bias:[32]</p></td>
<td><p>output_1:[1, 123, 123, 32]</p></td>
<td><p>The number of channels in the input/output tensor of this convolution node is low (smaller than 32).</p></td>
<td><p>Try increasing the number of channels in the input/output tensor to 32 or greater to get better performance.</p></td>
<td><p>{‘package’: ‘qti.aisw’, ‘type’: ‘Conv2d’, ‘tensor_params’: …}</p></td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><strong>How to read the example output csv?</strong></div>
<div class="line">Row 1: This is an issue on the graph, the graph is using 16-bit activation data, as said in the recommendation, changing the activation from 16 bit to 8 bit gives better performance.</div>
<div class="line">Row 2: The issue is on the node which QNN node name is “Node_name_1”. This node has three inputs: input_1, __param_1 and convolution_0_bias where the dimensions are [1, 250, 250, 3], [5, 5, 3, 32] and [32] respectively. This node has one output with QNN tensor name output_1 and the dimension of this tensor is [1, 123, 123, 32]. There is full set of node parameters avaliable in the additional parameters’ column that can be used to locate the node inside the original model. The issue for this node is the channel of the input tensor is low, as the channel is smaller than 32, would recommend to increase the channel to at least 32 to get better performance on HTP backend. Currently the input dimension is [1, 250, 250, 3] and ideally have that to be [1, x, x, 32].</div>
</div>
<div class="line-block">
<div class="line"><strong>Is the QNN node/tensor name the same in the original model?</strong></div>
<div class="line">It is not the same but should be similar. There is naming sanitization in converter in order to meet the QNN naming standard. The input tensor, output tensor and all the additional parameters are avaliable in the output csv file to help locate the correct node inside the original model.</div>
</div>
</div>
<div class="section" id="qnn-accuracy-debugger-experimental">
<h3>qnn-accuracy-debugger (Experimental)<a class="headerlink" href="#qnn-accuracy-debugger-experimental" title="Permalink to this headline">¶</a></h3>
<p><strong>Dependencies</strong></p>
<p>The Accuracy debugger depends on the environment setup and platform dependencies as outlined in the <a class="reference internal" href="setup.html"><span class="doc">setup page</span></a>. User needs to run the envsetup.sh script and setup the ONNX and TensorFlow frameworks. User also need to install the following pip packages before using the tool.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">absl</span><span class="o">==</span><span class="mf">0.0</span>
<span class="n">absl_py</span><span class="o">==</span><span class="mf">0.13.0</span>
<span class="n">flatbuffers</span><span class="o">==</span><span class="mf">23.3.3</span>
<span class="n">onnxruntime</span><span class="o">==</span><span class="mf">1.14.1</span>
<span class="n">protobuf</span><span class="o">==</span><span class="mf">4.22.4</span>
</pre></div>
</div>
<p>The following environment variables are used inside this guide (User may change the following path depending on their needs):</p>
<ol class="arabic simple">
<li><p>RESOURCESPATH = {Path to the directory where all models and input files reside}</p></li>
<li><p>PROJECTREPOPATH = {Path to your accuracy debugger project directory}</p></li>
</ol>
<p><strong>Overview</strong></p>
<p>The <strong>accuracy-debugger</strong> tool finds inaccuracies in a neural-network at the layer
level. The tool compares the golden outputs produced by running a model
through a specific ML framework (ie. Tensorflow, Onnx, TFlite) with the results produced
by running the same model through Qualcomm’s QNN Inference Engine. The inference engine can be run on a variety of computing mediums including GPU, CPU and DSP.</p>
<p>There are three stages in running Accuracy Debugger. Each stage can be run using the binary with corresponding stage’s option like qnn-accuracy-debugger –{stage}. The three stages are as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>qnn-accuracy-debugger -–framework_diagnosis</strong> This stage uses frameworks ie. tensorflow, tflite and onnx to run the model to get intermediate outputs.</p></li>
<li><p><strong>qnn-accuracy-debugger –-inference_engine</strong> This stage uses QNN engine to run the models to get intermediate outputs.</p></li>
<li><p><strong>qnn-accuracy-debugger –-verification</strong> This stage compares the output generated by framework diagnosis and inference engine stages using the verifiers like CosineSimilarity, RtolAtol, etc.</p></li>
</ol>
</div></blockquote>
<dl class="simple">
<dt>Tip:</dt><dd><ul class="simple">
<li><p>You can use –help after the bin commands to see what other options (required or optional) you can add.</p></li>
<li><p>To run all three options together just skip the –{stage} option.</p></li>
</ul>
</dd>
</dl>
<p>Below you may read the step wise instructons for running Accuracy Debugger:</p>
<div class="section" id="step-1-framework-diagnosis">
<h4>Step 1: Framework Diagnosis<a class="headerlink" href="#step-1-framework-diagnosis" title="Permalink to this headline">¶</a></h4>
<p>The Framework diagnosis stage is designed to run models with different machine learning frameworks
(i.e. Tensorflow, etc). A selected model is run with a specific ML framework.
Golden outputs are produced for future comparison with inference result.</p>
</div>
<div class="section" id="id1">
<h4>Usage<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: ./qnn-accuracy-debugger --framework_diagnosis [-h]
                                   -f FRAMEWORK [FRAMEWORK ...]
                                   -m MODEL_PATH
                                   -i INPUT_TENSOR [INPUT_TENSOR ...]
                                   -o OUTPUT_TENSOR
                                   [-w WORKING_DIR]
                                   [--output_dirname OUTPUT_DIRNAME]
                                   [-v]

Script to generate intermediate tensors from an ML Framework.

optional arguments:
     -h, --help            show this help message and exit

required arguments:
     -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                             Framework type and version, version is optional. Currently
                             supported frameworks are [&quot;tensorflow&quot;,&quot;onnx&quot;,&quot;tflite&quot;] case
                             insensitive but spelling sensitive
     -m MODEL_PATH, --model_path MODEL_PATH
                             Path to the model file(s).
     -i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                             The name, dimensions, raw data, and optionally data
                             type of the network input tensor(s) specifiedin the
                             format &quot;input_name&quot; comma-separated-dimensions path-
                             to-raw-file, for example: &quot;data&quot; 1,224,224,3 data.raw
                             float32. Note that the quotes should always be
                             included in order to handle special characters,
                             spaces, etc. For multiple inputs specify multiple
                             --input_tensor on the command line like:
                             --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw
                             --input_tensor &quot;data2&quot; 1,50,100,3 data2.raw float32.
     -o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                             Name of the graph&#39;s specified output tensor(s).

     optional arguments:
     -w WORKING_DIR, --working_dir WORKING_DIR
                             Working directory for the framework_diagnosis to store
                             temporary files. Creates a new directory if the
                             specified working directory does not exist
     --output_dirname OUTPUT_DIRNAME
                             output directory name for the framework_diagnosis to
                             store temporary files under
                             &lt;working_dir&gt;/framework_diagnosis. Creates a new
                             directory if the specified working directory does not
                             exist
     -v, --verbose         Verbose printing

Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.
</pre></div>
</div>
<p><strong>Sample Commands</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework_diagnosis</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input:0&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="o">:</span><span class="mi">0</span>

<span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework_diagnosis</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">dlv3onnx</span><span class="o">/</span><span class="n">dlv3plus_mbnet_513</span><span class="mi">-513</span><span class="n">_op9_mod_basic</span><span class="p">.</span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="n">Input</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">513</span><span class="p">,</span><span class="mi">513</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">dlv3onnx</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="mo">00000</span><span class="n">_1_3_513_513</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">Output</span>
</pre></div>
</div>
<dl class="simple">
<dt>TIP:</dt><dd><ul class="simple">
<li><p>a working_directory, if not otherwise specified, is generated from wherever you are calling the script from; it is recommended to call all scripts from the same directory so all your outputs and results are stored under the same directory without having outputs everywhere</p></li>
<li><p>for tensorflow it is sometimes necessary to add the :0 after the input and output node name to signify the index of the node. Notice the :0 is dropped for onnx models.</p></li>
</ul>
</dd>
</dl>
<p><strong>Output</strong></p>
<p>The program also creates a folder named <em>latest</em> found in <cite>working_directory/framework_diagnosis</cite> which is symbolic linked to the most recently generated directory. In the example below, <em>latest</em> will
have data that is symbolic linked to the data in the most recent directory <em>YYYY-MM-DD_HH:mm:ss</em>. User may choose to override the directory name by passing it to –output_dirname (i.e. –output_dirname myTest1Ouput).</p>
<p>The <em>float data</em> produced by <strong>Framework Diagnosis</strong> step offers precise reference material for the <strong>Verification</strong> component to diagnose the accuracy of the network generated by <strong>Inference Engine Diagnosis</strong>
Unless a path is otherwise specified, network diagnosis will create directories within the <cite>working_directory/framework_diagnosis</cite> folder found in the current working directory. The directories will be named with the date
and time of the program’s execution, and contain tensor data. Depending on the tensor naming convention of the model, there may be numerous sub-directories within the new
directory. This occurs when tensor names include a slash “/”. For example, for the tensor names ‘inception_3a/1x1/bn/sc’, ‘inception_3a/1x1/bn/sc_internal’ and ‘inception_3a/1x1/bn’, we would end
up with subdirectories.</p>
<div class="figure align-default">
<img alt="../_static/resources/framework_diagnosis.png" src="../_static/resources/framework_diagnosis.png" />
</div>
<p>The figure above shows a sample output from one of the framework_diagnosis runs. Framework diagnosis basically runs inference at each op in the network.
InceptionV3 and Logits contain the outputs of each layer before the last layer. Each output directory contains the .raw files corresponding each node. Every raw file that can be seen is the output of an op.
The outputs of the final layer are saved inside Predictions directory. The file framework_diagnosis_options.json contains all the options this stage was using.</p>
</div>
<div class="section" id="step-2-inference-engine">
<h4>Step 2: Inference Engine<a class="headerlink" href="#step-2-inference-engine" title="Permalink to this headline">¶</a></h4>
<p>The Inference Engine stage is designed to find the outputs for a QNN SDK compatible model. The output produced by this step can be compared with the Golden outputs produced by the framework diagnosis step.</p>
</div>
<div class="section" id="id2">
<h4>Usage<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: ./qnn-accuracy-debugger --inference_engine [-h]
                                   [--stage {source,converted,compiled}] -r
                                   {cpu,gpu,dsp,dspv65,dspv66,dspv68,dspv69,dspv73,aic}
                                   -p ENGINE_PATH -a
                                   {aarch64-android,x86_64-linux-clang}
                                   -l INPUT_LIST
                                   [-i INPUT_TENSOR [INPUT_TENSOR ...]]
                                   [-o OUTPUT_TENSOR] [-m MODEL_PATH]
                                   [-f FRAMEWORK [FRAMEWORK ...]]
                                   [-qmcpp QNN_MODEL_CPP_PATH]
                                   [-qmbin QNN_MODEL_BIN_PATH]
                                   [-qmb QNN_MODEL_BINARY_PATH]
                                   [--deviceId DEVICEID] [-v]
                                   [--host_device {x86}] [-w WORKING_DIR]
                                   [--output_dirname OUTPUT_DIRNAME]
                                   [--engine_version ENGINE_VERSION]
                                   [--debug_mode_off]
                                   [--print_version PRINT_VERSION]
                                   [--offline_prepare] [-bbw {8,32}]
                                   [-abw {8,16}]
                                   [--golden_dir_for_mapping GOLDEN_DIR_FOR_MAPPING]
                                   [-wbw {8}] [--lib_name LIB_NAME]
                                   [-bd BINARIES_DIR] [-qmn MODEL_NAME]
                                   [-pq {tf,enhanced,adjusted,symmetric}]
                                   [-qo QUANTIZATION_OVERRIDES]
                                   [--act_quantizer {tf,enhanced,adjusted,symmetric}]
                                   [--algorithms ALGORITHMS]
                                   [--ignore_encodings]
                                   [--per_channel_quantization]
                                   [-idt {float,native}]
                                   [-odt {float_only,native_only,float_and_native}]
                                   [--profiling_level {basic,detailed}]
                                   [--perf_profile {low_balanced,balanced,high_performance,sustained_high_performance,burst,low_power_saver,power_saver,high_power_saver,extreme_power_saver,system_settings}]
                                   [--log_level {error,warn,info,debug,verbose}]
                                   [--qnn_model_net_json QNN_MODEL_NET_JSON]
                                   [--qnn_netrun_config_file QNN_NETRUN_CONFIG_FILE]
                                   [--extra_converter_args EXTRA_CONVERTER_ARGS]
                                   [--extra_runtime_args EXTRA_RUNTIME_ARGS]
                                   [--compiler_config COMPILER_CONFIG]
                                   [--precision {int8,fp16}]

Script to run QNN inference engine.

optional arguments:
     -h, --help            show this help message and exit

Core Arguments:
     --stage {source,converted,compiled}
                             Specifies the starting stage in the Accuracy Debugger
                             pipeline.
                             Source: starting with source framework model [default].
                             Converted: starting with model.cpp and .bin files.
                             Compiled: starting with a model&#39;s .so binary.
     -r {cpu,gpu,dsp,dspv65,dspv66,dspv68,dspv69,dspv73,aic}, --runtime {cpu,gpu,dsp,dspv65,dspv66,dspv68,dspv69,dspv73,aic}
                             Runtime to be used.
     -a {aarch64-android,x86_64-linux-clang}, --architecture {aarch64-android,x86_64-linux-clang}
                             Name of the architecture to use for inference engine.
     -l INPUT_LIST, --input_list INPUT_LIST
                             Path to the input list text.

Arguments required for SOURCE stage:
     -i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                             The name, dimension, and raw data of the network input
                             tensor(s) specified in the format &quot;input_name&quot; comma-
                             separated-dimensions path-to-raw-file, for example:
                             &quot;data&quot; 1,224,224,3 data.raw. Note that the quotes
                             should always be included in order to handle special
                             characters, spaces, etc. For multiple inputs specify
                             multiple --input_tensor on the command line like:
                             --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw
                             --input_tensor &quot;data2&quot; 1,50,100,3 data2.raw.
     -o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                             Name of the graph&#39;s output tensor(s).
     -m MODEL_PATH, --model_path MODEL_PATH
                             Path to the model file(s).
     -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                             Framework type to be used, followed optionally by
                             framework version.

Arguments required for CONVERTED stage:
     -qmcpp QNN_MODEL_CPP_PATH, --qnn_model_cpp_path QNN_MODEL_CPP_PATH
                             Path to the qnn model .cpp file
     -qmbin QNN_MODEL_BIN_PATH, --qnn_model_bin_path QNN_MODEL_BIN_PATH
                             Path to the qnn model .bin file

Arguments required for COMPILED stage:
     -qmb QNN_MODEL_BINARY_PATH, --qnn_model_binary_path QNN_MODEL_BINARY_PATH
                             Path to the qnn model .so binary.

Optional Arguments:
     --deviceId DEVICEID   The serial number of the device to use. If not
                             available, the first in a list of queried devices will
                             be used for validation.
     -v, --verbose         Verbose printing
     --host_device {x86}   The device that will be running conversion. Set to x86
                             by default.
     -w WORKING_DIR, --working_dir WORKING_DIR
                             Working directory for the inference_engine to store
                             temporary files. Creates a new directory if the
                             specified working directory does not exist
     --output_dirname OUTPUT_DIRNAME
                             output directory name for the inference_engine to
                             store temporary files under
                             &lt;working_dir&gt;/inference_engine .Creates a new
                             directory if the specified working directory does not
                             exist
     --engine_version ENGINE_VERSION
                             engine version, will retrieve the latest available if
                             not specified
     -p ENGINE_PATH, --engine_path ENGINE_PATH
                             Path to the inference engine.
     --debug_mode_off      Specifies if wish to turn off debug_mode mode.
     --print_version PRINT_VERSION
                             Print the QNN SDK version alongside the output.
     --offline_prepare     Use offline prepare to run qnn model.
     -bbw {8,32}, --bias_bitwidth {8,32}
                             option to select the bitwidth to use when quantizing
                             the bias. default 8
     -abw {8,16}, --act_bitwidth {8,16}
                             option to select the bitwidth to use when quantizing
                             the activations. default 8
     --golden_dir_for_mapping GOLDEN_DIR_FOR_MAPPING
                             Optional parameter to indicate the directory of the
                             goldens, it&#39;s used for tensor mapping without
                             framework.
     -wbw {8}, --weights_bitwidth {8}
                             option to select the bitwidth to use when quantizing
                             the weights. Only support 8 atm
     --lib_name LIB_NAME   Name to use for model library (.so file)
     -bd BINARIES_DIR, --binaries_dir BINARIES_DIR
                             Directory to which to save model binaries, if they
                             don&#39;t yet exist.
     -qmn MODEL_NAME, --model_name MODEL_NAME
                             Name of the desired output qnn model
     -pq {tf,enhanced,adjusted,symmetric}, --param_quantizer {tf,enhanced,adjusted,symmetric}
                             Param quantizer algorithm used.
     -qo QUANTIZATION_OVERRIDES, --quantization_overrides QUANTIZATION_OVERRIDES
                             Path to quantization overrides json file.
     --act_quantizer {tf,enhanced,adjusted,symmetric}
                             Optional parameter to indicate the activation
                             quantizer to use
     --algorithms ALGORITHMS
                             Use this option to enable new optimization algorithms.
                             Usage is: --algorithms &lt;algo_name1&gt; ... The available
                             optimization algorithms are: &#39;cle &#39; - Cross layer
                             equalization includes a number of methods for
                             equalizing weights and biases across layers in order
                             to rectify imbalances that cause quantization errors.
                             and bc - Bias correction adjusts biases to offse
                             activation quantization errors. Typically used in
                             conjunction with cle to improve quantization accuracy.
     --ignore_encodings    Use only quantizer generated encodings, ignoring any
                             user or model provided encodings.
     --per_channel_quantization
                             Use per-channel quantization for convolution-based op
                             weights.
     -idt {float,native}, --input_data_type {float,native}
                             the input data type, must match with the supplied
                             inputs
     -odt {float_only,native_only,float_and_native}, --output_data_type {float_only,native_only,float_and_native}
                             the desired output data type
     --profiling_level {basic,detailed}
                             Enables profiling and sets its level.
     --perf_profile {low_balanced,balanced,high_performance,sustained_high_performance,burst,low_power_saver,power_saver,high_power_saver,extreme_power_saver,system_settings}
     --log_level {error,warn,info,debug,verbose}
                             Enable verbose logging.
     --qnn_model_net_json QNN_MODEL_NET_JSON
                             Path to the qnn model net json. Only necessary if its being run from the converted stage. It has information about what structure the data is in within framework_diagnosis and inference_engine steps.
                             This file is required to generate model_graph_struct.json file which is good to have in the verification stage.
     --qnn_netrun_config_file QNN_NETRUN_CONFIG_FILE
                             allow backend_extention features to be applied during
                             qnn-net-run
     --extra_converter_args EXTRA_CONVERTER_ARGS
                             additional convereter arguments in a string. example:
                             --extra_converter_args input_dtype=data
                             float;input_layout=data1 NCHW
     --extra_runtime_args EXTRA_RUNTIME_ARGS
                             additional convereter arguments in a quoted string.
                             example: --extra_runtime_args
                             profiling_level=basic;log_level=debug
     --compiler_config COMPILER_CONFIG
                             Path to the compiler config file.
     --precision {int8,fp16}
                             select precision

Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.
</pre></div>
</div>
<p>The inference engine config file can be found in <cite>{accuracy_debugger tool root directory}/python/qti/aisw/accuracy_debugger/lib/inference_engine/configs/config_files</cite> and is a <strong>JSON</strong> file. This config file
stores information that helps the inference engine know which tool and parameters to read in. Each different inference engine, and possibly engine versions in certain cases, will require its own config file.</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_engine</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input:0&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_engine</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">deviceId</span><span class="w"> </span><span class="mi">357415</span><span class="n">c4</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input:0&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<dl class="simple">
<dt>Tip:</dt><dd><ul class="simple">
<li><p>for runtime (choose from ‘cpu’, ‘gpu’, ‘dsp’, ‘dspv65’, ‘dspv66’, ‘dspv68’, ‘dspv69’, ‘dspv73’, ‘aic’). Make sure the runtime is 73 for kailua, 69 for waipio, etc.</p></li>
<li><p>the input_tensor (–i) and output_tensor (-o) does not need the :0 indexing like when runing tensorflow framework diagnosis</p></li>
<li><p>two files, namely tensor_mapping.json and qnn_model_graph_struct.json are generated to be used in verification, be sure to locate these 2 files in the working_directory/inference_engine/latest</p></li>
</ul>
</dd>
</dl>
<p>More example commands running from different stages:</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">source</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">stage</span><span class="o">:</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">above</span><span class="w"> </span><span class="n">section</span><span class="w"> </span><span class="p">(</span><span class="n">stage</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="s">&quot;source&quot;</span><span class="p">)</span>

<span class="n">running</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">converted</span><span class="w"> </span><span class="n">stage</span><span class="w"> </span><span class="p">(</span><span class="n">x86</span><span class="p">)</span><span class="o">:</span>
<span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_engine</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">stage</span><span class="w"> </span><span class="n">converted</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">qmcpp</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen_qnn_model</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">qmbin</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen_qnn_model</span><span class="p">.</span><span class="n">bin</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">qnn_model_net_json</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen_qnn_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">golden_dir_for_mapping</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">golden_from_framework_diagnosis</span><span class="o">/</span>

<span class="n">Android</span><span class="w"> </span><span class="n">Devices</span><span class="w"> </span><span class="p">(</span><span class="n">ie</span><span class="p">.</span><span class="w"> </span><span class="n">MTP</span><span class="p">)</span><span class="o">:</span>
<span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_engine</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">stage</span><span class="w"> </span><span class="n">converted</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">qmcpp</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen_qnn_model</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">qmbin</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen_qnn_model</span><span class="p">.</span><span class="n">bin</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">deviceId</span><span class="w"> </span><span class="n">f366ce60</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">qnn_model_net_json</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen_qnn_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">golden_dir_for_mapping</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">golden_from_framework_diagnosis</span><span class="o">/</span>


<span class="n">running</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">compiled</span><span class="w"> </span><span class="n">stage</span><span class="w"> </span><span class="p">(</span><span class="n">x86</span><span class="p">)</span><span class="o">:</span>

<span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_engine</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">stage</span><span class="w"> </span><span class="n">compiled</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">qnn_model_binary</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">qnn_model_binaries</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">libqnn_model</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">qnn_model_net_json</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen_qnn_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">golden_dir_for_mapping</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">golden_from_framework_diagnosis</span><span class="o">/</span>

<span class="n">Android</span><span class="w"> </span><span class="n">devices</span><span class="w"> </span><span class="p">(</span><span class="n">ie</span><span class="w"> </span><span class="n">MTP</span><span class="p">)</span><span class="o">:</span>
<span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_engine</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">stage</span><span class="w"> </span><span class="n">compiled</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">qnn_model_binary</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">qnn_model_binaries</span><span class="o">/</span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="o">/</span><span class="n">libqnn_model</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">qnn_model_net_json</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen_qnn_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">golden_dir_for_mapping</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">golden_from_framework_diagnosis</span><span class="o">/</span>
</pre></div>
</div>
<dl class="simple">
<dt>Tip:</dt><dd><ul class="simple">
<li><p>The qnn_model_net_json file is not required to run this stage. However, it is needed to build the qnn_model_graph_struct.json, which would be good to have in the Verification Stage. The model_net.json file is generated from the original model -&gt; converted model stage. Hence if you are debugging this model from converted model stage, it is recommended to ask for this mode_net.json file.</p></li>
<li><p>framework and golden_dir_for_mapping, or just golden_dir_for_mapping itself is an alternative to the original model to be provided to generate the tensor_mapping.json, however, providing only the golden_dir_for_mapping, the get_tensor_mapping module will try it’s best to map but it is not guaranteed the mapping would be 100% accurate.</p></li>
</ul>
</dd>
</dl>
<p><strong>Output</strong></p>
<p>Once the inference engine has finished running, it will store the output in the specified directory
(or the current working directory by default) and store the files in that folder. By default, it
will store the output in <cite>working_directory/inference_engine</cite> in the current working directory.</p>
<div class="figure align-default">
<img alt="../_static/resources/inference_engine.png" src="../_static/resources/inference_engine.png" />
</div>
<p>The figure above shows the sample output from one of the runs of inference engine step. The output directory contains raw files. Each raw file is an output of an op involved in various layers in the network. model.bin and model.cpp are the files created
by the model converter. The directory qnn_model_binaries contains the .so file that is generated by modellibgenerator utility. File image_list.txt contains the path for sample test images. inference_engine_options.json contains all the options with which this run was launched.
In addition to generating the .raw files, inference_engine also generates the model’s graph structure in a .json file. The name of the file is the same as the name of the protobuf model file.
model_graph_struct.json aids in providing the structure related information of the converted model graph during the verification stage. Specifically, it helps with organizing the nodes in order (for e.g. the beginning nodes should come earlier than ending nodes).
It helps to see the nodes in a streamlined manner. model_net.json has information about what structure the data is in within framework_diagnosis and inference_engine steps (data can be in different formats for e.g. channels first vs channels last). So that in the
verification stage the data can be properly transposed and compared so that the data is in the similar format. It is an optional parameter which can be provided during inference engine stage for generating the model_graph_struct.json file (mandate only when running inference engine from the converted stage).
Finally, tensor_mapping file contains a mapping of the various intermediate output file names generated from the framework diagnosis step and the inference engine step.</p>
<div class="figure align-default">
<img alt="../_static/resources/inference_engine_2.png" src="../_static/resources/inference_engine_2.png" />
</div>
<p>The created .raw files are organized in the same manner as framework_diagnosis (see above).</p>
</div>
<div class="section" id="step-3-verification">
<h4>Step 3: Verification<a class="headerlink" href="#step-3-verification" title="Permalink to this headline">¶</a></h4>
<p>The Verification step compares the output (from the intermediate tensors of a given model) produced by framework diagonsis step with the one that’s produced by the inference engine step.
Once the comparison is complete the verification results are compiled and displayed visually in a format that can be easily interpreted by the user.</p>
<p>There are different types of verifiers for e.g.: CosineSimilarity, RtolAtol, etc. To see what all verifiers are there please use the –help option like ./qnn-accuracy-debugger –verification –help.
Each verifier compares the Framework Diagnosis and Inference Engine output using an error metric. It also prepares reports and/or visualizations to help the user analyze the network’s error data.</p>
</div>
<div class="section" id="id3">
<h4>Usage<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: ./qnn-accuracy-debugger --verification [-h]

Script to run verification.

required arguments:
     --default_verifier DEFAULT_VERIFIER [DEFAULT_VERIFIER ...]
                             Default verifier used for verification. The options
                             &quot;RtolAtol&quot;, &quot;AdjustedRtolAtol&quot;, &quot;TopK&quot;, &quot;L1Error&quot;,
                             &quot;CosineSimilarity&quot;, &quot;MSE&quot;, &quot;MAE&quot;, &quot;SQNR&quot;, &quot;MeanIOU&quot;,
                             &quot;ScaledDiff&quot; are supported. An optional list of
                             hyperparameters can be appended. For example:
                             --default_verifier
                             rtolatol,rtolmargin,0.01,atolmargin,0,01. An optional
                             list of placeholders can be appended. For example:
                             --default_verifier CosineSimilarity param1 1 param2 2.
                             to use multiple verifiers, add additional
                             --default_verifier CosineSimilarity
     --framework_results FRAMEWORK_RESULTS
                             Path to root directory generated from framework
                             diagnosis. Paths may be absolute, or relative to the
                             working directory.
     --inference_results INFERENCE_RESULTS
                             Path to root directory generated from inference engine
                             diagnosis. Paths may be absolute, or relative to the
                             working directory.

optional arguments:
     --tensor_mapping TENSOR_MAPPING
                             Path to the file describing the tensor name mapping
                             between inference and golden tensors.can be generated
                             with in the inference engine step.
     --verifier_config VERIFIER_CONFIG
                             Path to the verifiers&#39; config file
     --graph_struct GRAPH_STRUCT
                             Path to the inference graph structure .json file. This file
                             aids in providing the structure related information of the converted model graph during this stage.
     -v, --verbose         Verbose printing
     -w WORKING_DIR, --working_dir WORKING_DIR
                             Working directory for the verification to store
                             temporary files. Creates a new directory if the
                             specified working directory does not exist
     --output_dirname OUTPUT_DIRNAME
                             output directory name for the verification to store
                             temporary files under &lt;working_dir&gt;/verification.
                             Creates a new directory if the specified working
                             directory does not exist
     --qnn_model_json_path QNN_MODEL_JSON_PATH
                             Path to the model json for transforming intermediate
                             tensors to spatial-first axis order.

arguments for generating a new tensor_mapping.json:
     -m MODEL_PATH, --model_path MODEL_PATH
                             path to original model for tensor_mapping uses here.
     -e ENGINE_NAME [ENGINE_VERSION ...], --engine ENGINE_NAME [ENGINE_VERSION ...]
                             Name of engine that will be running inference,
                             optionally followed by the engine version. Used here
                             for tensor_mapping.
     -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                             Framework type to be used, followed optionally by
                             framework version. Used here for tensor_mapping.

Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.
</pre></div>
</div>
<p>The main verification process run using ./qnn-accuracy-debugger –verification optionally uses –tensor_mapping and –graph_struct to find files to compare.  These files are generated by the
inference engine step, and should be supplied to verification for best results.  By default they are named tensor_mapping.json and {model name}_graph_struct.json, and can be
found in the output directory of the inference engine results.</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Compare</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">framework</span><span class="w"> </span><span class="n">diagnosis</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">engine</span><span class="o">:</span>

<span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">verification</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span><span class="w"> </span><span class="n">param1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">param2</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">SQNR</span><span class="w"> </span><span class="n">param1</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="n">param2</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">framework_results</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">framework_diagnosis</span><span class="o">/</span><span class="mi">2022-10-31</span><span class="n">_17</span><span class="mo">-07</span><span class="mi">-58</span><span class="o">/</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">inference_results</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">Result_0</span><span class="o">/</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">tensor_mapping</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">tensor_mapping</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">graph_struct</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">qnn_model_graph_struct</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">qnn_model_json_path</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">qnn_model_net</span><span class="p">.</span><span class="n">json</span>
</pre></div>
</div>
<dl class="simple">
<dt>Tip:</dt><dd><ul class="simple">
<li><p>If you passed multiple images in the image_list.txt from run inference engine diagnosis, you’ll receive multiple output/Result_x, choose result that matches the input you used for framework diagnosis for comparison (ie. in framework you used chair.raw and inference chair.raw was the first item in the image_list.txt then choose output/Result_0, if chair.raw was the second item in image_list.txt, then choose output/Result_1).</p></li>
<li><p>It is recommended to always supply ‘graph_struct’ and ‘tensor_mapping’ to the command as it is used to line up the report and find the corresponding files for comparison. if tensor_mapping did not get generated from previous steps, you can supplement with ‘model_path’, ‘engine’, ‘framework’ to have module generate ‘tensor_mapping’ during runtime.</p></li>
<li><p>You can also compare inference_engine outputs to inference_engine outputs by passing the /output of the inference_engine output to the ‘framework_results’. If you want the outputs to be exact-name-matching, then you do not need to provide a tensor_mapping file.</p></li>
<li><p>Note that if you need to generate a tensor mapping instead of providing a path to prexisting tensor mapping file. You can provide the ‘model_path’ option.</p></li>
</ul>
</dd>
</dl>
<p>Verifier uses two optional config files. The first file is used to prefer parameters to specific
verifiers, as well as which tensors to use these verifiers on. The second file is used to map tensor
names from inference_engine to framework_diagnosis, since certain tensors generated by
framework_diagnosis have different names than tensors generated by inference_engine.</p>
<p>Verifier Config:</p>
<p>The verifier config file is a JSON file that tells verification which verifiers
(asides from the default verifier) to use and with which parameters and on what specific tensors.
If no config file is provided, the tool will only use the default verifier specified from the
command line, with its default parameters, on all the tensors. The JSON file is keyed by verifier
names, with each verifier as its own dictionary keyed by “parameters” and “tensors”.</p>
<p><strong>Config File</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>```json
{
    &quot;MeanIOU&quot;: {
        &quot;parameters&quot;: {
            &quot;background_classification&quot;: 1.0
        },
        &quot;tensors&quot;: [[&quot;Postprocessor/BatchMultiClassNonMaxSuppression_boxes&quot;, &quot;detection_classes:0&quot;]]
    },
    &quot;TopK&quot;: {
        &quot;parameters&quot;: {
            &quot;k&quot;: 5,
            &quot;ordered&quot;: false
        },
        &quot;tensors&quot;: [[&quot;Reshape_1:0&quot;], [&quot;detection_classes:0&quot;]]
    }
}
```
</pre></div>
</div>
<p>Note that the “tensors” field is a list of lists. This is done because specific verifiers
(e.g. MeanIOU) runs on two tensor at a time. Hence the two tensors are placed in a list. Otherwise
if a verifier only runs on one tensor, it will have a list of lists with only one tensor name in
each list.</p>
<p>Tensor Mapping:</p>
<p>Tensor mapping is a JSON file keyed by inference tensor names, of framework tensor names. If the
tensor mapping is not provided, the tool will assume inference and golden tensor names are
identical.</p>
<p><strong>Tensor Mapping File</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>```json
{
    &quot;Postprocessor/BatchMultiClassNonMaxSuppression_boxes&quot;: &quot;detection_boxes:0&quot;,
    &quot;Postprocessor/BatchMultiClassNonMaxSuppression_scores&quot;: &quot;detection_scores:0&quot;
}
```
</pre></div>
</div>
<p><strong>Output</strong></p>
<p>Verification’s output is divided into different verifiers. For example, if both RtolAtol and TopK
verifiers are used, there will be two sub-folders named “RtolAtol” and “TopK”. Availble verifiers can be
found by just issuing –help option.</p>
<div class="figure align-default">
<img alt="../_static/resources/verification_2.png" src="../_static/resources/verification_2.png" />
</div>
<p>Under each sub-folder, the verification analysis for each tensor is organized similar to how
framework_diagnosis (see above) and inference_engine are organized. For each tensor, a CSV and HTML
file is generated. In addition to the tensor-specific analysis, the tool also generates a summary
CSV and HTML file which summarizes the data from all verifiers and their subsequent tensors. The following
figure shows how a sample summary generated in the verification step looks. Each row in this summary corresponds to
one tensor name that is identified by the framework diagnosis and inference engine steps. The final column shows
cosinesimilarity score which can vary between 0 to 1 (this range might be different for other verifiers). If the score is high enough then it means that the result produced
by both the steps for that particular tensor are fairly similar. However, if the score is too low then it gives the developer a point of
inspection. The developer can then further investigate those specific tensors (if multiple) into details. Developer should inspect tensors from top-to-bottom order,
meaning if a tensor is broken at an earlier node, anything that was generated post that node is unreliable until that node
is properly fixed. Hence, this process might help in improving the overall accuracy of the sdk. Thats how this tool helps in debugging.</p>
<div class="figure align-default">
<img alt="../_static/resources/verification_results.png" src="../_static/resources/verification_results.png" />
</div>
</div>
<div class="section" id="run-qnn-accuracy-debugger-e2e">
<h4>Run QNN Accuracy Debugger E2E<a class="headerlink" href="#run-qnn-accuracy-debugger-e2e" title="Permalink to this headline">¶</a></h4>
<p>This feature is designed to run all three steps namely framework diagnosis, inference engine and verification sequentially with a single command.</p>
</div>
<div class="section" id="id4">
<h4>Usage<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-accuracy-debugger [--framework_diagnosis] [--inference_engine] [--verification] [-h]

Script that runs Framework Diagnosis, Inference Engine or Verification.

Arguments to select which component of the tool to run.  Arguments are mutually exclusive (at most 1 can be selected).  If none are selected, then all components are run:
    --framework_diagnosis
                          Run framework
    --inference_engine    Run inference engine
    --verification        Run verification

optional arguments:
    -h, --help            Show this help message. To show help for any of the
                          components, run script with --help and --&lt;component&gt;.
                          For example, to show the help for Framework Diagnosis,
                          run script with the following: --help
                          --framework_diagnosis
usage: qnn-accuracy-debugger [-h] -f FRAMEWORK [FRAMEWORK ...] -m MODEL_PATH
                            -i INPUT_TENSOR [INPUT_TENSOR ...] -o
                            OUTPUT_TENSOR -r RUNTIME -a
                            {aarch64-android,x86_64-linux-clang}
                            -l INPUT_LIST --default_verifier DEFAULT_VERIFIER
                            [DEFAULT_VERIFIER ...] [-v] [-w WORKING_DIR]
                            [--output_dirname OUTPUT_DIRNAME]
                            [--deep_analyzer {modelDissectionAnalyzer}]

Options for running the Accuracy Debugger components

optional arguments:
    -h, --help            show this help message and exit

Arguments required by both Framework Diagnosis and Inference Engine:
    -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                          Framework type and version, version is optional. For
                          example &quot;tensorflow 2.3.0&quot;.
    -m MODEL_PATH, --model_path MODEL_PATH
                          Path to the model file(s).
    -i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                          The name, dimensions, raw data, and optionally data
                          type of the network input tensor(s) specifiedin the
                          format &quot;input_name&quot; comma-separated-dimensions path-
                          to-raw-file, for example: &quot;data&quot; 1,224,224,3 data.raw
                          float32. Note that the quotes should always be
                          included in order to handle special characters,
                          spaces, etc. For multiple inputs specify multiple
                          --input_tensor on the command line like:
                          --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw
                          --input_tensor &quot;data2&quot; 1,50,100,3 data2.raw float32.
    -o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                          Name of the graph&#39;s specified output tensor(s).

Arguments required by Inference Engine:
    -r RUNTIME, --runtime RUNTIME
                          Runtime to be used for inference.
    -a {aarch64-android,x86_64-linux-clang}, --architecture {aarch64-android,x86_64-linux-clang}
                          Name of the architecture to use for inference engine.
    -l INPUT_LIST, --input_list INPUT_LIST
                          Path to the input list text.

Arguments required by Verification:
    --default_verifier DEFAULT_VERIFIER [DEFAULT_VERIFIER ...]
                          Default verifier used for verification. The options
                          &quot;RtolAtol&quot;, &quot;AdjustedRtolAtol&quot;, &quot;TopK&quot;, &quot;L1Error&quot;,
                          &quot;CosineSimilarity&quot;, &quot;MSE&quot;, &quot;MAE&quot;, &quot;SQNR&quot;, &quot;MeanIOU&quot;,
                          &quot;ScaledDiff&quot; are supported. An optional list of
                          hyperparameters can be appended. For example:
                          --default_verifier
                          rtolatol,rtolmargin,0.01,atolmargin,0,01. An optional
                          list of placeholders can be appended. For example:
                          --default_verifier CosineSimilarity param1 1 param2 2.
                          to use multiple verifiers, add additional
                          --default_verifier CosineSimilarity

optional arguments:
    -v, --verbose         Verbose printing
    -w WORKING_DIR, --working_dir WORKING_DIR
                          Working directory for the wrapper to store temporary
                          files. Creates a new directory if the specified
                          working directory does not exitst.
    --output_dirname OUTPUT_DIRNAME
                          output directory name for the wrapper to store
                          temporary files under &lt;working_dir&gt;/wrapper. Creates a
                          new directory if the specified working directory does
                          not exist
    --deep_analyzer {modelDissectionAnalyzer}
                          Deep Analyzer to perform deep analysis
</pre></div>
</div>
<p><strong>Sample Commands</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input:0&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span><span class="n">$PATHTOGOLDENI</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="o">:</span><span class="mi">0</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework_results</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">framework_diagnosis</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_results</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">Result_0</span><span class="o">/</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">tensor_mapping</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">tensor_mapping</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">graph_struct</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">qnn_model_graph_struct</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">qnn_model_json_path</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">qnn_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<p>The program creates the respective output directories/files as discussed in framework diagnosis, inference engine and verification.</p>
</div>
</div>
<div class="section" id="qnn-platform-validator">
<h3>qnn-platform-validator<a class="headerlink" href="#qnn-platform-validator" title="Permalink to this headline">¶</a></h3>
<p>qnn-platform-validator checks the QNN compatibility/capability of a device. The output is saved in a CSV file in the
“output” directory, in a csv format. Basic logs are also displayed on the console.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">DESCRIPTION</span><span class="p">:</span>
<span class="o">------------</span>
<span class="n">Helper</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">launch</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">platform</span><span class="o">-</span>
<span class="n">validator</span><span class="w"> </span><span class="n">executable</span><span class="p">.</span>

<span class="n">REQUIRED</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="o">--</span><span class="n">backend</span><span class="w">            </span><span class="o">&lt;</span><span class="n">BACKEND</span><span class="o">&gt;</span><span class="w">          </span><span class="n">Specify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">validate</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">gpu</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="o">&lt;</span><span class="n">dsp</span><span class="o">&gt;</span>
<span class="w">                                        </span><span class="o">&lt;</span><span class="n">all</span><span class="o">&gt;</span><span class="p">.</span>

<span class="o">--</span><span class="n">directory</span><span class="w">          </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;</span><span class="w">              </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">root</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">unpacked</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">containing</span>
<span class="w">                                        </span><span class="n">the</span><span class="w"> </span><span class="n">executable</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">files</span>

<span class="o">--</span><span class="n">dsp_type</span><span class="w">           </span><span class="o">&lt;</span><span class="n">DSP_VERSION</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Specify</span><span class="w"> </span><span class="n">DSP</span><span class="w"> </span><span class="n">variant</span><span class="o">:</span><span class="w"> </span><span class="n">v66</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">v68</span>

<span class="n">OPTIONALS</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">--------------------</span>
<span class="o">--</span><span class="n">buildVariant</span><span class="w">       </span><span class="o">&lt;</span><span class="n">TOOLCHAIN</span><span class="o">&gt;</span><span class="w">        </span><span class="n">Specify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="n">variant</span>
<span class="w">                                        </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">windows</span><span class="o">-</span><span class="n">msvc</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">validated</span><span class="p">.</span>
<span class="w">                                        </span><span class="nl">Default</span><span class="p">:</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span>

<span class="o">--</span><span class="n">testBackend</span><span class="w">                           </span><span class="n">Runs</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">small</span><span class="w"> </span><span class="n">program</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Checks</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">supported</span><span class="w"> </span><span class="k">for</span>
<span class="w">                                        </span><span class="n">backend</span><span class="p">.</span>

<span class="o">--</span><span class="n">deviceId</span><span class="w">           </span><span class="o">&lt;</span><span class="n">DEVICE_ID</span><span class="o">&gt;</span><span class="w">        </span><span class="n">Uses</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">command</span><span class="p">.</span>
<span class="w">                                        </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">devices</span><span class="w"> </span><span class="n">list</span><span class="p">..</span>

<span class="o">--</span><span class="n">coreVersion</span><span class="w">                           </span><span class="n">Outputs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">target</span><span class="p">.</span>

<span class="o">--</span><span class="n">libVersion</span><span class="w">                            </span><span class="n">Outputs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">target</span><span class="p">.</span>

<span class="o">--</span><span class="n">targetPath</span><span class="w">          </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;</span><span class="w">             </span><span class="n">The</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">device</span><span class="p">.</span>
<span class="w">                                        </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">platformValidator</span>

<span class="o">--</span><span class="n">remoteHost</span><span class="w">         </span><span class="o">&lt;</span><span class="n">REMOTEHOST</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Run</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">remote</span><span class="w"> </span><span class="n">host</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="n">remote</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">server</span><span class="p">.</span>
<span class="w">                                        </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">localhost</span><span class="p">.</span>

<span class="o">--</span><span class="n">debug</span><span class="w">                                 </span><span class="n">Set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Debug</span><span class="w"> </span><span class="n">log</span>
</pre></div>
</div>
</div>
<div class="section" id="qnn-profile-viewer">
<h3>qnn-profile-viewer<a class="headerlink" href="#qnn-profile-viewer" title="Permalink to this headline">¶</a></h3>
<p>The <strong>qnn-profile-viewer</strong> tool is used to parse profiling data that is generated using
<strong>qnn-net-run</strong>. Additionally, the same data can be saved to a csv file.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">profile</span><span class="o">-</span><span class="n">viewer</span><span class="w"> </span><span class="o">--</span><span class="n">input_log</span><span class="w"> </span><span class="n">PROFILING_LOG</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">output</span><span class="o">=</span><span class="n">CSV_FILE</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">reader</span><span class="o">=</span><span class="n">CUSTOM_READER_SHARED_LIB</span><span class="p">]</span>

<span class="n">Reads</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">contents</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">stdout</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">input_log</span><span class="w">     </span><span class="n">PROFILING_LOG</span>
<span class="w">                  </span><span class="n">Profiling</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">file</span><span class="p">.</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">output</span><span class="w">        </span><span class="n">PATH</span>
<span class="w">                  </span><span class="n">Output</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">processed</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="w"> </span><span class="n">File</span><span class="w"> </span><span class="n">formats</span><span class="w"> </span><span class="n">vary</span><span class="w"> </span><span class="n">depending</span><span class="w"> </span><span class="n">upon</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">reader</span><span class="w"> </span><span class="n">used</span>
<span class="w">                  </span><span class="p">(</span><span class="n">see</span><span class="w"> </span><span class="o">--</span><span class="n">reader</span><span class="p">).</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">provided</span><span class="p">,</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">created</span><span class="p">.</span>

<span class="w">  </span><span class="o">-</span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">help</span><span class="w">      </span><span class="n">Displays</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">reader</span><span class="w">        </span><span class="n">CUSTOM_READER_SHARED_LIB</span>
<span class="w">                  </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">reader</span><span class="w"> </span><span class="n">library</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">specified</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">reader</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">CSV</span><span class="w"> </span><span class="n">file</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">version</span><span class="w">       </span><span class="n">Displays</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">information</span><span class="p">.</span>
</pre></div>
</div>
</div>
<div class="section" id="qnn-netron-beta">
<h3>qnn-netron (Beta)<a class="headerlink" href="#qnn-netron-beta" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>QNN Netron tool is provided as Beta quality (it is undergoing more rigorous testing and may have known bugs or limitations).</p>
</div>
<div class="section" id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h4>
<p>QNN Netron tool is making model debugging and visualization less daunting. qnn-netron is an extension of the
<a class="reference external" href="https://github.com/lutzroeder/netron">netron</a> graph tool. It provides for easier graph debugging and convenient runtime information.
There are currently two key functionalities of the tool:</p>
<ol class="arabic simple">
<li><p>The Visualize section allows customers to view their desired models after using the QNN Converter by importing the
JSON representation of the model</p></li>
<li><p>The Diff section allows customers to run networks of their choosing on different
runtimes in order to compare network accuracy and performance</p></li>
</ol>
</div>
<div class="section" id="launching-tool">
<h4>Launching Tool<a class="headerlink" href="#launching-tool" title="Permalink to this headline">¶</a></h4>
<p><strong>Dependencies</strong></p>
<p>The QNN netron tool leverages electron JS framework for building GUI frontend and depends on npm/node_js to be available
in system. Additionally, python libraries for accuracy analysis are required by backend of tool. A convenient script is
available in the QNN SDK to download necessary dependencies for building and running the tool.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># Note: following command should be run as administrator/root to be able to install system libraries</span>
<span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">bash</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">check</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">dependency</span><span class="p">.</span><span class="n">sh</span>
<span class="n">$</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">check</span><span class="o">-</span><span class="n">python</span><span class="o">-</span><span class="n">dependency</span>
</pre></div>
</div>
<p><strong>Launching Application</strong></p>
<p><cite>qnn-netron</cite> script is used to be able to launch the QNN Netron application. This script:</p>
<ol class="arabic simple">
<li><p>Clones vanilla netron git project</p></li>
<li><p>Applies custom patches for enabling Netron for QNN</p></li>
<li><p>Build the npm project</p></li>
<li><p>Launches application</p></li>
</ol>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">netron</span><span class="w"> </span><span class="o">-</span><span class="n">h</span>
<span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">netron</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="n">working_dir</span><span class="o">&gt;</span><span class="p">]</span>
<span class="n">Script</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">launch</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">Netron</span><span class="w"> </span><span class="n">tool</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">visualizing</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Qnn</span><span class="w"> </span><span class="n">Models</span><span class="p">.</span>

<span class="n">Optional</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">-</span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="n">working_dir</span><span class="o">&gt;</span><span class="w">                      </span><span class="n">Location</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">building</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">Netron</span><span class="w"> </span><span class="n">tool</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="o">:</span><span class="w"> </span><span class="n">current_dir</span>


<span class="cp"># To build and run application use</span>
<span class="n">$</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">netron</span><span class="w"> </span><span class="o">-</span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="n">my_working_dir</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="qnn-netron-visualize-deep-dive">
<h4>QNN Netron Visualize Deep Dive<a class="headerlink" href="#qnn-netron-visualize-deep-dive" title="Permalink to this headline">¶</a></h4>
<p>First, the user is prompted to open a JSON file that represents their converted model. This JSON comes from the
converter tool. Please refer to this <a class="reference internal" href="converters.html#overview"><span class="std std-ref">Overview</span></a> for more details.</p>
<div class="figure align-default">
<img alt="../_static/resources/landing_page_netron.jpg" src="../_static/resources/landing_page_netron.jpg" />
</div>
<p>Once the file is loaded into the tool, the graph should be displayed in the UI as shown below:</p>
<p>After loading in the model, the user can click on any of the nodes and a side pop-up section will display node
information such as the type and name as well as vital parameter information such as inputs and outputs
(datatypes, encodings, and shapes)</p>
<div class="figure align-default">
<img alt="../_static/resources/netron_detailed_nodes_visualization.jpg" src="../_static/resources/netron_detailed_nodes_visualization.jpg" />
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="netron-diff-customization-deep-dive">
<h4>Netron Diff Customization Deep Dive<a class="headerlink" href="#netron-diff-customization-deep-dive" title="Permalink to this headline">¶</a></h4>
<p><strong>Limitations</strong></p>
<ol class="arabic simple">
<li><p>Diff Tool comparison between source framework goldens only works for framework goldens that are spatial first axis order. (NHWC)</p></li>
<li><p>For usecases where source framework golden is used for comparison, Diff Tool is only tested to work for tensorflow and tensorflow variant frameworks.</p></li>
</ol>
<p>In order for the user to open the Diff Customization tool, they can either click file and then “Open Diff…” or on
tool startup by clicking “Diff…” as shown below:</p>
<div class="figure align-default">
<img alt="../_static/resources/netron_diff_ui_opening.jpg" src="../_static/resources/netron_diff_ui_opening.jpg" />
</div>
<div class="figure align-default">
<img alt="../_static/resources/open_diff_tool_netron.png" src="../_static/resources/open_diff_tool_netron.png" />
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Upon launch of the Diff Customization tool, at the top, the user is prompted to select a use case for the tool.
There are 3 options to choose from:</p>
<div class="figure align-default">
<img alt="../_static/resources/use_case_netron.png" src="../_static/resources/use_case_netron.png" />
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For the purposes of this documentation, only inference vs inference will be detailed. The setup procedure for the other
use cases is similar. The other two use cases are explained below:</p>
<ol class="arabic simple">
<li><p>Golden vs Inference: Used to test inference run using goldens from a particular ML framework and comparing against
the output of a QNN backend</p></li>
<li><p>Output vs Output: Used to test existing inference results against ML framework goldens OR used to test differences
between two existing inference results</p></li>
<li><p>Inference Vs Inference: Used to test inference between two converted QNN models or the same QNN model on different
QNN backends</p></li>
</ol>
</div>
<div class="section" id="inference-vs-inference">
<h4>Inference vs Inference<a class="headerlink" href="#inference-vs-inference" title="Permalink to this headline">¶</a></h4>
<p>If this use case is selected, the user is presented with various form fields for the purposes of running two
jobs asynchronously with the option of choosing different runtimes for each QNN network being run.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/inferencevinference.png" />
</div>
<p>A more detailed view of what the user is prompted is displayed below:</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/inferencedetailed.png" />
</div>
<p>In order to execute the networks, the user has two options:</p>
<p><strong>Running on Host machine</strong></p>
<p>When the Target Device is selected as “host”, the user can only use the CPU as a runtime. In addition, the user can
only select “x86_64-linux-clang” as the architecture in this use case.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_host_use_case.png" />
</div>
<p><strong>Running On-Device</strong></p>
<p>When the Target Device is selected as “on-device”, a Device ID is required to connect to the device via adb.
Thereafter, the user can select any of the three QNN backend runtimes available (CPU, GPU, or DSPv[68, 69, 73]) and the user
can select architecture “aarch64-android”</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_device_use_case.png" />
</div>
<p>After choosing the desired target device and runtime configurations, the rest of the fields are explained in detail
below:</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Users are able to click again and change the location to any of the path fields</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Setup Parameters</p></th>
<th class="head"><p>Configurations to Select</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>The options for what verifier to run on the outputs of the model are (See Note below table for custom verifier
(accuracy + performance) thresholds and see table below for providing custom accuracy verifier hyperparameters):</p></td>
<td><p>RtolAtol, AdjustedRtolAtol, TopK, MeanIOU, L1Error, CosineSimilarity, MSE, SQNR</p></td>
</tr>
<tr class="row-odd"><td><p>Model JSON</p></td>
<td><p>upload &lt;model&gt;_net.json file that was outputted from the QNN converters.</p></td>
</tr>
<tr class="row-even"><td><p>Model Cpp</p></td>
<td><p>upload &lt;model&gt;.cpp that was outputted from the QNN converters.</p></td>
</tr>
<tr class="row-odd"><td><p>Model Bin</p></td>
<td><p>upload &lt;model&gt;.bin that was outputted from the QNN converters.</p></td>
</tr>
<tr class="row-even"><td><p>NDK Path</p></td>
<td><p>upload the path to your Android NDK</p></td>
</tr>
<tr class="row-odd"><td><p>Devices Engine Path</p></td>
<td><p>upload the path to the top-level of the unzipped qnn-sdk</p></td>
</tr>
<tr class="row-even"><td><p>Input List</p></td>
<td><p>provide a path to the input file for the model</p></td>
</tr>
<tr class="row-odd"><td><p>Save Run Configurations</p></td>
<td><p>provide a location where the inference and runtime results from the Diff customization tool will be stored</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Users have the option of providing a custom accuracy and performance verifier threshold when running diff.
A custom accuracy verifier threshold can be provided for any of the accuracy verifiers. By default the
verifier thresholds are 0.01. The custom thresholds can be provided in the text boxes labelled “Accuracy
Threshold” and “Perf Threshold”.</p>
</div>
<p>Users now have the option to enter accuracy verifier specific hyperparameters inside textboxes. The Default Values are displayed
inside the text-boxes and can be customized as per user needs. The table below highlights the hyperparameters for each
verifier that can be customized.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Verifier</p></th>
<th class="head"><p>Hyperparameters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AdjustedRtolAtol</p></td>
<td><p>Number of Levels</p></td>
</tr>
<tr class="row-odd"><td><p>RtolAtol</p></td>
<td><p>Rtol Margin, Atol Margin</p></td>
</tr>
<tr class="row-even"><td><p>Topk</p></td>
<td><p>K, Ordered</p></td>
</tr>
<tr class="row-odd"><td><p>MeanIOU</p></td>
<td><p>Background Classification</p></td>
</tr>
<tr class="row-even"><td><p>L1Error</p></td>
<td><p>Multiplier, Scale</p></td>
</tr>
<tr class="row-odd"><td><p>CosineSimilarity</p></td>
<td><p>Multiplier, Scale</p></td>
</tr>
<tr class="row-even"><td><p>MSE (Mean Square Error)</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>SQNR (Signal-To-Noise Ratio)</p></td>
<td><p>N/A</p></td>
</tr>
</tbody>
</table>
<p>Below is an example of what the fields should look like once filled to completion:</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/inferencefinalview.png" />
</div>
<p>After running the Diff Customization tool, the output directories/files should be present in the working directory
file path provided in the last field</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/output_directories_netron_diff.png" />
</div>
</div>
<div class="section" id="results-and-outputs">
<h4>Results and Outputs:<a class="headerlink" href="#results-and-outputs" title="Permalink to this headline">¶</a></h4>
<p>After pressing the Run button as mentioned above, the visualization of the network should pop-up. Nodes will be
highlighted if there are any accuracy and/or performance variations. Clicking on each node will show more information
about the accuracy and performance diff information as shown below.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_diff_overall_view.png" />
</div>
</div>
<div class="section" id="performance-and-accuracy-diff-visualizations">
<h4>Performance and Accuracy Diff Visualizations:<a class="headerlink" href="#performance-and-accuracy-diff-visualizations" title="Permalink to this headline">¶</a></h4>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_accuracy_performance_diff.png" />
</div>
<p>As seen above, the performance and accuracy diff information is shown under the Diff section of any given node.
The color of the node boundary in the viewer represents whether a performance or accuracy error (above the default
verifier threshold of 0.01) was reported. For example, in the Conv2d node shown below, there are two boundaries of
orange and red indicating that this node has both an accuracy and performance difference across the runs. The
FullyConnected node shown only has a yellow boundary indicating that only a performance difference was found.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_node_overlay_both.png" />
</div>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_node_overlay_perf.png" />
</div>
</div>
<div class="section" id="qnn-netron-diff-navigation">
<h4>QNN Netron Diff Navigation<a class="headerlink" href="#qnn-netron-diff-navigation" title="Permalink to this headline">¶</a></h4>
<p>QNN Netron has the ability to locate the first node in the graph with any performance or accuracy diffs. When the
user clicks on the next and previous arrows, the visualization of the graph will zoom into the desired node with the
first performance or accuracy difference. This makes model debugging much easier for larger models as the user doesn’t
have to look for the nodes themselves to find where the network performance and accuracy errors starts to diverge.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_error_first_node_selected.png" />
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="converters.html" class="btn btn-neutral float-right" title="Converters" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="example_op_defs.html" class="btn btn-neutral float-left" title="Example XML OpDef Configs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>