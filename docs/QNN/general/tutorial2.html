

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <title>Tutorial: Converting and executing a CNN model with QNN &mdash; Qualcomm® AI Engine Direct</title>



  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />










  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->


      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>

    <script type="text/javascript" src="../_static/js/theme.js"></script>


    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
</head>

<body class="wy-body-for-nav">


  <div class="wy-grid-for-nav">

    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >



            <a href="../index.html" class="icon icon-home"> Qualcomm® AI Engine Direct



          </a>




              <div class="version">
                v2.13.0
              </div>




<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>


        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">






              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="backend.html">Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_packages.html">Op Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>



        </div>

      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">


      <nav class="wy-nav-top" aria-label="top navigation">

          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® AI Engine Direct</a>

      </nav>


      <div class="wy-nav-content">

        <div class="rst-content">



















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">

      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>

      <li>Tutorial: Converting and executing a CNN model with QNN</li>


      <li class="wy-breadcrumbs-aside">



      </li>

  </ul>


  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <div class="section" id="tutorial-converting-and-executing-a-cnn-model-with-qnn">
<h1>Tutorial: Converting and executing a CNN model with QNN<a class="headerlink" href="#tutorial-converting-and-executing-a-cnn-model-with-qnn" title="Permalink to this headline">¶</a></h1>
<p>The following tutorial will demonstrate the end to end usage of <a class="reference internal" href="tools.html"><span class="doc">QNN Tools</span></a>
and the <a class="reference internal" href="api.html"><span class="doc">QNN API</span></a>. This process begins with a trained source framework model,
which is converted and built into a series of QNN API calls using the QNN Converter, which are then executed on a particular
backend.</p>
<p>The tutorial will use Inception V3 as the source framework model and the <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> executable as
the example application. The execution will show usage on the CPU, GPU, DSP, and HTP backends on both host (for CPU
and HTP) and device.</p>
<p>The sections of the tutorial are as follows:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#id1">Tutorial Setup</a></p></li>
<li><p><a class="reference internal" href="#model-conversion">Model Conversion</a></p></li>
<li><p><a class="reference internal" href="#model-build">Model Build</a></p></li>
<li><p><a class="reference internal" href="#executing-example-model">Executing Example Model</a></p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On a Windows host, the Tutorial Setup and Model Conversion sections should be completed in the WSL (x86) environment. Separate Linux and Windows sections are provided for the Model Build and Execution steps. Windows developers can refer to <a class="reference internal" href="overview.html#workflow-wsl"><span class="std std-ref">Integration Workflow on Windows</span></a> to see an overview of the workflow.</p>
</div>
<div class="section" id="tutorial-setup">
<span id="id1"></span><h2>Tutorial Setup<a class="headerlink" href="#tutorial-setup" title="Permalink to this headline">¶</a></h2>
<p>The tutorial assumes general setup instructions have been followed
at <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a>.</p>
<p>Additionally, this tutorial requires the acquisition of the Inception V3 Tensorflow model file and
sample images. This is handled by the provided setup script <code class="docutils literal notranslate"><span class="pre">setup_inceptionv3.py</span></code>. The script is located at:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">setup_inceptionv3</span><span class="p">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Usage is as follows:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">setup_inceptionv3</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="n">a</span><span class="w"> </span><span class="n">ASSETS_DIR</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">d</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">q</span><span class="p">]</span>

<span class="n">Prepares</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">inception_v3</span><span class="w"> </span><span class="n">assets</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">tutorial</span><span class="w"> </span><span class="n">examples</span><span class="p">.</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">-</span><span class="n">a</span><span class="w"> </span><span class="n">ASSETS_DIR</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">assets_dir</span><span class="w"> </span><span class="n">ASSETS_DIR</span>
<span class="w">                        </span><span class="n">directory</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">inception_v3</span><span class="w"> </span><span class="n">assets</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">-</span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">download</span><span class="w">        </span><span class="n">Download</span><span class="w"> </span><span class="n">inception_v3</span><span class="w"> </span><span class="n">assets</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">inception_v3</span><span class="w"> </span><span class="n">example</span>
<span class="w">                        </span><span class="n">directory</span>
<span class="w">  </span><span class="o">-</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">convert_model</span><span class="w">   </span><span class="n">Convert</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">compile</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">once</span><span class="w"> </span><span class="n">acquired</span><span class="p">.</span>
<span class="w">  </span><span class="o">-</span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">quantize_model</span><span class="w">  </span><span class="n">Quantize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">during</span><span class="w"> </span><span class="n">conversion</span><span class="p">.</span><span class="w"> </span><span class="n">Only</span><span class="w"> </span><span class="n">available</span>
<span class="w">                        </span><span class="k">if</span><span class="w"> </span><span class="o">--</span><span class="n">c</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="o">--</span><span class="n">convert_model</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">chosen</span>
</pre></div>
</div>
<p>Before using the script, please set the environment variable <code class="docutils literal notranslate"><span class="pre">TENSORFLOW_HOME</span></code> to point to the
location where TensorFlow package is installed. The script uses TensorFlow utilities like
<code class="docutils literal notranslate"><span class="pre">optimize_for_inference.py</span></code>, which are present in the TensorFlow installation directory.
To find the location of the TensorFlow package run the following command:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="n">pip</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="n">tensorflow</span>
</pre></div>
</div>
<p>Set the environment variable using the Location field from the output of the above command:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">export</span><span class="w"> </span><span class="n">TENSORFLOW_HOME</span><span class="o">=&lt;</span><span class="n">tensorflow</span><span class="o">-</span><span class="n">location</span><span class="o">&gt;/</span><span class="n">tensorflow_core</span>
</pre></div>
</div>
<p>To run the script use:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">setup_inceptionv3</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">a</span><span class="w"> </span><span class="o">~/</span><span class="n">tmpdir</span><span class="w"> </span><span class="o">-</span><span class="n">d</span>
</pre></div>
</div>
<p>This will populate the model file at:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span>
</pre></div>
</div>
<p>And the raw images at:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span>
</pre></div>
</div>
</div>
<div class="section" id="model-conversion">
<span id="model-setup"></span><h2>Model Conversion<a class="headerlink" href="#model-conversion" title="Permalink to this headline">¶</a></h2>
<p>After the model assets have been acquired the model can be converted to a series of invocations of QNN API,
and subsequently built for use by an application.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A quantized model is needed for use on the HTP and DSP backend. See <a class="reference internal" href="#id2">Model Quantization</a> to generate a quantized model.</p>
</div>
<p>To convert the Inception V3 model use the <code class="docutils literal notranslate"><span class="pre">qnn-tensorflow-converter</span></code>:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">tensorflow</span><span class="o">-</span><span class="n">converter</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">input_network</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">input_dim</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">out_node</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">Inception_v3</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span>\
</pre></div>
</div>
<p>This will produce the following artifacts:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3.cpp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3.bin</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3_net.json</span></code></p></li>
</ul>
<p>The artifacts include .cpp file containing the sequence of API calls, and a .bin file containing
the static data associated with the model.</p>
<div class="section" id="id2">
<h3>Model Quantization<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>To use a quantized model instead of a floating point model follow the below steps:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">tensorflow</span><span class="o">-</span><span class="n">converter</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">input_network</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">input_dim</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">out_node</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">Inception_v3_quantized</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span>\
<span class="w">  </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>This will produce the following artifacts:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3_quantized.cpp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3_quantized.bin</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3_quantized_net.json</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When quantizing a model during conversion the input list must contain absolute
path to input data.</p>
</div>
</div>
</div>
<div class="section" id="model-build">
<h2>Model Build<a class="headerlink" href="#model-build" title="Permalink to this headline">¶</a></h2>
<p>Once the model is converted it is built with <code class="docutils literal notranslate"><span class="pre">qnn-model-lib-generator</span></code>:</p>
<div class="section" id="model-build-on-linux-host">
<h3>Model Build on Linux Host<a class="headerlink" href="#model-build-on-linux-host" title="Permalink to this headline">¶</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ ${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator \
  -c ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3.cpp \
  -b ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3.bin \
  -o ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs # This can be any path
</pre></div>
</div>
<p>This will produce the following artifacts:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/aarch64-android/libInception_v3.so</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/x86_64-linux-clang/libInception_v3.so</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default libraries are built for all targets. To compile for a specific target, use the
-t &lt;target&gt; option with qnn-model-lib-generator. Choices of &lt;target&gt; are aarch64-android and
x86_64-linux-clang.</p>
</div>
<p>Optionally, the above steps (model conversion &amp; model build) can be completed with the provided setup script. To convert and build
the Inception v3 model using the script run:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">setup_inceptionv3</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">a</span><span class="w"> </span><span class="o">~/</span><span class="n">tmpdir</span><span class="w"> </span><span class="o">-</span><span class="n">d</span><span class="w"> </span><span class="o">-</span><span class="n">c</span>
</pre></div>
</div>
<p>This will produce the same artifacts as above.</p>
<p>To build the quantized model, the steps are the same as above:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ ${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator \
  -c ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3_quantized.cpp \
  -b ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model/Inception_v3_quantized.bin \
  -o ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs # This can be any path
</pre></div>
</div>
<p>This will produce the following artifacts:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/aarch64-android/libInception_v3_quantized.so</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/x86_64-linux-clang/libInception_v3_quantized.so</span></code></p></li>
</ul>
<p>Optionally, the above steps can be completed with the provided setup script. To convert, quantize, and build
the model Inception V3 using the script run:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">setup_inceptionv3</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">a</span><span class="w"> </span><span class="o">~/</span><span class="n">tmpdir</span><span class="w"> </span><span class="o">-</span><span class="n">d</span><span class="w"> </span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="o">-</span><span class="n">q</span>
</pre></div>
</div>
<p>This will produce the same artifacts as above.</p>
</div>
<div class="section" id="model-build-on-windows-host">
<h3>Model Build on Windows Host<a class="headerlink" href="#model-build-on-windows-host" title="Permalink to this headline">¶</a></h3>
<p>To build the model files into a DLL library on a Windows host, we need to open <code class="docutils literal notranslate"><span class="pre">Developer</span> <span class="pre">PowerShell</span> <span class="pre">for</span> <span class="pre">VS</span> <span class="pre">2022</span></code>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ mkdir C:\tmp\qnn_tmp
</pre></div>
</div>
<p>Copy the following files to <code class="docutils literal notranslate"><span class="pre">c:\tmp\qnn_tmp</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">Inception_v3</span><span class="p">.</span><span class="n">cpp</span>
<span class="o">-</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">Inception_v3</span><span class="p">.</span><span class="n">bin</span>
</pre></div>
</div>
<p>Make sure you have setup <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}</span></code> environment variable with <a class="reference internal" href="setup.html#environment-setup-windows"><span class="std std-ref">Environment Setup for Windows</span></a>.</p>
<p id="windows-model-lib-generator">Finally, we can generate our model DLL library via:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\tmp\qnn_tmp

$ py -3 ${QNN_SDK_ROOT}\bin\x86_64-windows-msvc\qnn-model-lib-generator `
    -c .\Inception_v3.cpp `
    -b .\Inception_v3.bin `
    -o model_lib `
    -t windows-x86_64 # you can also specify windows-aarch64 for ARM64 platform
</pre></div>
</div>
<p>Now you will have Inception_v3.dll under <code class="docutils literal notranslate"><span class="pre">C:\tmp\qnn_tmp\model_lib\x64</span></code>, and are ready to perform inference.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The model DLL library can be built for x64 or ARM64 platforms by specifying the desired platform.</p>
</div>
<p>Similarly, to build the quantized model files into a DLL library on a Windows host, we need to open <code class="docutils literal notranslate"><span class="pre">Developer</span> <span class="pre">PowerShell</span> <span class="pre">for</span> <span class="pre">VS</span> <span class="pre">2022</span></code>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ mkdir C:\tmp\qnn_tmp
</pre></div>
</div>
<p>Copy the following files to <code class="docutils literal notranslate"><span class="pre">c:\tmp\qnn_tmp</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">Inception_v3_quantized</span><span class="p">.</span><span class="n">cpp</span>
<span class="o">-</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">Inception_v3_quantized</span><span class="p">.</span><span class="n">bin</span>
</pre></div>
</div>
<p>Make sure you have setup <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}</span></code> environment variable with <a class="reference internal" href="setup.html#environment-setup-windows"><span class="std std-ref">Environment Setup for Windows</span></a>.</p>
<p>Finally, we can generate our model DLL library via:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\tmp\qnn_tmp

$ py -3 ${QNN_SDK_ROOT}\bin\x86_64-windows-msvc\qnn-model-lib-generator `
    -c .\Inception_v3_quantized.cpp `
    -b .\Inception_v3_quantized.bin `
    -o model_lib `
    -t windows-aarch64
</pre></div>
</div>
<p>Now you will have <code class="docutils literal notranslate"><span class="pre">Inception_v3_quantized.dll</span></code> under <code class="docutils literal notranslate"><span class="pre">C:\tmp\qnn_tmp\model_lib\ARM64</span></code>.</p>
<p>To perform inference on the Windows host, the input data needs to be copied over.</p>
<p>Copy the following files and directories to the Windows host <code class="docutils literal notranslate"><span class="pre">C:\tmp\qnn_tmp</span></code>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span>
<span class="o">-</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span>
<span class="o">-</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">imagenet_slim_labels</span>
<span class="o">-</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">show_inceptionv3_classifications</span><span class="p">.</span><span class="n">py</span>
</pre></div>
</div>
<p>We are now ready for the Execution on Windows Host sections.</p>
</div>
</div>
<div class="section" id="cpu-backend-execution">
<span id="executing-example-model"></span><h2>CPU Backend Execution<a class="headerlink" href="#cpu-backend-execution" title="Permalink to this headline">¶</a></h2>
<div class="section" id="execution-on-linux-host">
<h3>Execution on Linux Host<a class="headerlink" href="#execution-on-linux-host" title="Permalink to this headline">¶</a></h3>
<p>With the model library compiled, the model can be executed using <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> with the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span>
<span class="n">$</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">libQnnCpu</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model_libs</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">libInception_v3</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>This will produce the results at:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/output</span></code></p></li>
</ul>
<p>To view the results use:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">show_inceptionv3_classifications</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">output</span><span class="o">/</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">imagenet_slim_labels</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<div class="section" id="execution-on-android">
<h3>Execution on Android<a class="headerlink" href="#execution-on-android" title="Permalink to this headline">¶</a></h3>
<p>Running the CPU Backend on an Android target is largely
similar to running on the Linux x86 target.</p>
<p>First, create a directory for the example on device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># make inception_v3 if necessary</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mkdir /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
<p>Now push the necessary libraries to device:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnCpu.so /data/local/tmp/inception_v3
$ adb push ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/aarch64-android/*.so /data/local/tmp/inception_v3
</pre></div>
</div>
<p>Now push the input data and input lists to device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Push the <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Now set up the environment on device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span>
<span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">export</span><span class="w"> </span><span class="n">LD_LIBRARY_PATH</span><span class="o">=/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Finally, use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> with the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">libQnnCpu</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">libInception_v3</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>Outputs from the run will be located at the default ./output directory. Exit the device and view the results:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">exit</span>
<span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">pull</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span><span class="o">/</span><span class="n">output</span><span class="w"> </span><span class="n">output_android</span>
<span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">show_inceptionv3_classifications</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">output_android</span><span class="o">/</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">imagenet_slim_labels</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<div class="section" id="execution-on-windows-host">
<h3>Execution on Windows Host<a class="headerlink" href="#execution-on-windows-host" title="Permalink to this headline">¶</a></h3>
<p>Please ensure the <a class="reference internal" href="#model-build-on-windows-host">Model Build on Windows Host</a> section has been completed before proceeding.</p>
<p>First, create the following folder on the Windows host: <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code>.</p>
<p>Now, copy the necessary libraries to the working directory: <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- ${QNN_SDK_ROOT}\lib\x86_64-windows-msvc\QnnCpu.dll
- C:\tmp\qnn_tmp\model_lib\x64\Inception_v3.dll (generated above)
</pre></div>
</div>
<p>Now, copy the input data and input list to <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- C:\tmp\qnn_tmp\cropped
- C:\tmp\qnn_tmp\target_raw_list.txt
- C:\tmp\qnn_tmp\imagenet_slim_labels
- C:\tmp\qnn_tmp\show_inceptionv3_classifications.py
</pre></div>
</div>
<p>Now, copy the <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool to <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code>:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- ${QNN_SDK_ROOT}\bin\x86_64-windows-msvc\qnn-net-run.exe
</pre></div>
</div>
<p>To run <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool, please open <code class="docutils literal notranslate"><span class="pre">&quot;Developer</span> <span class="pre">PowerShell</span> <span class="pre">for</span> <span class="pre">VS</span> <span class="pre">2022&quot;</span></code>:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\qnn_test_package

$ .\qnn-net-run.exe `
   --model .\Inception_v3.dll `
   --input_list .\target_raw_list.txt `
   --backend .\QnnCpu.dll
</pre></div>
</div>
<p>After the inference, we can check the classification result. By default, outputs from the run
will be located in the <code class="docutils literal notranslate"><span class="pre">.\output</span></code> directory.</p>
<p>Then, open <code class="docutils literal notranslate"><span class="pre">&quot;Developer</span> <span class="pre">PowerShell</span> <span class="pre">for</span> <span class="pre">VS</span> <span class="pre">2022&quot;</span></code> to run:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\qnn_test_package

$ py -3 .\show_inceptionv3_classifications.py `
     -i .\cropped\raw_list.txt `
     -o output `
     -l .\imagenet_slim_labels.txt
</pre></div>
</div>
<p>The classification results should be:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">trash_bin</span><span class="p">.</span><span class="n">raw</span><span class="w">   </span><span class="mf">0.777344</span><span class="w"> </span><span class="mi">413</span><span class="w"> </span><span class="n">ashcan</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w">      </span><span class="mf">0.253906</span><span class="w"> </span><span class="mi">832</span><span class="w"> </span><span class="n">studio</span><span class="w"> </span><span class="n">couch</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">plastic_cup</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.980469</span><span class="w"> </span><span class="mi">648</span><span class="w"> </span><span class="n">measuring</span><span class="w"> </span><span class="n">cup</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">notice_sign</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.167969</span><span class="w"> </span><span class="mi">459</span><span class="w"> </span><span class="n">brass</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="dsp-backend-execution">
<h2>DSP Backend Execution<a class="headerlink" href="#dsp-backend-execution" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>Execution on Android<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Running the DSP Backend on an Android target is largely
similar to running the CPU, HTP and GPU backend on Android target. The remainder of
this section will assume the target has a v66 DSP.</p>
<p>Similar to HTP backend, DSP backend also requires a quantized model. To generate quantized model,
see <a class="reference internal" href="#id2">Model Quantization</a>.
First, create a directory for the example on device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># make inception_v3 if necessary</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mkdir /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
<p>Now push the necessary libraries to device:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnDsp.so /data/local/tmp/inception_v3
$ adb push ${QNN_SDK_ROOT}/lib/hexagon-v66/unsigned/libQnnDspV66Skel.so /data/local/tmp/inception_v3
$ adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnDspV66Stub.so /data/local/tmp/inception_v3
$ adb push ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/aarch64-android/* /data/local/tmp/inception_v3
</pre></div>
</div>
<p>Now push the input data and input lists to device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Push the <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Now set up the environment on device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ adb shell
$ cd /data/local/tmp/inception_v3
$ export VENDOR_LIB=/vendor/lib/ # /vendor/lib64/ if aarch64
$ export LD_LIBRARY_PATH=/data/local/tmp/inception_v3:/vendor/dsp/cdsp:$VENDOR_LIB
$ export ADSP_LIBRARY_PATH=&quot;/data/local/tmp/inception_v3;/vendor/dsp/cdsp;/vendor/lib/rfsa/adsp;/system/lib/rfsa/adsp;/dsp&quot;
</pre></div>
</div>
<p>Finally, use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> with the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">libQnnDsp</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">libInception_v3_quantized</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">output_android</span>
</pre></div>
</div>
<p>Outputs from the run will be located at the ./output_android directory. Exit the device and view the results:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">exit</span>
<span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">pull</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span><span class="o">/</span><span class="n">output_android</span>
<span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">show_inceptionv3_classifications</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">output_android</span><span class="o">/</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">imagenet_slim_labels</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<div class="section" id="execution-on-windows-device">
<h3>Execution on Windows Device<a class="headerlink" href="#execution-on-windows-device" title="Permalink to this headline">¶</a></h3>
<p>This section describes performing inference on a Windows host with a v66 DSP backend. The steps are
similar to execution on a Windows host with a CPU or HTP backend.</p>
<p>Execution with the DSP backend requires a quantized model. Please complete the steps to build the quantized model
in the section <a class="reference internal" href="#model-build-on-windows-host">Model Build on Windows Host</a> above.</p>
<p>First, connect to the windows device with:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">mstsc</span><span class="w"> </span><span class="o">-</span><span class="n">v</span><span class="w"> </span><span class="o">&lt;</span><span class="n">your</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="n">IP</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Then, create the folder <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code> on the device.</p>
<p>Now, copy the necessary libraries from the Windows host to the Windows device <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- ${QNN_SDK_ROOT}\lib\aarch64-windows-msvc\QnnDsp.dll
- ${QNN_SDK_ROOT}\lib\aarch64-windows-msvc\QnnDspV66Stub.dll
- ${QNN_SDK_ROOT}\lib\hexagon-v66\unsigned\libQnnDspV66Skel.so
- C:\tmp\qnn_tmp\model_lib\ARM64\Inception_v3_quantized.dll (generated above)
</pre></div>
</div>
<p>Now, copy the input data and input list from the Windows host to the Windows device <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- C:\tmp\qnn_tmp\cropped
- C:\tmp\qnn_tmp\target_raw_list.txt
</pre></div>
</div>
<p>Now, copy the <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool to the Windows device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- ${QNN_SDK_ROOT}\bin\aarch64-windows-msvc\qnn-net-run.exe
</pre></div>
</div>
<p>To run <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool, please open <code class="docutils literal notranslate"><span class="pre">&quot;Administrator:Windows</span> <span class="pre">Power</span> <span class="pre">Shell&quot;</span></code>:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\qnn_test_package

$ .\qnn-net-run.exe `
   --model .\Inception_v3_quantized.dll `
   --input_list .\target_raw_list.txt `
   --backend .\QnnDsp.dll
</pre></div>
</div>
<p>After the inference, we can check the classification result. By default, outputs from the run
will be located in the <code class="docutils literal notranslate"><span class="pre">.\output</span></code> directory. We will need to copy the results back to the Windows host.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>copy C:\qnn_test_package\output to C:\tmp\qnn_tmp
</pre></div>
</div>
<p>Then, open <code class="docutils literal notranslate"><span class="pre">&quot;Developer</span> <span class="pre">PowerShell</span> <span class="pre">for</span> <span class="pre">VS</span> <span class="pre">2022&quot;</span></code> to run:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\tmp\qnn_tmp

$ py -3 .\show_inceptionv3_classifications.py `
     -i .\cropped\raw_list.txt `
     -o output `
     -l .\imagenet_slim_labels.txt
</pre></div>
</div>
<p>The classification results should be:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">trash_bin</span><span class="p">.</span><span class="n">raw</span><span class="w">   </span><span class="mf">0.777344</span><span class="w"> </span><span class="mi">413</span><span class="w"> </span><span class="n">ashcan</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w">      </span><span class="mf">0.253906</span><span class="w"> </span><span class="mi">832</span><span class="w"> </span><span class="n">studio</span><span class="w"> </span><span class="n">couch</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">plastic_cup</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.980469</span><span class="w"> </span><span class="mi">648</span><span class="w"> </span><span class="n">measuring</span><span class="w"> </span><span class="n">cup</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">notice_sign</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.167969</span><span class="w"> </span><span class="mi">459</span><span class="w"> </span><span class="n">brass</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="gpu-backend-execution">
<h2>GPU Backend Execution<a class="headerlink" href="#gpu-backend-execution" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Running the GPU Backend on a Windows device is not supported.</p>
</div>
<div class="section" id="id4">
<h3>Execution on Android<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>Running the GPU Backend on an Android target is largely
similar to running the CPU backend on Android target.</p>
<p>First, create a directory for the example on device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># make inception_v3 if necessary</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mkdir /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
<p>Now push the necessary libraries to device:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnGpu.so /data/local/tmp/inception_v3
$ adb push ${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/aarch64-android/*.so /data/local/tmp/inception_v3
</pre></div>
</div>
<p>Now push the input data and input lists to device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Push the <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Now set up the environment on device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span>
<span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">export</span><span class="w"> </span><span class="n">LD_LIBRARY_PATH</span><span class="o">=/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Finally, use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> with the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">libQnnGpu</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">libInception_v3</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>Outputs from the run will be located at the default ./output directory. Exit the device and view the results:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">exit</span>
<span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">pull</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span><span class="o">/</span><span class="n">output</span><span class="w"> </span><span class="n">output_android</span>
<span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">show_inceptionv3_classifications</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">output_android</span><span class="o">/</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">imagenet_slim_labels</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="htp-backend-execution">
<h2>HTP Backend Execution<a class="headerlink" href="#htp-backend-execution" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id5">
<h3>Execution on Linux Host<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>The HTP backend can be exercised on Linux Host through the use of the HTP Emulation backend. With
the model library compiled, the model can be executed using <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> with the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span>
<span class="n">$</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">libQnnHtp</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model_libs</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">libInception_v3_quantized</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>in order to use the HTP Emulation backend, a quantized model is required. For more information
on quantization see <a class="reference internal" href="#id2">Model Quantization</a>.</p>
</div>
<p>This will produce the results at:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/output</span></code></p></li>
</ul>
<p>To view the results use:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">show_inceptionv3_classifications</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">output</span><span class="o">/</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">imagenet_slim_labels</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h3>Execution on Android<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>Running the HTP Backend on an Android target is largely
similar to running the CPU and GPU backend on Android target.</p>
<p>One distinction is the HTP backend requires a quantized model. For more information
on quantization see <a class="reference internal" href="#id2">Model Quantization</a>. Additionally, running the HTP on device requires
the generation of a serialized context. To generate the context run:</p>
<div class="highlight-c notranslate" id="context-generator"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">context</span><span class="o">-</span><span class="n">binary</span><span class="o">-</span><span class="n">generator</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">libQnnHtp</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">model_libs</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="o">/</span><span class="n">libInception_v3_quantized</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="w">              </span><span class="o">--</span><span class="n">binary_file</span><span class="w"> </span><span class="n">Inception_v3_quantized</span><span class="p">.</span><span class="n">serialized</span>
</pre></div>
</div>
<p>This creates the context at:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/output/Inception_v3_quantized.serialized.bin</span></code></p></li>
</ul>
<p>First, create a directory for the example on device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># make inception_v3 if necessary</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mkdir /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
<p>Now push the necessary libraries to device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">hexagon</span><span class="o">-</span><span class="n">v68</span><span class="o">/</span><span class="kt">unsigned</span><span class="o">/</span><span class="n">libQnnHtpV68Skel</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="o">/</span><span class="n">libQnnHtpV68Stub</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="o">/</span><span class="n">libQnnHtp</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">Inception_v3_quantized</span><span class="p">.</span><span class="n">serialized</span><span class="p">.</span><span class="n">bin</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section is to demonstrate HTP execution on Android with offline prepared graph steps.
If we would like to execute on-device(online) prepared graph, push on-device prepare libray
to device as well.</p>
<p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">adb</span> <span class="pre">push</span> <span class="pre">${QNN_SDK_ROOT}/lib/aarch64-android/libQnnHtpPrepare.so</span> <span class="pre">/data/local/tmp/inception_v3</span></code></p>
</div>
<p>Now push the input data and input lists to device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Push the <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
</pre></div>
</div>
<p>Now set up the environment on device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span>
<span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">export</span><span class="w"> </span><span class="n">LD_LIBRARY_PATH</span><span class="o">=/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span>
<span class="n">$</span><span class="w"> </span><span class="n">export</span><span class="w"> </span><span class="n">ADSP_LIBRARY_PATH</span><span class="o">=</span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
<p>Finally, use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> with the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">libQnnHtp</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w"> </span><span class="n">Inception_v3_quantized</span><span class="p">.</span><span class="n">serialized</span><span class="p">.</span><span class="n">bin</span>
</pre></div>
</div>
<p>Outputs from the run will be located at the default ./output directory. Exit the device and view the results:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">exit</span>
<span class="n">$</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span>
<span class="n">$</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">pull</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span><span class="o">/</span><span class="n">output</span><span class="w"> </span><span class="n">output_android</span>
<span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">show_inceptionv3_classifications</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">output_android</span><span class="o">/</span><span class="w"> </span>\
<span class="w">                                                                               </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">imagenet_slim_labels</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h3>Execution on Windows Device<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<dl class="simple">
<dt>This section illustrates how to run end-to-end inference on HTP using <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> in 2 different ways:</dt><dd><ol class="arabic simple">
<li><p>Running HTP Backend on windows-aarch64 using an offline prepared graph</p></li>
<li><p>Running HTP Backend on windows-aarch64 using an on-device prepared graph</p></li>
</ol>
</dd>
</dl>
<p><strong>Executing with HTP Backend Using an Offline Prepared Graph</strong></p>
<p>First, prepare the serialized context by following the steps <a class="reference internal" href="#context-generator"><span class="std std-ref">here</span></a>.
The following assumes the Windows device has a v68 DSP.</p>
<p>Connect to the windows device with:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">mstsc</span><span class="w"> </span><span class="o">-</span><span class="n">v</span><span class="w"> </span><span class="o">&lt;</span><span class="n">your</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="n">IP</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Create the folder <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code> on the device.</p>
<p>Now, copy the necessary libraries from the Windows host to the Windows device <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- ${QNN_SDK_ROOT}\lib\aarch64-windows-msvc\QnnHtp.dll
- ${QNN_SDK_ROOT}\lib\aarch64-windows-msvc\QnnHtpV68Stub.dll
- ${QNN_SDK_ROOT}\lib\hexagon-v68\unsigned\libQnnHtpV68Skel.so
- Inception_v3_quantized.serialized.bin (serialized context prepared above)
</pre></div>
</div>
<p>Now, copy the input data and input list from the Windows host to the Windows device <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- C:\tmp\qnn_tmp\cropped
- C:\tmp\qnn_tmp\target_raw_list.txt
</pre></div>
</div>
<p>Now, copy the <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool to the Windows device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- ${QNN_SDK_ROOT}\bin\aarch64-windows-msvc\qnn-net-run.exe
</pre></div>
</div>
<p>To run <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool, please open <code class="docutils literal notranslate"><span class="pre">&quot;Administrator:Windows</span> <span class="pre">Power</span> <span class="pre">Shell&quot;</span></code>:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\qnn_test_package

$ .\qnn-net-run.exe `
   --retrieve_context .\Inception_v3_quantized.serialized.bin `
   --input_list .\target_raw_list.txt `
   --backend .\QnnHtp.dll
</pre></div>
</div>
<p>After performing the inference, we can check the classification result. By default, outputs from the run
will be located in the <code class="docutils literal notranslate"><span class="pre">.\output</span></code> directory. We will need to copy the results back to the Windows Host.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>copy C:\qnn_test_package\output to C:\tmp\qnn_tmp
</pre></div>
</div>
<p>Then, open <code class="docutils literal notranslate"><span class="pre">&quot;Developer</span> <span class="pre">PowerShell</span> <span class="pre">for</span> <span class="pre">VS</span> <span class="pre">2022&quot;</span></code> to run:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\tmp\qnn_tmp

$ py -3 .\show_inceptionv3_classifications.py `
     -i .\cropped\raw_list.txt `
     -o output `
     -l .\imagenet_slim_labels.txt
</pre></div>
</div>
<p>Then, the classification results should be:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">trash_bin</span><span class="p">.</span><span class="n">raw</span><span class="w">   </span><span class="mf">0.777344</span><span class="w"> </span><span class="mi">413</span><span class="w"> </span><span class="n">ashcan</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w">      </span><span class="mf">0.253906</span><span class="w"> </span><span class="mi">832</span><span class="w"> </span><span class="n">studio</span><span class="w"> </span><span class="n">couch</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">plastic_cup</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.980469</span><span class="w"> </span><span class="mi">648</span><span class="w"> </span><span class="n">measuring</span><span class="w"> </span><span class="n">cup</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">notice_sign</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.167969</span><span class="w"> </span><span class="mi">459</span><span class="w"> </span><span class="n">brass</span>
</pre></div>
</div>
<p><strong>Executing with HTP Backend Using an On-Device Prepared Graph</strong></p>
<p>First, build the quantized model following the steps in the <a class="reference internal" href="#model-build-on-windows-host">Model Build on Windows Host</a> section.
This will produce the required DLL library.</p>
<p>The following assumes the Windows device has a v68 DSP.</p>
<p>Connect to the windows device with:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">mstsc</span><span class="w"> </span><span class="o">-</span><span class="n">v</span><span class="w"> </span><span class="o">&lt;</span><span class="n">your</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="n">IP</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Create the folder <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code> on the device.</p>
<p>Copy the following libraries from the Windows host to the Windows device in the folder: <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- ${QNN_SDK_ROOT}\lib\aarch64-windows-msvc\QnnHtp.dll
- ${QNN_SDK_ROOT}\lib\aarch64-windows-msvc\QnnHtpV68Stub.dll
- ${QNN_SDK_ROOT}\lib\aarch64-windows-msvc\QnnHtpPrepare.dll
- ${QNN_SDK_ROOT}\lib\hexagon-v68\unsigned\libQnnHtpV68Skel.so
- Inception_v3_quantized.dll (Quantized model (*.dll) as described in prerequisite)
</pre></div>
</div>
<p>Now, copy the input data and input list from the Windows host to the Windows device <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- C:\tmp\qnn_tmp\cropped
- C:\tmp\qnn_tmp\target_raw_list.txt
</pre></div>
</div>
<p>Now, copy the <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool to the Windows device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>- ${QNN_SDK_ROOT}\bin\aarch64-windows-msvc\qnn-net-run.exe
</pre></div>
</div>
<p>To run <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool, please open <code class="docutils literal notranslate"><span class="pre">&quot;Administrator:Windows</span> <span class="pre">Power</span> <span class="pre">Shell&quot;</span></code>:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\qnn_test_package

$ .\qnn-net-run.exe `
   --model .\Inception_v3_quantized.dll `
   --input_list .\target_raw_list.txt `
   --backend .\QnnHtp.dll
</pre></div>
</div>
<p>After performing the inference, we can check the classification result. By default, outputs from the run
will be located in the <code class="docutils literal notranslate"><span class="pre">.\output</span></code> directory. We will need to copy the results back to the Windows Host.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>copy C:\qnn_test_package\output to C:\tmp\qnn_tmp
</pre></div>
</div>
<p>Then, open <code class="docutils literal notranslate"><span class="pre">&quot;Developer</span> <span class="pre">PowerShell</span> <span class="pre">for</span> <span class="pre">VS</span> <span class="pre">2022&quot;</span></code> to run:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ cd C:\tmp\qnn_tmp

$ py -3 .\show_inceptionv3_classifications.py `
     -i .\cropped\raw_list.txt `
     -o output `
     -l .\imagenet_slim_labels.txt
</pre></div>
</div>
<p>Then, the classification results should be:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">trash_bin</span><span class="p">.</span><span class="n">raw</span><span class="w">   </span><span class="mf">0.777344</span><span class="w"> </span><span class="mi">413</span><span class="w"> </span><span class="n">ashcan</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w">      </span><span class="mf">0.253906</span><span class="w"> </span><span class="mi">832</span><span class="w"> </span><span class="n">studio</span><span class="w"> </span><span class="n">couch</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">plastic_cup</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.980469</span><span class="w"> </span><span class="mi">648</span><span class="w"> </span><span class="n">measuring</span><span class="w"> </span><span class="n">cup</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">notice_sign</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.167969</span><span class="w"> </span><span class="mi">459</span><span class="w"> </span><span class="n">brass</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>

          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>



    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a

    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>

    provided by <a href="https://readthedocs.org">Read the Docs</a>.

</footer>
        </div>
      </div>

    </section>

  </div>


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>






</body>
</html>
