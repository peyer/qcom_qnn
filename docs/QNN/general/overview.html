

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Overview &mdash; Qualcomm® AI Engine Direct</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Setup" href="setup.html" />
    <link rel="prev" title="Introduction" href="introduction.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® AI Engine Direct
          

          
          </a>

          
            
            
              <div class="version">
                v2.13.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#features">Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="#software-architecture">Software Architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#device">Device</a></li>
<li class="toctree-l3"><a class="reference internal" href="#backend">Backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="#context">Context</a></li>
<li class="toctree-l3"><a class="reference internal" href="#graph">Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operation-package-registry">Operation Package Registry</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#integration-workflow-on-linux">Integration Workflow on Linux</a></li>
<li class="toctree-l2"><a class="reference internal" href="#integration-workflow-on-windows">Integration Workflow  on Windows</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="backend.html">Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_packages.html">Op Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® AI Engine Direct</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Overview</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h1>
<p>Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> is Qualcomm Technologies Inc. (QTI) software architecture for AI/ML use cases
on QTIs chipsets and AI acceleration cores.</p>
<p>Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> architecture is designed to provide an unified API and modular and extensible
per-accelerator libraries which form a reusable basis for full stack AI solutions,
both QTI’s own and third party frameworks (as illustrated with <a class="reference internal" href="#qnn-sw-stack-figure"><span class="std std-ref">AI Software Stack with Qualcomm AI Engine Direct</span></a> diagram).</p>
<p class="centered" id="qnn-sw-stack-figure">
<strong><strong>AI Software Stack with Qualcomm AI Engine Direct</strong></strong></p><div class="figure align-default">
<img alt="../_static/resources/qnn_software_stack.png" src="../_static/resources/qnn_software_stack.png" />
</div>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h2>
<p><strong>Modularity based on hardware accelerators</strong></p>
<p>The Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> architecture is designed to be modular and allows for clean separation in the software
for different hardware cores/accelerators such as the CPU, GPU and DSP that are designated as
<em>backends</em>. Learn more about Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> backends <a class="reference internal" href="backend.html"><span class="doc">here</span></a>.</p>
<p>The Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> backends for different hardware cores/accelerators are compiled into
individual core-specific libraries that come packaged with the SDK.</p>
<p><strong>Unified API across IP Cores</strong></p>
<p>One of the key highlights of Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> is that it provides a unified API to delegate operations
such as graph creation and execution across all hardware accelerator backends. This allows users
to treat Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> as a hardware abstraction API and port applications easily to different cores.</p>
<p><strong>Right level of abstraction</strong></p>
<p>The Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> API is designed to support an efficient execution model
with capabilities such as graph optimizations to be taken care of internally.
At the same time however, it leaves out broader functionality such as model parsing and
network partitioning to higher level frameworks.</p>
<p><strong>Flexibility in composition</strong></p>
<p>With Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a>, users can choose appropriate tradeoffs between capabilities provided by the backends
and the footprint in terms of library size and memory utilization. This offers the ability to
compose a Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> operation package with only operations required to serve a set of models
targeted by a use-case <a class="footnote-reference brackets" href="#id2" id="id1">1</a>. With this, users can create nimble applications with
low memory footprint that fits a wide variety of hardware products.</p>
<p><strong>Extensible Operation Support</strong></p>
<p>Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> also provides support for clients to integrate custom operations to work seamlessly alongside
the built-in operations.</p>
<p><strong>Improved Execution Performance</strong></p>
<p>With optimized network loading and asynchronous execution support Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> serves to provide a highly
efficient interface for ML frameworks and applications to load and execute network graphs on
their desired hardware accelerator.</p>
</div>
<div class="section" id="software-architecture">
<h2>Software Architecture<a class="headerlink" href="#software-architecture" title="Permalink to this headline">¶</a></h2>
<p>Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> API and the associated software stack provides all the constructs required by an application
to construct, optimize and execute network models on the desired hardware accelerator core.
Key constructs are illustrated by the <a class="reference internal" href="#qnn-highlevel-view-figure"><span class="std std-ref">Qualcomm AI Engine Direct Components - High Level View</span></a> diagram.</p>
<p class="centered" id="qnn-highlevel-view-figure">
<strong><strong>Qualcomm AI Engine Direct Components - High Level View</strong></strong></p><div class="figure align-default">
<img alt="../_static/resources/qnn_highlevel_view.png" src="../_static/resources/qnn_highlevel_view.png" />
</div>
<div class="section" id="device">
<h3>Device<a class="headerlink" href="#device" title="Permalink to this headline">¶</a></h3>
<p>Software abstraction of a hardware accelerator platform. Provides all constructs required to associate desired hardware
accelerator resources for execution of user composed graphs. A platform is broken down into potentially multiple
devices. Devices may have multiple cores.</p>
</div>
<div class="section" id="backend">
<h3>Backend<a class="headerlink" href="#backend" title="Permalink to this headline">¶</a></h3>
<p>The backend is a top level API component which hosts and manages most of the backend resources required for graph
composition and execution, including an operation registry that stores all available operations.
Learn more about Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> backends <a class="reference internal" href="backend.html"><span class="doc">here</span></a>.</p>
</div>
<div class="section" id="context">
<h3>Context<a class="headerlink" href="#context" title="Permalink to this headline">¶</a></h3>
<p>A construct that represents all Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> components required to sustain a user application. Hosts networks provided by the
user and allows constructed entities to be cached into serialized objects for future use. It enables interoperability
between multiple graphs by providing a shareable memory space in which tensors can be exchanged between graphs.</p>
</div>
<div class="section" id="graph">
<h3>Graph<a class="headerlink" href="#graph" title="Permalink to this headline">¶</a></h3>
<p>The Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> way of representing a loadable network model. Consists of nodes that represent
operations and tensors that interconnect them to compose a directed acyclic graph.
The Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> graph construct supports APIs that perform initialization, optimization and execution of
network models.</p>
</div>
<div class="section" id="operation-package-registry">
<h3>Operation Package Registry<a class="headerlink" href="#operation-package-registry" title="Permalink to this headline">¶</a></h3>
<p>A registry that maintains record of all operations available to execute a model.
These operations can be built-in or supplied by the user as Custom Operations.
Learn more about operation packages <a class="reference internal" href="op_packages.html"><span class="doc">here</span></a>.</p>
</div>
</div>
<div class="section" id="integration-workflow-on-linux">
<h2>Integration Workflow on Linux<a class="headerlink" href="#integration-workflow-on-linux" title="Permalink to this headline">¶</a></h2>
<p>The Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> SDK provides tools and extensible per-accelerator libraries with uniform API enabling
flexible integration and efficient execution of ML/DL neural networks on QTI chipsets. The Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> API
is designed to support inference of trained neural networks and as such clients are responsible for
training a ML/DL network in a training framework of their choice. The training process is normally
done on server hosts, off-device. Once network is trained clients can use Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> to get it ready to
deploy and run on-device. This workflow is illustrated with the
<a class="reference internal" href="#training-inference-workflow-figure"><span class="std std-ref">Training vs. Inference Workflow</span></a> diagram.</p>
<p class="centered" id="training-inference-workflow-figure">
<strong><strong>Training vs. Inference Workflow</strong></strong></p><div class="figure align-default">
<img alt="../_static/resources/training_inference_workflow.png" src="../_static/resources/training_inference_workflow.png" />
</div>
<p>The Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> SDK includes tools to aid clients in integrating trained DL networks into their
applications. The basic integration workflow is illustrated with the
<a class="reference internal" href="#qnn-basic-workflow-figure"><span class="std std-ref">Qualcomm AI Engine Direct Integration Workflow</span></a> diagram.</p>
<p class="centered" id="qnn-basic-workflow-figure">
<strong><strong>Qualcomm AI Engine Direct Integration Workflow</strong></strong></p><div class="figure align-default">
<img alt="../_static/resources/qnn_basic_workflow.png" src="../_static/resources/qnn_basic_workflow.png" />
</div>
<ol class="arabic">
<li><p>Clients call Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> converter tool by providing their trained network model file as input.
The network must be trained in a framework supported by Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> converter tools.
See <a class="reference internal" href="tools.html"><span class="doc">Tools</span></a> section for more details on Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> converters.</p></li>
<li><p>When source models contain operations that are not supported natively by Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> backend,
clients need to provide OpPackage definition files to the converter, expressing custom / client
defined operations. Optionally, they can use the OpPackage generator tool in order to generate
skeleton code to implement and compile their custom operations into OpPackage libraries.
See <a class="reference internal" href="tools.html#qnn-op-package-generator"><span class="std std-ref">qnn-op-package-generator</span></a> for usage details.</p></li>
<li><p>Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> model converter is a tool aiding clients in writing a sequence of Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a>
API calls to construct Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> graph representation of a trained network which was provided as input to the tool.
The converter outputs the following files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">.cpp</span></code> source file (e.g. <code class="docutils literal notranslate"><span class="pre">model.cpp</span></code>) containing required Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> API calls to construct a
network graph</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.bin</span></code> binary file (e.g. <code class="docutils literal notranslate"><span class="pre">model.bin</span></code>) containing network weights and biases as float32 data</p></li>
</ul>
<p>Clients can optionally direct converter to output a quantized model instead of the default one,
as indicated in the diagram as <em>quantized model.cpp</em>. In this case <code class="docutils literal notranslate"><span class="pre">model.bin</span></code> file will
contain quantized data, and <code class="docutils literal notranslate"><span class="pre">model.cpp</span></code> will reference quantized tensor data types and
include quantization encodings. Quantized models may be required by some Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> backend libraries,
e.g. HTP or DSP (see <span class="xref std std-ref">general/api:Backend Supplements</span> for information on supported
data types). For details on converter quantization function and options see
<a class="reference internal" href="tools.html#quantization-support"><span class="std std-ref">Quantization Support</span></a>.</p>
</li>
<li><p>Clients optionally can use Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> model library generator tool to produce a model library.
See <a class="reference internal" href="tools.html#qnn-model-lib-generator"><span class="std std-ref">qnn-model-lib-generator</span></a> for usage details.</p></li>
<li><p>Clients integrate Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> model into their application by either dynamically loading model library
or compiling and statically linking <code class="docutils literal notranslate"><span class="pre">model.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">model.bin</span></code>.
In order to prepare and execute model (i.e. run inference), clients also need to load required
Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> backend accelerator and OpPackage libraries. Qualcomm® <a class="reference internal" href="introduction.html#qnn-ai-engine-note"><span class="std std-ref">AI Engine Direct</span></a> OpPackage libraries are registered with and
loaded by the backend.</p></li>
<li><p>Clients can optionally save context binary cache with prepared and finalized graphs. See
<a class="reference internal" href="api_overview.html#context-caching"><span class="std std-ref">Context Caching</span></a> for reference.
Such graphs can be repeatedly loaded from the cache without the need for model .cpp / library
any further. Loading model graph from the cache is significantly faster than preparing through
a sequence of graph composition API calls provided in model .cpp / library. Cached graphs cannot
be further modified; they are meant for deployment of prepared graphs, enabling faster
initialization of client applications.</p></li>
</ol>
</div>
<div class="section" id="integration-workflow-on-windows">
<h2>Integration Workflow  on Windows<a class="headerlink" href="#integration-workflow-on-windows" title="Permalink to this headline">¶</a></h2>
<p id="workflow-wsl">The workflow on a Windows host is the same as on a Linux host, though some steps will require execution on
WSL (x86) and others will be executed natively on Windows as outlined below.</p>
<ol class="arabic simple">
<li><p>For environment setup on a Windows host, please see <span class="xref std std-ref">Windows Platform Dependencies</span>.</p></li>
<li><p>For Model Conversion, the Linux Model Converter Tool is run on WSL (x86).</p></li>
<li><p>For OP Customization, the op package skeleton code is generated by running the Linux OpPackage Generator Tool on WSL (x86).</p></li>
<li><p>For Context Binary Generation, the prepared and finalized graphs are saved by running the Linux Context Binary Generator Tool on WSL (x86).</p></li>
<li><p>For Model Library Generation, the model library is produced by running the Windows Model Library Generator Tool natively on Windows.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When run on Windows natively, the Model Library Generation Tool must be run with python. Please refer to <a class="reference internal" href="tutorial3.html#windows-model-lib-generator"><span class="std std-ref">Model Build on Windows Host</span></a> section for an example.</p>
</div>
<p><strong>Notes</strong></p>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Future feature.</p>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="setup.html" class="btn btn-neutral float-right" title="Setup" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="introduction.html" class="btn btn-neutral float-left" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>