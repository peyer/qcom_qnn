

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Quantized 16 bit activations (A16) vs FP16: Performance and power differences &mdash; Qualcomm® AI Engine Direct</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Avoid converting PyTorch models to onnx first" href="htp_guidelines_avoid_pytorch_to_onnx_conversion.html" />
    <link rel="prev" title="Number of channels" href="htp_guidelines_channels.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Qualcomm® AI Engine Direct
          

          
          </a>

          
            
            
              <div class="version">
                v2.13.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../general/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/setup.html">Setup</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../general/backend.html">Backend</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../general/backend.html#backend-specific-pages">Backend Specific Pages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../general/dsp/dsp_backend.html">DSP</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../../general/htp/htp_backend.html">HTP</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#api-specializations">API Specializations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#usage-expectations">Usage Expectations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-supported-operations">QNN HTP Supported Operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-variable-batch">QNN HTP Variable Batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-backend-api">QNN HTP Backend API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-performance-infrastructure-api">QNN HTP Performance Infrastructure API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-precision">QNN HTP Precision</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-deep-learning-bandwidth-compression-dlbc">QNN HTP Deep Learning Bandwidth Compression (DLBC)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-setting-number-of-hvx-threads">QNN HTP - Setting Number of HVX Threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-backend-extensions">QNN HTP Backend Extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#qnn-htp-profiling">QNN HTP Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#op-writing-guidelines">Op Writing Guidelines</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../../general/htp/htp_backend.html#recommendations-for-network-design">Recommendations for Network Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#yielding-and-pre-emption">Yielding and Pre-Emption</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#vtcm-sharing">VTCM Sharing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#subsystem-restart-ssr">SubSystem Restart (SSR)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../general/htp/htp_backend.html#htp-session-artifact-usage-guidlines">HTP Session &amp; Artifact Usage Guidlines</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../general/hta/hta_backend.html">HTA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../general/cpu/cpu_backend.html">CPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../general/gpu/gpu_backend.html">GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../general/saver/saver_backend.html">Saver</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../general/op_packages.html">Op Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/glossary.html">Glossary</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Qualcomm® AI Engine Direct</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../general/backend.html">Backend</a> &raquo;</li>
        
          <li><a href="../../general/htp/htp_backend.html">HTP</a> &raquo;</li>
        
      <li>Quantized 16 bit activations (A16) vs FP16: Performance and power differences</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="quantized-16-bit-activations-a16-vs-fp16-performance-and-power-differences">
<h1>Quantized 16 bit activations (A16) vs FP16: Performance and power differences<a class="headerlink" href="#quantized-16-bit-activations-a16-vs-fp16-performance-and-power-differences" title="Permalink to this headline">¶</a></h1>
<p>As mentioned previously in the document:
<a class="reference internal" href="../../general/htp/htp_backend.html#recommendations-for-network-design"><span class="std std-ref">Recommendations for Network Design</span></a>,
quantized A16 bit activations generally achieve higher performance and have lower
power requirements. In order to get most benefit from using 16 bit activations, it
is critical to maximize utilization of the hardware. We recommend that the network
is designed to take maximum advantage of HMX. Some of these practices are outlined
below with examples.</p>
<div class="section" id="minimize-data-movement-operations">
<h2>Minimize data movement operations<a class="headerlink" href="#minimize-data-movement-operations" title="Permalink to this headline">¶</a></h2>
<p>Non computational operations that move or permute data in some way are expected to have similar
performance on FP16 vs A16, as the data size is the same. If a network is heavily dominated by
these operations then one might not see a very significant inf/sec gains and power benefits
using A16 activations compared to FP16 ones.</p>
<dl class="simple">
<dt>These operations include (but are not limited to):</dt><dd><ul class="simple">
<li><p>Reshape</p></li>
<li><p>Transpose</p></li>
<li><p>Depth to Space</p></li>
<li><p>Space to Depth</p></li>
<li><p>Concat</p></li>
</ul>
</dd>
</dl>
<p>Therefore, it is recommended to avoid these operations whenever it is feasible, and to consider approaches
which minimize usage of data movers.</p>
</div>
<div class="section" id="activation-fusion">
<h2>Activation Fusion<a class="headerlink" href="#activation-fusion" title="Permalink to this headline">¶</a></h2>
<p>The latest hardware can fuse some non-trivial activation functions (i.e, non-Relu)
into the previous convolution. We currently support fusing PRelu and HardSwish in
all precisions, namely, A8, A16 and FP16. Many of the networks provided in A16 make
heavy use of PRelu, which will benefit from this added support. However, there are
some caveats, and networks should be designed to make the best use of this feature.</p>
<p><cite>Note that simple activation functions like Relu are not subject to same constraints,
so we still recommend usage of Relu over PRelu/HardSwish whenever this can be achieved.</cite></p>
<div class="section" id="example-network-topology">
<h3>Example: Network Topology<a class="headerlink" href="#example-network-topology" title="Permalink to this headline">¶</a></h3>
<p>When fusing an activation, if the results of the preceding convolution are needed
elsewhere in the network, then the activation fusion is not done, as we would need to
do the convolution twice. This type of pattern is quite common in many networks and
prevents activation fusion.</p>
<div class="figure align-center" id="figure-11-figure">
<img alt="../../_static/resources/htp_guidelines_fig_11.png" src="../../_static/resources/htp_guidelines_fig_11.png" />
</div>
<p class="centered">
<strong><strong>Figure 1</strong></strong></p><p>From the Fig. 11, one can see that the results of the convolution are fed to a PRelu
(highlighted in yellow), and to an Add (the bottom AddV2), which prevents activation fusion.
It is therefore recommended to optimize the network topology so that activation fusion is always possible.</p>
</div>
</div>
<div class="section" id="example-replacing-add-with-convolution-to-maximize-activation-fusion">
<h2>Example: Replacing Add with Convolution to Maximize Activation Fusion<a class="headerlink" href="#example-replacing-add-with-convolution-to-maximize-activation-fusion" title="Permalink to this headline">¶</a></h2>
<p>Current activation fusion is supported for only convolution type ops. It does not apply
to other ops such as Add or Mul. If these types of ops are followed by a non-trivial activation,
it may be beneficial to replace these elementwise ops with convolution, which will allow
the activation functions to be fused into the convolution. For example in the Fig. 12,
the first Add can be replaced with a convolution which would allow the following PRelu to be fused
with the convolution. However, we still have the restriction that the output of the replaced Add can
not be used in another place. So some patterns like the one shown in Fig. 13 may not be fused.</p>
<div class="figure align-center" id="figure-12-figure">
<img alt="../../_static/resources/htp_guidelines_fig_12.png" src="../../_static/resources/htp_guidelines_fig_12.png" />
</div>
<p class="centered">
<strong><strong>Figure 2</strong></strong></p><div class="figure align-center" id="figure-13-figure">
<img alt="../../_static/resources/htp_guidelines_fig_13.png" src="../../_static/resources/htp_guidelines_fig_13.png" />
</div>
<p class="centered">
<strong><strong>Figure 3</strong></strong></p></div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="htp_guidelines_avoid_pytorch_to_onnx_conversion.html" class="btn btn-neutral float-right" title="Avoid converting PyTorch models to onnx first" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="htp_guidelines_channels.html" class="btn btn-neutral float-left" title="Number of channels" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>